{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5cb165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewroberts/Desktop/git-repos/bip-surrogates-paper/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from jax import config\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from flax import nnx\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gpjax as gpx\n",
    "from gpjax import Dataset\n",
    "from gpjax.gps import Prior as GPPrior\n",
    "from gpjax.kernels.approximations import RFF\n",
    "from gpjax.kernels import RBF\n",
    "from gpjax.parameters import transform, DEFAULT_BIJECTION\n",
    "\n",
    "from uncprop.utils.gpjax_models import construct_gp, construct_batch_gp, train_gp_hyperpars\n",
    "from uncprop.core.surrogate import GPJaxSurrogate\n",
    "from uncprop.utils.gpjax_multioutput import (\n",
    "    BatchIndependentGP,\n",
    "    get_batch_gp_from_template,\n",
    "    fit_batch_independent_gp,\n",
    "    _make_batched_loss_and_grad,\n",
    "    _get_single_output_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f985a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent functions\n",
    "f1 = lambda x: jnp.sin(x) + 0.3 * jnp.cos(10*x)\n",
    "f2 = lambda x: -0.5 * x**2\n",
    "\n",
    "# design data\n",
    "x = jnp.linspace(0, 10, 5).reshape(-1, 1)\n",
    "xgrid = jnp.linspace(0, 10, 100).reshape(-1, 1)\n",
    "\n",
    "y = jnp.hstack([f1(x), f2(x)])\n",
    "ygrid = jnp.hstack([f1(xgrid), f2(xgrid)])\n",
    "\n",
    "design = Dataset(x, y)\n",
    "testdata = Dataset(xgrid, ygrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93c574",
   "metadata": {},
   "source": [
    "## Helper functions for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4af6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for testing: computing loss/gradient one at a time (not vectorized)\n",
    "\n",
    "def _print_tree_vals(param):\n",
    "    l = param.prior.kernel.lengthscale.get_value()\n",
    "    v = param.prior.kernel.variance.get_value()\n",
    "    sd = param.likelihood.obs_stddev.get_value()\n",
    "    m = param.prior.mean_function.constant.get_value()\n",
    "\n",
    "    print(l, v, sd, m)\n",
    "\n",
    "\n",
    "def single_losses_grads(i, batchgp, objective, bijection):\n",
    "    D = _get_single_output_dataset(design, i)\n",
    "    g, p, s = nnx.split(batchgp.posterior_list[i], gpx.parameters.Parameter, ...)\n",
    "    phi = transform(p, bijection, inverse=True)\n",
    "\n",
    "    def l(phi):\n",
    "        params = transform(phi, bijection)\n",
    "        model = nnx.merge(g, params, s)\n",
    "        return objective(model, D)\n",
    "\n",
    "    loss = l(phi)\n",
    "    gradient = jax.grad(l)(phi)\n",
    "\n",
    "    print(f'loss: {loss}')\n",
    "    print('gradient:')\n",
    "    _print_tree_vals(gradient)\n",
    "\n",
    "\n",
    "def vectorized_losses_grads(loss_and_grad_vect, batchgp, bijection):\n",
    "    phi = transform(batchgp.params, bijection, inverse=True)\n",
    "    loss, gradient = loss_and_grad_vect(phi)\n",
    "\n",
    "    print(f'loss: {loss}')\n",
    "    print('gradient:')\n",
    "    _print_tree_vals(gradient)\n",
    "\n",
    "\n",
    "def plot_gps(design, testdata, gp: GPJaxSurrogate, pred=None):\n",
    "    fig, axs = plt.subplots(1, gp.output_dim)\n",
    "    if gp.output_dim > 1:\n",
    "        axs = axs.ravel()\n",
    "    else:\n",
    "        axs = [axs]\n",
    "\n",
    "    xgrid = testdata.X.ravel()\n",
    "    if pred is None:\n",
    "        pred = gp(testdata.X)\n",
    "    \n",
    "    for i, ax in enumerate(axs):\n",
    "        m = jnp.atleast_2d(pred.mean)[i]\n",
    "        sd = jnp.atleast_2d(pred.stdev)[i]\n",
    "\n",
    "        ax.fill_between(xgrid, m-2*sd, m+2*sd, alpha=0.4, color='lightblue')\n",
    "        ax.plot(xgrid, testdata.y[:,i], color='black', label='true')    \n",
    "        ax.plot(xgrid, m, color='blue', label='mean')\n",
    "        ax.plot(design.X.ravel(), design.y[:,i], 'ro')\n",
    "        ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a66eab",
   "metadata": {},
   "source": [
    "### Constructing batch GP from single-output template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a24e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_factory(dataset):\n",
    "    return construct_gp(dataset, set_bounds=False)[0]\n",
    "\n",
    "batchgp = get_batch_gp_from_template(gp_factory, design)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30362e9",
   "metadata": {},
   "source": [
    "# Optimizing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc14312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and objective function\n",
    "opt = optax.adam(1e-1)\n",
    "objective = lambda p, d: -gpx.objectives.conjugate_mll(p, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24f584",
   "metadata": {},
   "source": [
    "### Testing that batched loss function is correctly defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4040262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP 1:\n",
      "loss: 2084.5677156610914\n",
      "gradient:\n",
      "[3081.89094655] -1641.3601269753967 -7.698937248460797e-06 47.6538068659691\n",
      "\n",
      "GP 2:\n",
      "loss: 19.407487716687505\n",
      "gradient:\n",
      "[0.89806426] -0.008906382813585856 -5.088002350510418e-12 0.08703846890916944\n",
      "\n",
      "Vectorized:\n"
     ]
    }
   ],
   "source": [
    "# vectorized computations\n",
    "bijection = DEFAULT_BIJECTION\n",
    "loss_and_grad_vect = _make_batched_loss_and_grad(batchgp, objective, bijection, design)\n",
    "\n",
    "print('GP 1:')\n",
    "single_losses_grads(0, batchgp, objective, bijection)\n",
    "\n",
    "print('\\nGP 2:')\n",
    "single_losses_grads(1, batchgp, objective, bijection)\n",
    "\n",
    "print('\\nVectorized:')\n",
    "vectorized_losses_grads(loss_and_grad_vect, batchgp, bijection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03094b9f",
   "metadata": {},
   "source": [
    "### Run vectorized optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f352e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = transform(batchgp.params, DEFAULT_BIJECTION, inverse=True)\n",
    "print(f'Starting loss: {loss_and_grad_vect(phi)[0]}')\n",
    "\n",
    "batchgp, history = fit_batch_independent_gp(\n",
    "    batch_gp=batchgp,\n",
    "    objective=objective,\n",
    "    optim=opt,\n",
    "    num_iters=1000\n",
    ")\n",
    "\n",
    "phi_final = transform(batchgp.params, DEFAULT_BIJECTION, inverse=True)\n",
    "print(f'Final loss: {loss_and_grad_vect(phi_final)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9038b",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Once the hyperparameters of a BatchIndependentGP object are optimized and fixed, we cannot use this\n",
    "directly for vectorized prediction since gpjax kernels do not natively support batching over\n",
    "hyperparamters. We thus convert to a GP conjugate posterior object with a custom kernel that handles\n",
    "the batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncprop.utils.gpjax_multioutput import BatchedRBF\n",
    "\n",
    "def convert_gp_to_batch_kernel(gp: gpx.gps.ConjugatePosterior, design: Dataset):\n",
    "    batch_dim = design.y.shape[1]\n",
    "    kernel = gp.prior.kernel\n",
    "    meanf = gp.prior.mean_function\n",
    "    likelihood = gp.likelihood\n",
    "\n",
    "    batched_kernel = BatchedRBF(batch_dim=batch_dim,\n",
    "                                input_dim=design.in_dim,\n",
    "                                lengthscale=kernel.lengthscale, \n",
    "                                variance=kernel.variance)\n",
    "\n",
    "    batched_prior = gpx.gps.Prior(mean_function=meanf, kernel=batched_kernel)\n",
    "    batched_posterior = batched_prior * likelihood\n",
    "    surrogate = GPJaxSurrogate(batched_posterior, design)\n",
    "\n",
    "    return surrogate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0779a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate = convert_gp_to_batch_kernel(batchgp.batch_posterior, design)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544a354",
   "metadata": {},
   "source": [
    "### Validating batch predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01578c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gps(design, testdata, surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d79578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At present, to wrap a gpjax posterior that does not have a batched kernel, one \n",
    "# must first convert to a batch GP with batch size 1. \n",
    "\n",
    "# output index\n",
    "idx = 0\n",
    "\n",
    "# convert gpjax GP to batch with batch size 1\n",
    "gp_single = batchgp.posterior_list[idx]\n",
    "design_single = _get_single_output_dataset(design, idx)\n",
    "surrogate_single = convert_gp_to_batch_kernel(gp_single, design_single)\n",
    "\n",
    "# predictions\n",
    "testdata_single = _get_single_output_dataset(testdata, idx)\n",
    "fig, ax = plot_gps(design_single, testdata_single, surrogate_single)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac3689",
   "metadata": {},
   "source": [
    "### Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becea9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioning on new points in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9909ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioning on new point in single output example\n",
    "xnew = jnp.array([[6.0]])\n",
    "ynew = f1(xnew)\n",
    "newdata = Dataset(xnew, ynew)\n",
    "conditional_pred = surrogate_single.condition_then_predict(testdata_single.X, given=(xnew, ynew))\n",
    "\n",
    "plot_gps(design_single + newdata, testdata_single, surrogate_single, pred=conditional_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6de09",
   "metadata": {},
   "source": [
    "### Validating batch vs single output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f99c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_gp(surrogate, batchgp, train_data, test_inputs):\n",
    "    test_batch_gp_hyperpars(surrogate, batchgp)\n",
    "    test_batch_gp_meanf(surrogate, batchgp, test_inputs)\n",
    "    test_batch_gp_gram(surrogate, batchgp, test_inputs)\n",
    "    test_batch_gp_cross_cov(surrogate, batchgp, test_inputs)\n",
    "    # test_batch_gp_pred(surrogate, batchgp, train_data, test_inputs)\n",
    "\n",
    "def test_batch_gp_hyperpars(surrogate, batchgp):\n",
    "    posts = batchgp.posterior_list\n",
    "\n",
    "    # hyperparameters\n",
    "    l_surr = surrogate.gp.prior.kernel.lengthscale.get_value()\n",
    "    l_posts = jnp.array([post.prior.kernel.lengthscale.get_value() for post in posts])\n",
    "\n",
    "    v_surr = surrogate.gp.prior.kernel.variance\n",
    "    v_posts = jnp.array([post.prior.kernel.variance.get_value() for post in posts])\n",
    "\n",
    "    obs_sd_surr = surrogate.gp.likelihood.obs_stddev.get_value()\n",
    "    obs_sd_posts = jnp.array([post.likelihood.obs_stddev.get_value() for post in posts])\n",
    "\n",
    "    m_surr = surrogate.gp.prior.mean_function.constant.get_value()\n",
    "    m_posts = jnp.array([post.prior.mean_function.constant.get_value() for post in posts])\n",
    "\n",
    "    print('lengthscales equal:', jnp.array_equal(l_surr, l_posts))\n",
    "    print('variances equal:', jnp.array_equal(v_surr, v_posts))\n",
    "    print('obs stdevs equal:', jnp.array_equal(obs_sd_surr, obs_sd_posts))\n",
    "    print('obs stdevs equal:', jnp.array_equal(m_surr, m_posts))\n",
    "\n",
    "\n",
    "def test_batch_gp_meanf(surrogate, batchgp, test_inputs):\n",
    "    post_list = batchgp.posterior_list\n",
    "    mean_surr = surrogate.gp.prior.mean_function(test_inputs)\n",
    "    mean_posts = jnp.hstack([post.prior.mean_function(test_inputs) for post in post_list])\n",
    "    print(f'prior means equal: {jnp.array_equal(mean_surr, mean_posts)}')\n",
    "\n",
    "def test_batch_gp_gram(surrogate, batchgp, test_inputs):\n",
    "    post_list = batchgp.posterior_list\n",
    "    gram_surr = surrogate.prior_gram(test_inputs)\n",
    "    gram_posts = jnp.stack([post.prior.kernel.gram(test_inputs).to_dense() for post in post_list])\n",
    "    print(f'prior grams equal: {jnp.array_equal(gram_surr, gram_posts)}')\n",
    "\n",
    "def test_batch_gp_cross_cov(surrogate, batchgp, test_inputs):\n",
    "    post_list = batchgp.posterior_list\n",
    "    cross_surr = surrogate.prior_cross_covariance(test_inputs, test_inputs)\n",
    "    cross_posts = jnp.stack([post.prior.kernel.cross_covariance(test_inputs, test_inputs) for post in post_list])\n",
    "    print(f'prior cross covs equal: {jnp.array_equal(cross_surr, cross_posts)}')\n",
    "\n",
    "def test_batch_gp_pred(surrogate, batchgp, train_data, test_inputs):\n",
    "    post_list = batchgp.posterior_list\n",
    "    pred_surr = surrogate(test_inputs)\n",
    "    surr_mean = pred_surr.mean\n",
    "    surr_var = pred_surr.variance\n",
    "\n",
    "    for i in range(batchgp.dim_out):\n",
    "        pred_post_latent = post_list[i](test_inputs, train_data)\n",
    "        pred_post = post_list[i].likelihood(pred_post_latent)\n",
    "\n",
    "        print(f'output {i+1} predictions:')\n",
    "        print(f'\\tpred means equal: {jnp.array_equal(surr_mean[i], pred_post.mean)}')\n",
    "        print(f'\\tpred variances equal: {jnp.array_equal(surr_var[i], pred_post.variance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncprop.utils.gpjax_multioutput import BatchIndependentGP, BatchedRBF\n",
    "from uncprop.utils.gpjax_models import _get_distance_stats_from_design\n",
    "import gpjax as gpx\n",
    "import optax\n",
    "\n",
    "def construct_batch_gp(design):\n",
    "    Y = design.y\n",
    "    batch_dim = Y.shape[1]\n",
    "    constants_init = gpx.parameters.Real(value=Y.mean(axis=0))\n",
    "    meanf = gpx.mean_functions.Constant(constants_init)\n",
    "\n",
    "    # prior (batch) kernel\n",
    "    dist_stats = _get_distance_stats_from_design(design)\n",
    "    lengthscale_init = dist_stats['mean']\n",
    "    vars_init = Y.var(axis=0)\n",
    "\n",
    "    kernel = BatchedRBF(batch_dim=batch_dim,\n",
    "                        input_dim=design.in_dim,\n",
    "                        lengthscale=lengthscale_init, \n",
    "                        variance=vars_init)\n",
    "\n",
    "    gp_prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n",
    "\n",
    "    obs_sd_init = jnp.tile(gp_prior.jitter, (batch_dim, 1))\n",
    "    gp_likelihood = gpx.likelihoods.Gaussian(num_datapoints=design.n, \n",
    "                                             obs_stddev=obs_sd_init)\n",
    "    gp_posterior = gp_prior * gp_likelihood\n",
    "\n",
    "    return gp_posterior\n",
    "\n",
    "\n",
    "# fit and train\n",
    "gp_posterior = construct_batch_gp(design)\n",
    "batchgp = BatchIndependentGP(dataset=design, batch_posterior=gp_posterior)\n",
    "# batchgp = get_batch_gp_from_template(gp_factory, design)\n",
    "\n",
    "objective = lambda p, d: -gpx.objectives.conjugate_mll(p, d)\n",
    "opt = optax.adam(1e-1)\n",
    "\n",
    "batchgp, history = fit_batch_independent_gp(\n",
    "    batch_gp=batchgp,\n",
    "    objective=objective,\n",
    "    optim=opt,\n",
    "    num_iters=1000\n",
    ")\n",
    "# surrogate = GPJaxSurrogate(gp=batchgp.batch_posterior,\n",
    "#                            design=design)\n",
    "\n",
    " # run tests\n",
    "# test_batch_gp(surrogate, batchgp, surrogate.design, testdata.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.array([1,2,3]).reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "graphdef, params, *static = nnx.split(gp_posterior, gpx.parameters.Parameter, ...)\n",
    "params_promoted = jax.tree.map(lambda p: jnp.broadcast_to(p, ), params)\n",
    "\n",
    "\n",
    "\n",
    "flat = jax.tree.map(lambda p: p, params)\n",
    "shp = jax.tree.map(lambda p: print(p.shape), params)\n",
    "\n",
    "# jax.tree.leaves(shp)\n",
    "\n",
    "jax.tree.leaves(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "single_design = Dataset(surrogate.design.X, surrogate.design.y[:,[idx]])\n",
    "pred = surrogate(testdata.X)\n",
    "pred_baseline_latent = batchgp.posterior_list[idx](testdata.X, single_design)\n",
    "pred_baseline = batchgp.posterior_list[idx].likelihood(pred_baseline_latent)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs = axs.ravel()\n",
    "ax = axs[0]\n",
    "ax.scatter(pred_baseline.mean, pred.mean[idx])\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ymin, ymax = ax.get_ylim()\n",
    "grid = jnp.linspace(xmin, xmax, 100)\n",
    "ax.plot(grid, grid, color='gray', linewidth=1, linestyle='--', label='y = x')\n",
    "ax.set_title('mean')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.scatter(pred_baseline.variance, pred.variance[idx])\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ymin, ymax = ax.get_ylim()\n",
    "grid = jnp.linspace(xmin, xmax, 100)\n",
    "ax.plot(grid, grid, color='gray', linewidth=1, linestyle='--', label='y = x')\n",
    "ax.set_title('variance')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = testdata.X[:2]\n",
    "\n",
    "idx = 1\n",
    "gram_surr = surrogate.prior_gram(U)\n",
    "gram2 = surrogate.gp.prior.kernel.gram(U).to_dense()\n",
    "gram_baseline = batchgp.posterior_list[idx].prior.kernel.gram(U).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b254a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gram_surr[idx]) # equal\n",
    "print(gram2[..., idx]) # equal\n",
    "print(gram_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = batchgp.posterior_list[0].prior.mean_function(U)\n",
    "b = batchgp.posterior_list[1].prior.mean_function(U)\n",
    "\n",
    "jnp.hstack([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]) -> ScalarFloat:\n",
    "    x = self.slice_input(x) / self.lengthscale.value \n",
    "    y = self.slice_input(y) / self.lengthscale.value\n",
    "    K = self.variance.value * jnp.exp(-0.5 * squared_distance(x, y))\n",
    "    return K.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = U[0]\n",
    "v = U[1]\n",
    "\n",
    "kernel = surrogate.gp.prior.kernel\n",
    "\n",
    "kernel(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchgp.posterior_list[1].prior.kernel(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7a908",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648385ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpjax.kernels.stationary import RBF\n",
    "\n",
    "from uncprop.utils.gpjax_multioutput import (\n",
    "    BatchDenseKernelComputation,\n",
    "    BatchedStationaryKernel,\n",
    "    BatchedRBF,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_case_one(key):\n",
    "    d, q = 1, 3\n",
    "    x = jnp.array([1, 2, 3])\n",
    "    y = jnp.array([4, 5])\n",
    "    z = jnp.array(6)\n",
    "\n",
    "    lengthscales = jnp.array([1, 2, 3])[:, None]\n",
    "    variances = jnp.array([1, 2, 3])\n",
    "\n",
    "    return d, q, x, y, z, lengthscales, variances\n",
    "\n",
    "def setup_case_two(key):\n",
    "    d, q = 2, 2\n",
    "    kx, ky, kz = jr.split(key, 3)\n",
    "\n",
    "    x = jr.uniform(kx, (3, 2))\n",
    "    y = jr.uniform(ky, (2, 2))\n",
    "    z = jr.uniform(kz, (2,))\n",
    "\n",
    "    lengthscales = jnp.array([[0.5, 0.1], [0.4, 0.3]]) # (q, d)\n",
    "    variances = 1.0\n",
    "\n",
    "    return d, q, x, y, z, lengthscales, variances\n",
    "\n",
    "def kernel_list_to_batch(kers, x, y):\n",
    "    return jnp.stack([ker.cross_covariance(x, y) for ker in kers])\n",
    "\n",
    "def test_kernel(key, test_fn):\n",
    "    d, q, x, y, z, l, v = test_fn(key)\n",
    "    kernel = BatchedRBF(batch_dim=q, input_dim=d, lengthscale=l, variance=v)\n",
    "\n",
    "    x = x.reshape(-1, d)\n",
    "    y = y.reshape(-1, d)\n",
    "    z = z.reshape(-1, d)\n",
    "\n",
    "    l = kernel.lengthscale\n",
    "    v = kernel.variance\n",
    "\n",
    "    kers = [RBF(lengthscale=l[i], variance=v[i], n_dims=d) for i in range(q)]\n",
    "\n",
    "    def k0(x, y):\n",
    "        return kernel_list_to_batch(kers, x, y)\n",
    "\n",
    "    def k1(x, y):\n",
    "        return kernel.cross_covariance(x, y)\n",
    "\n",
    "    assert jnp.array_equal(k0(x, y), k1(x, y))\n",
    "    assert jnp.array_equal(k0(x, z), k1(x, z))\n",
    "    assert jnp.array_equal(k0(y, z), k1(y, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batched kernel\n",
    "d, q, x, y, z, lengthscales, variances = setup_case_one(_)\n",
    "\n",
    "kernel = BatchedRBF(batch_dim=q,\n",
    "                    input_dim=d,\n",
    "                    lengthscale=lengthscales, \n",
    "                    variance=variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.key(325234)\n",
    "test_keys = jr.split(key, 2)\n",
    "\n",
    "test_kernel(test_keys[0], setup_case_one)\n",
    "test_kernel(test_keys[1], setup_case_two)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
