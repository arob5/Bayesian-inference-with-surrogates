\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{caption, subcaption} % Captions and sub-figures. 
\usepackage{fancyvrb} % For writing using verbatim font inline. 
\usepackage{float}
% \usepackage[demo]{graphicx}

% Bibliography
\bibliographystyle{plainnat}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Tables. 
\usepackage{multirow}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{{./../output/plots/}}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Probabilistic Surrogates for Bayesian Inverse Problems: Posterior Approximation and Sequential Design}
\author{Andrew Roberts}

\begin{document}

\maketitle

% Introduction 
\section{Introduction}
In the modern era, data-driven and mechanistic modeling have become closely intertwined, with hybrid modeling strategies 
increasingly used to understand and predict the behavior of complex systems. Statistical techniques have become 
the standard for uncertainty quantification in such contexts, but challenges persist in adapting statistical inference procedures
to handle the computational burden imposed by expensive computer simulations. 
Within the Bayesian paradigm, there is thus growing interest in methods of Bayesian computation tailored to settings 
where evaluation of the likelihood function incurs significant computational expense. 
This situation commonly arises across various disciplines in the physical and engineering sciences, whereby evaluating 
the likelihood implies performing a complex computer simulation; e.g., \citep{ESM_modeling_2pt0,FerEmulation}. 
Such computer models often derive from systems of differential equations, with the goal being to calibrate (i.e., estimate) 
unknown parameters characterizing the physical model.
To address this computational bottleneck, a wide body of research has emerged that seeks to characterize the Bayesian 
posterior distribution over model parameters while minimizing the number of required likelihood evaluations. 
A popular class of methods involves replacing either the computer model or the log-likelihood function with a statistical 
surrogate (also called an emulator or meta-model), which induces a computationally cheap approximation of the likelihood.
The surrogate can be trained offline \citep{modularization} using a small set of expensive model runs (often performed in 
parallel), which then replaces the exact likelihood to facilitate the application of 
standard inference schemes such as Markov chain Monte Carlo (MCMC).

The present work is focused on the use of \textit{stochastic} emulators in accelerating Bayesian inference.
A probabilistic representation of a surrogate model acknowledges the additional uncertainty introduced by the likelihood 
approximation. We focus on the questions of how to leverage this uncertainty in constructing posterior
approximations and in designing goal-oriented sequential design (i.e., active learning) algorithms
that iteratively refine the surrogate to yield maximal improvement in the induced posterior approximation. 
Probabilistic emulators induce a stochastic representation of the posterior density, such that the true density is 
modeled as a realization of a random function. Such surrogates commonly arise when an emulator
model is fit using Bayesian methods, though non-Bayesian techniques such as conformal predictors (\todo: cite) and 
deep ensembles (\todo: cite) are increasingly being considered [\todo: also mention polynomial chaos]. 
Gaussian processes (GPs; \citet{gpML}) arguably represent the canonical model choice in this setting,
 and have been widely used as emulators in calibrating complex computer codes 
\citep{design_analysis_computer_experiments,SanterCompExp,gramacy2020surrogates}. A growing body of research 
has begun to focus specifically on the use of GP emulators in solving Bayesian inverse problems
\citep{KOH,StuartTeck1,StuartTeck2,VehtariParallelGP,gp_surrogates_random_exploration,quantileApprox,FerEmulation,
KandasamyActiveLearning2015,CES,Kandasamy_2017,SinsbeckNowak,Surer2023sequential,llikRBF,
ranjan2016inverse,weightedIVAR,hydrologicalModel,VillaniAdaptiveGP,GP_PDE_priors,random_fwd_models,ActiveLearningMCMC,
MCMC_GP_proposal,trainDynamics}. 
This focus on GPs facilitates analytical computations 
in both the tasks of surrogate-induced posterior approximation and sequential design. Beyond this convenience, the availability of 
closed-form expressions also allows for improved interpretability. On the other hand, the desire for analytical tractability
severely limits modeling flexibility. Many existing methodologies are limited to the Gaussian setting: requiring emulators with 
Gaussian predictive distributions, or imposing a Gaussian assumption on the likelihood being approximated.
Small deviations from these assumptions commonly break the closed-form computations enjoyed in this limited regime.
Therefore, in addition to providing a comprehensive review of the Gaussian setting, we emphasize recent work on 
developing general-purpose probabilistic workflows that relax the Gaussian requirements.

We begin our review of surrogate-assisted Bayesian inversion with a discussion of the surrogate construction itself,
drawing a distinction between two general strategies common in the literature: \textit{forward model emulation}, whereby
(some function of) the expensive computer model is approximated by an emulator, and \textit{log-density emulation}, where
a surrogate approximates either the log-likelihood or log-unnormalized posterior density directly. 
Note that the latter approach is a special case of the former, and both strategies ultimately induce an 
approximation of the posterior density. However, we still find it useful to make this distinction, 
as the selection of the target function for emulation has implications for propagating surrogate uncertainty. Moreover, the 
availability of analytical computations differs across the two approaches. We discuss the relative merits and downsides of each 
approach, as well as typical use cases. The choice between forward model and log-density emulation is also explored
in \citet{StuartTeck1,GP_PDE_priors,random_fwd_models}, and briefly noted in 
\citet{Surer2023sequential,trainDynamics,ActiveLearningMCMC,emPostDens}.

We next proceed to review methods for utilizing the surrogate model in approximating the posterior; i.e., 
defining a deterministic posterior approximation that propagates the surrogate uncertainty.
We begin with a thorough 
review of the GP setting, in which a variety of posterior approximations have been proposed across several different literatures; 
we collect these results here, presenting streamlined derivations aimed at clarifying the assumptions underlying the various 
methods. We then proceed to describe generalizations to non-Gaussian stochastic emulators, as well as recent work
that seeks to develop a framework for posterior approximation applicable to generic probabilistic surrogate models.
Of particular note is the work \citet{BurknerSurrogate}, which investigates several methods that can be viewed as 
Monte Carlo approximations of methods previously proposed in the GP setting.
We also discuss [our work], which provides an alternative approach to generate approximate posterior samples using a 
stochastic surrogate, with the additional benefit of handling the common setting of non-parametric emulator models.

The final portion of this article focuses on improving an existing surrogate by augmenting the training data 
using a new set of exact simulation evaluations. This constitutes a sequential design (i.e., active learning) problem, 
whereby the selection of new training points is optimized to yield maximal improvement in the induced 
posterior approximation. This leads to an instance of the ``exploration vs. exploitation'' balancing act,
familiar to fields such as Bayesian optimization \citep{reviewBayesOpt}, Bayesian quadrature 
\citep{BayesQuadrature,BayesQuadratureAL,BayesQuadRatios,quadratureLogGP}, reinforcement learning 
\citep{BadiaRL,LiuRL}, and bandit algorithms \citep{banditsEmpirical,LattimoreBandits}, among others.
In the present setting, this 
manifests as a tradeoff between placing new points in regions favored by the current posterior approximation
and exploring other regions of parameter space. For GP surrogates, a variety of 
design criteria (i.e., acquisition functions) have been proposed that seek to target high probability regions, 
while also acknowledging uncertainty in the GP model 
\citep{SinsbeckNowak,Surer2023sequential,KandasamyActiveLearning2015,weightedIVAR,VehtariParallelGP,VillaniAdaptiveGP}.
We review existing approaches to defining such goal-oriented design criteria, and provide useful intermediate results that 
facilitate improved interpretability in settings where closed-form expressions are available.
We also highlight batch sequential approaches, where multiple design points are acquired simultaneously. 
Batch sequential design becomes increasingly essential as the cost of likelihood evaluations increases, as it allows for 
parallel computations. 

We demonstrate these methods through various numerical experiments. [\todo: add a couple sentences here]

% Background
\section{Background}
In this section we introduce the Bayesian inference setting of interest, with an emphasis on Bayesian inverse problems
stemming from mechanistic dynamical models. We provide a concrete example of parameter estimation for ordinary 
differential equations, and conclude the section with a brief overview of Gaussian processes.
 
 \subsection{Bayesian Inverse Problems}
We focus on the Bayesian inference setting in which pointwise evaluations of the unnormalized posterior
density 
\begin{equation}
\postDens(\Par) \Def \priorDens(\Par) p(\obs \given \Par) \label{post_dens_generic}
\end{equation}
are available, but expensive owing to the cost of computing the likelihood $p(\obs \given \Par)$. This setting 
commonly arises in the Bayesian approach to inverse problems \citep{Stuart_BIP}. 
Consider a \textit{forward model} $\fwd: \parSpace \subseteq \R^{\dimPar} \to \R^{\dimObs}$ describing 
some system of interest, parameterized by input parameters $\Par \in \parSpace$. In addition, suppose that 
we have noisy observations $\obs \in \obsSpace \subseteq \R^{\dimObs}$ of the output signal that $\fwd(\Par)$ 
seeks to approximate. The \textit{inverse problem} concerns learning the parameter values $\Par$ such
 that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating} the model so that it agrees with the observations. 
 The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on $\obs \given \Par$. We assume that this distribution 
admits a density with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs \given \Par) = \Exp{\llik(\Par)}$. The notation $\llik(\Par)$ suppresses the dependence 
on $\obs$, as the observed data will be fixed throughout. We start by focusing inference only on the 
calibration parameter $\Par$, assuming that other likelihood parameters (e.g., noise covariance) 
are fixed. We discuss inference for such nuisance parameters in \Cref{section_lik_par}.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution
\begin{align}
\postDensNorm(\Par) \Def p(\Par \given \obs) = \frac{1}{\normCst}\postDens(\Par) = \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant given by 
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. We will also find it useful to introduce the notation
\begin{equation}
\lpost(\Par) \Def \log \postDens(\Par) = \log \priorDens(\Par) + \llik(\Par) \label{eq:lpost}
\end{equation}
for the logarithm of the unnormalized posterior density.
When relevant, we will make explicit the dependence on the forward 
model by writing $\llik(\Par; \fwd)$, $\lpost(\Par; \fwd)$, and $\postDens(\Par; \fwd)$. 

In addition to considering 
generic likelihoods, we will give special attention to the additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{inv_prob_Gaussian} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par; \fwd) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) 
= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)), \label{llik_Gaussian}
\end{align}
as this model features prominently in the inverse problems literature.
In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are serial in nature, often requiring $\sim 10^5 - 10^7$ iterations, with each 
iteration involving an evaluation of the unnormalized posterior density $\postDens(\Par; \fwd)$. 
In the inverse problem context, this computational requirement is often prohibitive when the forward model 
evaluations $\fwd(\Par)$ incur significant computational cost, as each density evaluation requires the 
expensive computation $\fwd(\Par)$. Motivated by this problem, a large body of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\lpost(\Par)$ (note that approximating the former induces 
an approximation of the latter). We focus on surrogate models that take the form of statistical 
regression models approximating the maps $\Par \mapsto \fwd(\Par)$ or $\Par \mapsto \lpost(\Par)$.
\footnote{This is in contrast to other surrogate modeling strategies (e.g., reduced-order modeling)
that exploit the specific structure of the forward model; see \citet{multifidelityReview} for a detailed review.}
We refer to methods that explicitly model the former map as \textit{forward model emulation}, and those that 
model the latter as \textit{log-density emulation}. We also include methods that emulate the log-likelihood
map $\Par \mapsto \llik(\Par)$ in this latter category (see \Cref{sec:llik_vs_lpost}).
Throughout this review, we will typically view the functions
$\fwd(\Par)$, $\llik(\Par)$, and $\lpost(\Par)$ as computationally expensive black-boxes. We will occasionally refer to 
such maps as \textit{simulators}, owing to the fact that in typical applications evaluating these maps requires
running an expensive computer code. The following section provides a concrete example of such a simulation
model stemming from the numerical solution of differential equations. 
 
\subsection{Motivating Example: Parameter Estimation for ODEs} \label{dynamical_models}
We now demonstrate how the problem of estimating the parameters of differential equation models 
 can be cast within the Bayesian inverse problem framework. Our primary motivating applications 
 stem from the use of dynamical models in Earth system modeling  \citep{ESM_modeling_2pt0,paramLSM}.
Consider an initial value problem 
\begin{align}
\frac{d}{d\Time} \state(\Time, \Par) &= \odeRHS(\state(\Time, \Par), \Par), &&x(\timeStart) = \stateIC, \label{ode_ivp}
\end{align}
describing the time evolution of $\dimState$ state variables 
$\state(\Time) \Def \left[\indexState[1]{\state}(\Time), \dots, \indexState[\dimState]{\state}(\Time)\right]^\top \in \mathbb{R}^{\dimState}$,
with dynamics depending on parameters $\Par \in \parSpace$. As our focus will be on estimating these parameters 
from observations, we consider the parameter-to-state map
\begin{align}
\Par &\mapsto \left\{\state(\Time, \Par) :  \Time \in [\timeStart, \timeEnd] \right\}.
\end{align}
In practice, the solution is typically approximated via a numerical discretization of the form 
\begin{align}
\solutionOp: \Par &\mapsto \left[\indexTime[0]{\state}(\Par), \dots, \indexTime[\NTimeStep]{\state}(\Par) \right]^\top. 
\end{align}
Here, $\solutionOp: \parSpace \to \R^{\NTimeStep \times \dimState}$ represents the map induced by a numerical solver, 
and $\indexTime[0]{\state}(\Par), \dots, \indexTime[\NTimeStep]{\state}(\Par)$ are approximations of the state 
values $\state(\Time, \Par)$ at a finite set of time points in $[\timeStart, \timeEnd]$. 
Going forward, we focus on the discrete-time operator $\solutionOp$, neglecting discretization error
for simplicity. Finally, suppose we have observed data $\obs \in \obsSpace \subseteq \R^{\dimObs}$ that we model as a
noise-corrupted function of the state trajectory. This is formalized by the definition of an observation operator 
$\obsOp: \R^{\NTimeStep \times \dimState} \to \obsSpace$ mapping from the state trajectory to a 
$\dimObs$-dimensional observable quantity. We assume the observations are generated as 
\begin{align}
\obs &= (\obsOp \circ \solutionOp)(\ParTrue) + \noise \label{ode_inv_prob} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber 
\end{align}
for some ``true'' parameter value $\ParTrue \in \parSpace$. Observe that this falls within the generic 
inverse problem framework with forward model $\fwd \Def \obsOp \circ \solutionOp$. 
Generalizations can be considered to settings with non-additive or non-Gaussian noise, or the 
presence of model discrepancy. In our numerical experiments we consider two problems of the 
form \cref{ode_inv_prob} motivated by applications to near-term terrestrial carbon cycle forecasting and 
long-term climate modeling. 

 \subsection{Gaussian Processes} \label{gp_review}
 In this section, we provide a brief review of Gaussian processes; for in-depth treatments, we refer 
 readers to \cite{gramacy2020surrogates, StuartTeck2, gpML}. We also discuss basic extensions
 to Gaussian process models for multi-output functions, which will be relevant in various examples
we discuss later.
 
 \subsubsection{Single Output Gaussian Processes}
 A Gaussian process (GP) can be thought of as a
random function $\funcPrior: \parSpace \to \R$, defined by the property that 
the random vector $[\funcPrior(\Par_1), \dots, \funcPrior(\Par_\Ndesign)]^\top$ has
a joint Gaussian distribution for any finite set of inputs $\parMat \Def \{\Par_1, \dots, \Par_\Ndesign\} \subset \parSpace$. 
A GP is defined by its mean function $\gpMeanPrior: \parSpace \to \R$ and positive-definite kernel
(i.e., covariance function) $\gpKerPrior: \parSpace \times \parSpace \to \R$ and is thus denoted by 
$\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$. Throughout this paper, we will vectorize notation as follows. Letting 
$\tilde{\parMat} \Def [\tilde{\Par}_1, \dots, \tilde{\Par}_M]  \subset \parSpace$ be a second set of inputs, we define 
$\funcPrior(\parMat) \Def [\funcPrior(\Par_1), \dots, \funcPrior(\Par_\Ndesign)]^\top \in \R^{\Ndesign}$, 
$\gpKerPrior(\parMat, \tilde{\parMat}) \Def \{\gpKerPrior(\Par_{\idxDesign}, \tilde{\Par}_m)\}_{\substack{1 \leq \idxDesign \leq \Ndesign \\ 1 \leq m \leq M}} \in \R^{\Ndesign \times M}$, 
and $\gpKerPrior(\parMat) \Def \gpKerPrior(\parMat, \parMat) \in \R^{\Ndesign \times \Ndesign}$. 
We similarly use this vectorized notation for other functions; e.g., $\gpMeanPrior(\parMat)$, $\fwd(\parMat)$, and $\llik(\parMat)$.
Given this notation, 
the defining property of the GP can be written as 
\begin{align}
\funcPrior(\parMat) &\sim \Gaussian(\gpMeanPrior(\parMat), \gpKerPrior(\parMat)).
\end{align}
For our purposes, the GP $\funcPrior$ will represent a prior distribution for a \textit{deterministic} function 
$\func: \parSpace \to \R$. 
However, the underlying ideas explored here can be extended to the setting where evaluations of $\func$ 
are corrupted by noise \citep{stochasticComputerModels,VehtariParallelGP,OakleyllikEm}. 
If we observe noiseless function evaluations $\func(\designIn)$ at a set of inputs $\designIn$, then we can consider
the conditional distribution $\funcPrior|[\funcPrior(\designIn)=\func(\designIn)]$. It is well-known
\citep{gpML} that the conditional is also a GP
\begin{align}
\funcEm[\Ndesign] \Def \funcPrior|[\funcPrior(\designIn)=\func(\designIn)]\sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign]), \label{generic_gp_conditional}
\end{align}
with mean and kernel given by
\begin{align}
\gpMean[\Ndesign](\parMat) &= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} [\func(\designIn) - \gpMeanPrior(\designIn)] \label{kriging_eqns} \\
\gpKer[\Ndesign](\parMat) &= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} \gpKerPrior(\designIn, \parMat). \nonumber
\end{align}

We will refer to observed input locations $\designIn$ as \textit{design points}, and the conditional
distribution $\GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$ as the \textit{predictive distribution}.
The predictive mean can be seen to interpolate the design points (i.e., $\gpMean[\Ndesign](\designIn) = \func(\designIn[\Ndesign])$), with 
the predictive variance vanishing at these inputs (i.e., $\gpKer[\Ndesign](\designIn) = 0$). 
To break these interpolation properties,
a constant can be added to the diagonal of the kernel matrix so that $(\gpKerPrior(\designIn) + \nuggetSD^2 I)^{-1}$ replaces 
$\gpKerPrior(\designIn)^{-1}$ in \cref{kriging_eqns}. In the present context of deterministic forward models, 
this interpolation property is typically desirable. Nevertheless, a small, positive fixed constant $\nuggetSD^2$ is still 
typically included for numerical stability. 

Appropriate choices for the prior mean $\gpMeanPrior$ and covariance $\gpKerPrior$ depend on the 
particular application, but we typically take the mean to be a constant or a linear model of the form 
$\gpMeanPrior(\Par) = \beta^\top \phi(\Par)$, for some feature map $\phi$. A common default choice of 
covariance function is the exponentiated quadratic (also called squared exponential, Gaussian, or radial basis function)
kernel,
\begin{align}
\gpKerPrior(\Par, \tilde{\Par}) &= 
\margSD^2 \Exp{- \sum_{\idxParDim=1}^{\dimPar} \left(\frac{\Par_{\idxParDim} - \tilde{\Par}_{\idxParDim}}{\lengthscale_{\idxParDim}} \right)^2}, \label{Gaussian_kernel}
\end{align}
defined by the choice of positive hyperparameters $\margSD, \lengthscale_{1}, \dots, \lengthscale_{\dimPar}$. 
This is an example of a stationary (i.e., shift-invariant) kernel since $\gpKerPrior(\Par, \tilde{\Par})$ only depends 
on its arguments through their difference $\Par - \tilde{\Par}$. 
A common approach to set the mean and kernel hyperparameters is the empirical Bayes method of fixing
their values at their maximum (marginal) likelihood estimates, which conveniently retains the Gaussian
predictive distribution in \cref{generic_gp_conditional} at the expense of failing to acknowledge uncertainty
in the hyperparameters. The fully Bayesian alternative significantly increases computation, 
and results in a non-Gaussian predictive distribution after marginalizing the hyperparameters. 

\subsubsection{Multiple Output Gaussian Processes}
We now consider the extension to multi-output functions $\funcPrior: \parSpace \to \R^{\dimObs}$. 
There is a large literature on multi-output kernels \citep{multiOutputKernels}, but we focus here on the 
simplest case in which each output is modeled separately as an independent GP. 
In this setting, the notation $\gpKerPrior(\Par)$ is now interpreted as the $\dimObs \times \dimObs$ 
diagonal matrix collecting the variance of each independent GP. Similarly, for a set of inputs 
$\parMat \in \R^{M \times \dimPar}$, $\gpKerPrior(\parMat)$ is now the 
$M\dimObs \times M\dimObs$ block diagonal matrix constructed from the covariances of 
each independent GP.
 
This basic multi-output strategy becomes problematic when the output dimension $\dimObs$ 
is large or the outputs are highly interdependent, rendering the independent GP approach infeasible or ill-advised.
A popular alternative is to approximate the function output as a linear combination of a small number
of basis vectors, with the coefficients modeled as independent GPs; that is, 
\begin{align}
\funcPrior(\Par) &= \sum_{\idxBasis=1}^{\dimBasis} \basisWeight_{\idxBasis}(\Par) \basisVec_{\idxBasis} + \noise_{\basisVec} \label{basis_func_model} \\
\basisWeight_{\idxBasis} &\overset{\textrm{ind}}{\sim} \GP(\emMean[0]{(\idxBasis)}, \emKer[0]{(\idxBasis)}), \label{basis_func_GPs} \\
 \noise_{\basisVec} &\sim \Gaussian(0, \sigma^2_{\basisVec}), \label{basis_func_noise}
\end{align}
where the $\basisVec_{\idxBasis}$ are fixed basis vectors.
These basis vectors are commonly constructed via a singular value decomposition, though other 
bases are also possible \citep{HigdonBasis, emulate_functional_output, functionValuedModels, PODemulation}.
Under this model, the univariate GPs $\basisWeight_{\idxBasis}$ are fit and conditioned independently, then 
plugged back into \cref{basis_func_model} to construct the predictive distribution. The linear Gaussian 
structure of \cref{basis_func_model} implies that this approach retains a Gaussian predictive distribution
for $\funcEm[\Ndesign]$, conditional on the hyperparameters.

% Initial Design and Surrogate Models
\section{Initial Design and Surrogate Models}
We begin our review by summarizing the surrogate modeling workflow in the context of solving
Bayesian inverse problems. We highlight commonly-used surrogates, emphasizing
models that provide a probabilistic representation of emulator uncertainty in addition to 
standard point predictions. We conclude the section with discussions of the tradeoffs
between the forward and log-density emulation strategies. Our aim is not to investigate any specific 
model in depth; see \cref{sec:existence} for broad practical recommendations for 
constructing surrogate models for use in posterior approximation.

\subsection{Initial Design}
Before discussing specific emulators, we comment on the general process of surrogate fitting.
We focus exclusively on a modular workflow \citep{modularization}, such that the surrogate is
 trained only on data produced
by the simulator, as opposed to jointly learning the surrogate with the parameters in a 
unified Bayesian model (e.g., as in the seminal work \citet{KOH}). Fitting the surrogate requires 
generating training data by evaluating the simulator at a chosen set of input parameters
$\designIn \Def \{\Par_1, \dots, \Par_{\Ndesign}\}$. We refer to $\designIn$ as the \textit{initial design}.
Depending on which map is being emulated, the training data for the emulator will take the form 
$\{\designIn, \fwd(\designIn)\}$, $\{\designIn, \llik(\designIn)\}$, or $\{\designIn, \lpost(\designIn)\}$. 
In the computer experiments literature, 
the initial design is typically selected to satisfy some sort of ``space-filling'' criterion; canonical examples 
include latin hypercube designs and Sobol sequences (\todo: cite). In the present setting, a natural 
approach is to sample $\designIn$ from the prior $\priorDens$, either via simple Monte Carlo or using 
algorithms that seek to minimize a specific notion of discrepancy \citep{supportPoints, SteinPoints}.
As we will demonstrate in experiments, fitting emulators on prior designs may simultaneously produce 
reasonable approximations in a prior-averaged sense, and poor approximations in a posterior-averaged
sense. This is due to a common phenomenon in Bayesian inverse problems, whereby the posterior is highly 
concentrated relative to the prior. Even in parameter spaces of moderate dimension, it is common that 
the initial design may contain few or no points in the region of significant posterior mass. This is one of the
central challenges of surrogate modeling in this setting, and motivates the use of sequential design algorithms
to iteratively construct the design in order to target high-posterior regions (see \Cref{sec:seq_design}).

Another approach to this problem is to first run an alternative algorithm (e.g., an approximate 
sampler or optimizer) to produce design points in regions of high posterior mass, which can then 
be used as part of an initial design to fit an emulator. This general strategy is exemplified by the 
\textit{calibrate, emulate, sample} workflow \citep{CES,idealizedGCM,CESSoftware,FATES_CES}, 
which uses Ensemble Kalman methods to produce the initial design. In similar spirit, the earlier work 
\citet{emPostDens} utilizes a sequence of importance sampling steps to construct a Gaussian 
approximation of the posterior, which is then sampled from to produce an initial surrogate design.
We do not discuss these methods further in this review, but note that continued methodological 
development investigating such workflows may prove a fruitful avenue for future research. 

\subsection{Probabilistic Surrogates}
Our focus in this review is on probabilistic (i.e., random/stochastic) surrogates, which 
are emulator models that provide a probabilistic description of approximation uncertainty.
We review common models falling within this category, and highlight the distinction
between parametric and nonparametric surrogates. 

\subsubsection{Common Models}

\paragraph{Gaussian Process.}
Gaussian processes (GP) have emerged as a canonical surrogate model, and provide the 
basis for examples given throughout this review (refer to \Cref{gp_review} for GP
background and notation). GPs have been extensively applied in a wide variety of 
surrogate modeling tasks, including as emulators for solving Bayesian inverse problems.
The Gaussian predictive distribution of a 
GP facilitates many convenient closed-form computations, which we will derive and present 
throughout this review. Extensions of the GP methodology can produce more flexible 
emulators with non-Gaussian predictive distributions. For example, fully Bayesian 
GPs \citep{fullyBayesianGPs} and deep GPs \citep{deepGPVecchia,deepGPAL}
typically yield predictions in the form of (infinite) mixtures of Gaussians.

\paragraph{Polynomial Chaos Expansion.}

Polynomial chaos expansions (PCE) represent a popular alternative to GPs, particularly in
the engineering and applied math communities \citep{polyChaosReview,MarzoukPolyChaos}
(\todo: add a couple sentences of description here).

\paragraph{Probabilistic Neural Networks.}
In general, any parametric statistical model fit using Bayesian methods yields a probabilistic
surrogate. This generic perspective is considered in \citep{BurknerSurrogate}, where the 
authors assume only that samples can be drawn from the posterior distribution over 
surrogate parameters. It should be noted, however, that this assumption does not naturally 
encompass nonparametric emulator models. For example, drawing samples from GPs 
generally requires sampling infinite-dimensional functions, and thus in practice requires 
discretization or other approximations.
\footnote{This computational challenge has been extensively studied in various fields, including
Bayesian inverse problems \citep{dimRedPolyChaos,functionSpaceMCMC} 
and Bayesian optimization \citep{pathwiseConditioning,samplingGPPosts}. 
Common approaches utilize low-rank approximations, which approximate GPs using a 
finite-dimensional parametric model.} In the nonparametric setting, a generic view of a 
surrogate model is as a map from finite sets of inputs $\parMat$ to a predictive distribution 
over the associated responses $\func(\parMat)$. This viewpoint has formed the basis 
for the generic implementation of surrogate models in software packages such as 
BoTorch \citep{botorch}, which provides support for both nonparametric and non-Gaussian surrogates. 

There has also been growing interest in models with predictive distributions not necessarily 
rooted in the Bayesian philosophy. This includes neural network based models such 
as deep ensembles, which summarize uncertainty via an ensemble of neural network 
models \citep{deepEnsembles}. Various other methods have also been proposed to 
incorporate a notion of predictive uncertainty within neural networks \citep{epistemicNN}.
Of particular note are conformal prediction methods, which allow for the construction of 
pointwise prediction intervals for models that might otherwise only produce point 
estimates \citep{conformalSurrogate,conformalGP,conformalTwoStageDesign,conformalEvidentialSurrogate,conformalBayesOpt}.
[\todo{add a few sentences on conformal prediction}]

\subsubsection{Parametric vs. Nonparametric Models}
The distinction between parametric and nonparametric surrogate models will 
prove significant at several points throughout this review. We illustrate this
distinction for a generic random surrogate $\funcEm[\Ndesign]$ for some 
function $\func$. 

\paragraph{Parametric Model.}
We refer to $\funcEm[\Ndesign]$ as parametric 
or finite-dimensional if the randomness in $\funcEm[\Ndesign]$ stems from 
a finite-dimensional parameter $\theta_{\Ndesign}$; i.e., 
$\funcEm[\Ndesign](\cdot) = g(\cdot; \theta_{\Ndesign})$ for some 
random vector $\theta_{\Ndesign}$ and non-random function $g$.
In this case, it is feasible to sample trajectories (sample paths) of 
$\funcEm[\Ndesign]$ via 
\begin{align}
&\func_{\mathrm{samp}}(\cdot) \Def g(\cdot, \theta_{\mathrm{samp}}),
&&\theta_{\mathrm{samp}} \sim \mathrm{law}(\theta_{\Ndesign}).
\end{align}
The sampled function $\func_{\mathrm{samp}}(\cdot)$ can now 
be evaluated at any input value. A standard example is a linear 
regression model of the form
\begin{align}
\funcEm[\Ndesign](\cdot) &= \sum_{\idxBasis=1}^{\dimBasis} (\theta_{\Ndesign})_{\idxBasis} \basisVec_{\idxBasis}.
\label{eq:finite-basis}
\end{align}
for some fixed basis vectors $\basisVec_{1}, \dots, \basisVec_{\dimBasis}$.
 
\paragraph{Nonparametric Model.} For a nonparametric model, no such finite-dimensional 
parameter $\theta_{\Ndesign}$ exists. In this setting, a generic view of a 
surrogate model is as a map from finite sets of inputs $\parMat$ to a predictive distribution 
over the associated responses $\func(\parMat)$. This viewpoint has formed the basis 
for the generic implementation of surrogate models in software packages such as 
BoTorch \citep{botorch}. It is therefore feasible to sample the finite-dimensional distributions
$\funcEm[\Ndesign](\parMat)$ for finite input sets $\parMat$, but sampling trajectories
$\func_{\mathrm{samp}}(\cdot) \sim \mathrm{law}(\funcEm[\Ndesign])$ would require 
infinite computing resources. Practical techniques to sample approximate trajectories have been
extensively studied when $\funcEm[\Ndesign]$ is a GP, including work in 
Bayesian inverse problems \citep{dimRedPolyChaos,functionSpaceMCMC} 
and Bayesian optimization \citep{pathwiseConditioning,samplingGPPosts}.
Common approaches construct finite-rank approximations of the form  
\Cref{eq:finite-basis}.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth} % GP dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/fwd_dist_fwdem.png}}
         \caption{$\fwdEm[\Ndesign]$}
         \label{fig:fwd_dist_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % llik dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/llik_dist_fwdem.png}}
         \caption{$\llik(\cdot; \fwdEm[\Ndesign])$}
         \label{fig:llik_dist_fwd}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % lik dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lik_dist_fwdem.png}}
         \caption{$\Exp{\llik(\cdot; \fwdEm[\Ndesign])}$}
         \label{fig:lik_dist_fwdem}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth} % GP dist [llik] (equal to llik dist so excluding)
     \includegraphics[width=\textwidth]{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % llik dist [llik] 
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/llik_dist_llikem.png}}
         \caption{$\llikEm[\Ndesign]$}
         \label{fig:llik_dist_llikem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % lik dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lik_dist_llikem.png}}
         \caption{$\Exp{\llikEm[\Ndesign]}$}
         \label{fig:lik_dist_llikem}
     \end{subfigure}
        \caption{Comparison of forward model and log-likelihood emulation in the simple one-dimensional problem 
        ($\dimPar = \dimObs = 1$) described in $\todo$. The top and bottom rows correspond to the forward model 
        emulation and log-likelihood emulation settings, respectively. The columns correspond to (1) the forward model emulator;
        (2) the log-likelihood emulator induced by the underlying GP; 
        (3) the likelihood emulator induced by the underlying GP. When directly emulating the 
        log-likelihood, the underlying GP is itself the log-likelihood emulator, and there is no forward model emulator.
        The black line is the ground truth and the red points are the design points used for emulator training. 
        The blue line is the mean of the respective emulators, and the shaded area corresponds to 90\% credible 
        intervals for the predictive distributions.}
        \label{fig:em_dist_1d}
\end{figure}

\subsection{Forward Model Emulation}
We now begin our discussion of emulation strategies geared towards the goal of Bayesian inversion.
One natural approach is to fit an emulator approximating the forward model map $\Par \mapsto \fwd(\Par)$,
which entails fitting a predictive model to a design $\{\designIn, \fwd(\designIn)\}$. 
We let $\fwdEm[\Ndesign]$ denote a stochastic surrogate that has been fit to this design,
thus representing a distribution over functions that summarizes the uncertainty about the 
numerically-inaccessible function $\fwd$. We let $\emMean[\Ndesign]{\fwd}(\Par)$ and 
$\emKer[\Ndesign]{\fwd}(\Par)$ denote the mean and variance of the random 
variable $\fwdEm[\Ndesign](\Par)$, even when $\fwdEm[\Ndesign]$ is not a GP.
Plugging the random function $\fwdEm[\Ndesign]$ in place of $\fwd$ in the log-likelihood 
induces random approximations of the likelihood and posterior. In general, we use 
$\llikEm[\Ndesign]$ to denote a stochastic approximation to the log-likelihood, induced
by an underlying emulator fit to $\Ndesign$ design points. We write 
$\llikEm[\Ndesign](\Par) = \llik(\Par; \fwdEm[\Ndesign])$ when it is necessary to specify that
the log-likelihood approximation is induced by a forward model emulator. We similarly use 
the notation $\postEm(\Par) = \postDens(\Par; \fwdEm[\Ndesign])$ and 
$\lpostEm(\Par) = \lpost(\Par; \fwdEm[\Ndesign])$ for the induced unnormalized 
posterior density and log posterior density approximations. The induced distributions for 
these quantities depend on the predictive distribution of the forward model emulator, as
well as the structure of the likelihood function. The first row of \cref{fig:em_dist_1d} 
demonstrates how a GP forward model emulator induces predictive distributions 
for the log-likelihood and likelihood. Gaussian processes and polynomial 
chaos expansions have been extensively employed as forward model emulators in 
practice (\todo: cite). The specific structure of the surrogate, which depends heavily on the 
characteristics of $\fwd$, is not the focus of this review. However, we do note that 
a common challenge in emulating forward models stems from the fact that the observation 
space (i.e., the output space of $\fwd(\Par)$) can be very high-dimensional. 
Such complications commonly arise in settings with spatial or temporal structure, 
such as epidemic modeling \citep{FadikarAgentBased},
engineering design \citep{PODemulation}, ecological forecasting 
\citep{emPostDens,DagonCLM}, and climate modeling \citep{ESM_modeling_2pt0,idealizedGCM}.
One popular approach to deal with this challenge is to approximate the forward model output
as a linear combination of a small number of basis vectors, and then emulate the scalar coefficients 
of these vectors \citep{HigdonBasis,FadikarAgentBased,PODemulation}. Alternatively, surrogates 
can be fit to low-dimensional summaries of the high-dimensional output, such as spatial or 
temporal averages \citep{ESM_modeling_2pt0,idealizedGCM,CLMBayesianCalibration,CLMSurrogates}.
A great deal of other approaches have been proposed, including emulators designed 
specifically for dynamical models \citep{GP_dynamic_emulation, Bayesian_emulation_dynamic, Liu_West_dynamic_emulation, dynamic_nonlinear_simulators_GP}.
Log-density emulation, described in \Cref{log_density_emulation}, offers another avenue for 
dimension reduction when the observation space is high-dimensional.

\subsubsection{Gaussian Setting}
In this section we consider the special case where both the likelihood and $\fwdEm[\Ndesign]$ are Gaussian.
These two distributional assumptions combine to allow closed-form computations, and thus have 
been considered frequently in the literature \citep{StuartTeck1,GP_PDE_priors,hydrologicalModel,Surer2023sequential,VillaniAdaptiveGP,weightedIVAR,idealizedGCM,CES}.

\begin{prop} \label{prop:fwd-em-Gaussian}
Assume that $\fwdEm[\Ndesign]$ is a forward model emulator with Gaussian predictive distribution; 
that is, $\fwdEm[\Ndesign](\Par) \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par))$
for all $\Par \in \parSpace$. Then, in the additive Gaussian likelihood setting, the induced unnormalized 
posterior density surrogate 
$\postDens(\Par; \fwdEm[\Ndesign]) = \priorDens(\Par)\Gaussian(\obs| \fwdEm[\Ndesign](\Par), \likPar)$
has mean and variance given by
\begin{align*}
\E_{\fwdEm}\left[\postDens(\Par; \fwdEm[\Ndesign]) \right] 
&= \priorDens(\Par) \Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par)) \\
\Var_{\fwdEm}\left[\postDens(\Par; \fwdEm[\Ndesign]) \right]
&= \priorDens^2(\Par) \left[\frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)  \right)}{\det(2\pi \likPar)^{1/2}}
- \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\left[\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right]  \right)}{\det(2\pi [\likPar + \emKer[\Ndesign]{\fwd}(\Par)])^{1/2}}\right]
\end{align*}
\end{prop}

See \Cref{prop:Gaussian_marginal_moments} in the appendix for the derivation, which follows from basic facts about Gaussians.
The canonical example of a Gaussian surrogate is a GP, which is defined by specifying a prior distribution  
\begin{equation}
\fwdPrior \sim \GP(\emMeanPrior{\fwd}, \emKerPrior{\fwd}).
\end{equation}
for $\fwd$. Conditioning on the design yields the predictive distribution 
$\fwdEm[\Ndesign]  \Def \fwdPrior|[\fwdPrior(\designIn) = \fwd(\designIn)] \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$, 
which constitutes the forward model emulator.
Unless otherwise indicated, we assume the mean and kernel hyperparameters are
fixed, so that the GP predictive distribution remains Gaussian. This is a common assumption in the literature, 
and facilitates the presentation of analytical expressions (such as \Cref{prop:fwd-em-Gaussian})
that aid intuition. We will make clear when such assumptions are being 
leveraged, and later highlight methodology that is agnostic to the predictive distribution of the surrogate model.

In the typical case that the observation space has dimension $\dimObs > 1$, then we interpret 
$\emMeanPrior{\fwd}, \emKerPrior{\fwd}$ as multi-output mean and covariance functions, respectively.
If $\dimObs$ is reasonably small, then it may be appropriate to simply fit an independent GP to 
each scalar output variable. However, if $\dimObs$ is large or the outputs are highly structured, 
then the independent GP approach may be infeasible.  
In such contexts, a popular 
alternative is to leverage the basis reduction strategy in \cref{basis_func_model,basis_func_GPs,basis_func_noise}.
The surrogate predictive distribution remains Gaussian under this model, and thus \cref{prop:fwd-em-Gaussian} still applies in this setting. The particular setup of a Gaussian likelihood with a GP forward model emulator using the basis vector approach is explored in \citet{Surer2023sequential}. 

\subsection{Log-Density Emulation} \label{log_density_emulation}
Observe in \cref{post_dens} that the expensive forward model evaluations appear in the posterior density only through the 
likelihood. Thus, for the purposes of posterior inference, it is sufficient to directly approximate either
$\Par \mapsto \llik(\Par)$ or $\Par \mapsto \lpost(\Par)$ without obtaining an approximation of the forward model.
This implies fitting a surrogate model to the design $\{\designIn, \llik(\designIn)\}$ or $\{\designIn, \lpost(\designIn)\}$,
which results in emulators $\llikEm[\Ndesign]$ and $\lpostEm[\Ndesign]$, respectively.
As with forward model emulators, we generically use $\emMean[\Ndesign]{\llik}(\Par)$ and $\emKer[\Ndesign]{\llik}(\Par)$ 
to denote the mean and variance of $\llikEm[\Ndesign](\Par)$. We likewise write $\emMean[\Ndesign]{\lpost}(\Par)$ and 
$\emKer[\Ndesign]{\lpost}(\Par)$ for the mean and variance of $\lpostEm[\Ndesign](\Par)$.
We refer to these two approaches collectively as \textit{log-density emulation}.
As with forward model surrogates, log-density surrogates induce a random approximation $\postEm[\Ndesign]$ of the 
unnormalized posterior density; we write $\postDens(\Par; \llikEm[\Ndesign])$ and $\postDens(\Par; \lpostEm[\Ndesign])$
when it is necessary to emphasize the quantity that is directly emulated.
The second row of \cref{fig:em_dist_1d} demonstrates how a GP log-likelihood emulator induces a predictive 
distribution for the likelihood.
The log-likelihood emulation approach is considered in 
\citet{VehtariParallelGP,FATES_CES,trainDynamics,quantileApprox,ActiveLearningMCMC,FerEmulation,
StuartTeck1,random_fwd_models,GP_PDE_priors,OakleyllikEm}.
Log-likelihood emulation has also been utilized for Bayesian quadrature for numerical 
integration \citep{BayesQuadrature,BayesQuadRatios} and approximate Bayesian 
computation \citep{llikEmABC}.
The closely-related unnormalized log-posterior density (i.e., joint parameter-data density) emulation
strategy is used in \citet{emPostDens,Kandasamy_2017,llikRBF,gp_surrogates_random_exploration,landslideCalibration}.
We colloquially refer to this latter approach as \textit{log-posterior emulation} for succinctness.
A related method is used in \citep{wang2018adaptive,adaptiveMultimodal}, in which surrogates are constructed 
to approximate a sequence of functions designed to be more regular than the log-likelihood.   
In general, emulating densities on the log scale is typically preferred as a way to improve numerical stability,
enforce non-negativity in the density approximation, and yield a smoother target function for emulation.
Perhaps the most significant benefit of this approach is the reduction to a scalar-valued output quantity for emulation,
as opposed to the potentially high-dimensional output space of the forward model. This notion is referred 
to as \textit{scalarization} in \citet{ranjan2016inverse, trainDynamics}.
On the other hand, log-density emulation has several downsides. Even on the log scale, log-densities can be fast-varying and 
exhibit large dynamic range, proving troublesome for surrogate models that assume stationarity \cite{wang2018adaptive}.
A simple toy example is used in \citep{Surer2023sequential} to demonstrate such modeling challenges, and the 
authors conclude that forward model emulation is preferred in their application of interest. A second potential weakness 
stems from the fact that log-densities often have known bound constraints, which may be difficult to enforce 
for certain surrogate models. The recent work \citet{quantileApprox} investigates the enforcement of 
bound constraints in GP-based log-likelihood emulation. In our numerical experiments, we demonstrate that ignoring
such constraints can yield very poor posterior approximations. A third challenge follows from the fact that, in many applications, 
the likelihood parameters (e.g., $\likPar$ in \cref{inv_prob_Gaussian}) are typically not known and must also be 
learned from data. An obvious solution is to extend the input space of the emulator to include the likelihood parameters
\citep{llikRBF,emPostDens}, but the resulting response surface may prove more challenging to emulate. This approach also increases both the 
input dimensionality of the emulator, as well as the number of design points required to achieve 
a reasonable fit. Various approaches are explored in \citet{llikRBF} for dealing with nuisance likelihood parameters
without requiring additional runs of the expensive simulator. Alternatively, specific likelihood 
choices may admit a sufficient statistic which can be emulated independently of the likelihood parameters. 
\Cref{ldens_Gaussian} provides such an example for multiplicative Gaussian likelihoods, following the 
approach of \citet{FerEmulation}.

\subsubsection{Log-Likelihood vs. Log-Posterior Emulation} \label{sec:llik_vs_lpost}
We take a moment to comment on the the distinction between emulating the log-likelihood versus
the log-posterior. The literature summary above indicates that both approaches are commonly used;
we are not aware of any cases in which the performance the two methods is compared. 
Indeed, the choice appears inconsequential, as one emulator can easily be converted into the other 
by adding or subtracting the log-prior density, which represents a deterministic shift to the predictive 
distribution. For example, given a fit log-likelihood emulator $\llikEm[\Ndesign]$, a log-posterior emulator 
can be constructed via
\begin{equation}
\lpostEm[\Ndesign](\Par) \Def \llikEm[\Ndesign](\Par) + \log \priorDens(\Par),
\end{equation}
which simply adjusts the predictive mean. Despite the similarity, we see one potential benefit of emulating 
the log-posterior from a modeling standpoint: the tails of the log-posterior must decay. This
qualitative information can be leveraged in designing an appropriate surrogate model for
$\lpost$. The tail behavior of $\llik$ is not always well-known, which can present more of a 
modeling challenge. These considerations are explored in detail in 
\Cref{sec:existence}, which discusses the design of surrogate models that induce well-defined 
posterior approximations. 

\subsubsection{Gaussian Setting} \label{ldens_Gaussian}
A Gaussian log-density emulator induces an approximation of the unnormalized posterior 
density with a predictive distribution that can be characterized fully in closed-form. The following 
result is the analog of \Cref{prop:fwd-em-Gaussian} in the forward model 
emulation setting.

\begin{prop} \label{prop:llik-em-Gaussian}
Assume that $\lpostEm[\Ndesign]$ is a log-posterior emulator with Gaussian predictive distribution; 
that is, $\lpostEm[\Ndesign](\Par) \sim \Gaussian(\emMean[\Ndesign]{\lpost}(\Par), \emKer[\Ndesign]{\lpost}(\Par))$
for all $\Par \in \parSpace$. Then the induced unnormalized posterior density surrogate 
$\postEm[\Ndesign](\Par) = \Exp{\lpostEm[\Ndesign](\Par)}$ is given by
\begin{align}
\postEm[\Ndesign](\Par) &\sim \text{LN}\left(\emMean[\Ndesign]{\lpost}(\Par), \emKer[\Ndesign]{\lpost}(\Par)\right),
\end{align}
which implies 
\begin{align}
\E\left[\postEm[\Ndesign](\Par)\right] &= \Exp{\emMean[\Ndesign]{\lpost}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\lpost}(\Par)}, \\
\Var\left[\postEm[\Ndesign](\Par)\right] &= \left[\Exp{\emKer[\Ndesign]{\lpost}(\Par)} - 1\right] \Exp{2\emMean[\Ndesign]{\lpost}(\Par) + \emKer[\Ndesign]{\lpost}(\Par)}.
\end{align}
\end{prop}

The assumptions in \cref{prop:llik-em-Gaussian} are satisfied when either the 
log-likelihood or log-posterior are emulated by GPs. This follows from specifying either of the 
prior distributions
\begin{align}
&\llikEm[0] \sim \GP(\emMean[0]{\llik}, \emKer[0]{\llik}),
&&\lpostEm[0] \sim \GP(\emMean[0]{\lpost}, \emKer[0]{\lpost})
\end{align}
and then defining the surrogates as the conditional processes 
\begin{equation}
\llikEm[\Ndesign] \Def \llikPrior | [\llikPrior(\designIn) = \llik(\designIn)] \sim \GP(\emMean[\Ndesign]{\llik}, \emKer[\Ndesign]{\llik})
\end{equation}
or
\begin{equation} 
\lpostEm[\Ndesign] \Def \lpostEm[0]  | [\lpostEm[0](\designIn) = \lpost(\designIn)] \sim \GP(\emMean[\Ndesign]{\lpost}, \emKer[\Ndesign]{\lpost}),
\end{equation}
respectively. In these cases, the induced unnormalized log-posterior density approximation 
$\postEm[\Ndesign]$ is the exponential of a Gaussian process, which we refer to as a 
\textit{log-normal process}.

Note that \cref{prop:llik-em-Gaussian} is agnostic to the specific form of the 
likelihood, unlike \cref{prop:fwd-em-Gaussian}. However, particular likelihood
structures may be leveraged to enable improved emulation in certain settings. For example, 
\citet{FerEmulation} consider a Gaussian likelihood with block-diagonal covariance structure, yielding
the log-likelihood
\begin{align}
\llik(\Par) 
&= \log \Gaussian(\obs \given \fwd(\Par), \likPar) \nonumber \\
&= - \frac{1}{2} \sum_{j=1}^{J} \left[\dimObs_j \log\left(2\pi\sigma_j^2\right) +
 \frac{1}{\sigma_j^2} \norm{\obs_j - \fwd_j(\Par)}_2^2 \right].
\end{align}
In this case, the maps $\Par \mapsto \norm{\obs_j - \fwd_j(\Par)}_2^2$ can be emulated 
independently of the likelihood parameter $\likPar$, allowing for easier inference of the 
likelihood parameters along with $\Par$.
Approaches such as this can be considered a variation of log-density emulation, or an instance of 
forward model emulation with a particular definition of $\fwd$.
 
\section{Posterior Approximation}
With a fit emulator in hand, we now turn to the question of how best to leverage the surrogate in approximating the 
posterior distribution of the Bayesian inverse problem. We view this as a question of uncertainty propagation. 
In realistic settings with restrictive computational budgets, the surrogate predictive distribution may retain a high degree of 
epistemic uncertainty stemming from the sparsity of the design points; our interest is in constructing posterior 
approximations that incorporate this uncertainty. We note that such approximations 
may serve various downstream goals. If computational constraints prevent the acquisition of additional 
design points, then one must accept a fixed posterior approximation.
In such cases, the surrogate uncertainty should be acknowledged to prevent 
overconfidence when drawing scientific conclusions. In other settings, it may be feasible to refine the surrogate 
via additional queries to the simulator. An approximate posterior constructed using the current 
surrogate may serve as a guide to determine the new input parameters at which the simulator will be 
evaluated. In this case, propagating surrogate uncertainty is key in weighing the exploration vs. exploitation 
tradeoff to select new design points (see \cref{sec:seq_design}).
This uncertainty propagation viewpoint aligns closely with the perspective taken in \citep{BurknerSurrogate}, 
who consider several MCMC-based methods for propagating surrogate uncertainty in a Bayesian framework. 
Alternative lines of research (reviewed in \cref{sec:seq_design}) have instead emphasized iterative schemes 
designed to achieve asymptotic convergence to the exact posterior. The tradeoff is that such approaches 
still typically require a large number of simulator evaluations--often performed serially-- and thus remain 
impractical in settings with computationally-intensive simulators.
Motivated by this fact, our focus in this section is on approximate posterior inference and uncertainty propagation
with a fixed surrogate.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth} % lpost dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lpost_dist_fwdem_trim.png}}
         \caption{$\lpost(\cdot; \fwdEm[\Ndesign])$}
         \label{fig:lpost_dist_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % post norm approx [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/post_norm_approx_fwdem.png}}
         \caption{$\postDensNorm$ approx}
         \label{fig:post_norm_approx_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % lpost dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lpost_dist_llikem_trim.png}}
         \caption{$\lpost(\cdot; \llikEm[\Ndesign])$}
         \label{fig:lpost_dist_llikem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % post norm approx [llik] 
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/post_norm_approx_llikem.png}}
         \caption{$\postDensNorm$ approx}
         \label{fig:post_norm_approx_llikem}
     \end{subfigure}
        \caption{A continuation of the example in \cref{fig:em_dist_1d}. The top and bottom rows correspond
        to the forward model and log-likelihood emulation settings, respectively. The first column summarizes
        the distributions of the log-posterior approximations induced by the underlying emulators, with 
        the predictive mean in blue and the shaded region containing 90\% of the mass. The second column 
        plots compares various (normalized) posterior approximations derived from the log-posterior 
        emulator. Note that even though  the emulators interpolate the design points, the posterior
        approximations do not owing to the normalization.}
        \label{fig:post_norm_approx_1d}
\end{figure}

\subsection{Plug-In Mean}
Before discussing methods to propagate surrogate uncertainty, we establish the baseline method of simply 
ignoring this uncertainty. A deterministic emulator can be defined by utilizing only the predictive mean function
of the random surrogate model. Plugging the predictive mean in place of the quantity it is emulating induces 
a deterministic approximation $\postApproxNormMean[\Ndesign]$ of the posterior distribution 
$\postDensNorm$, which we refer to as the \textit{plug-in mean}, or simply \textit{mean}, approximation. 
In the forward model and log-likelihood emulation settings, this implies the unnormalized density approximations 
\begin{align}
&\postApproxMean[\Ndesign](\Par; \fwdEm[\Ndesign]) \Def \postDens(\Par; \emMean[\Ndesign]{\fwd}), \label{eq:mean-approx-fwd}
&&\postApproxMean[\Ndesign](\Par; \llikEm[\Ndesign]) \Def \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par)}, \label{eq:mean-approx_llik}
\end{align}
with the normalizing constants $\normCstEm(\emMean[\Ndesign]{\fwd})$ and 
$\normCstEm(\emMean[\Ndesign]{\llik})$ defined by integrating the unnormalized densities over $\parSpace$
(existence of these integrals is discussed in \cref{sec:existence}).
The mean approximation has been applied in various contexts
\citep{VehtariParallelGP,trainDynamics,emPostDens,BurknerSurrogate,CLMBayesianCalibration} and analyzed
theoretically 
\citep{StuartTeck1,StuartTeck2,random_fwd_models,TeckHyperpar,gp_surrogates_random_exploration}.
If the emulator predictive mean is known to be highly accurate then this approximation may be reasonable,
but in general ignoring the surrogate uncertainty can lead to posterior approximations that are 
both inaccurate and overly confident \citep{BurknerSurrogate}. 

\subsection{Expected Posterior}
We now transition to focus on posterior approximations that incorporate the surrogate uncertainty.
Consider a random surrogate $\postEm[\Ndesign]$ for the unnormalized posterior density, 
induced by some underlying emulator model (e.g., a forward model or log-density emulator).
We will use the notation $\E_{\Ndesign}$ to indicate expectations with respect to the 
distribution of the underlying surrogate model; e.g., the law of $\fwdEm[\Ndesign]$ or $\lpostEm[\Ndesign]$.
By normalizing $\postEm[\Ndesign]$, we obtain a random density
\footnote{More generally, this is a random measure. We work with densities to avoid measure-theoretic technicalities.}
\begin{align}
&\postNormEm[\Ndesign](\Par) 
\Def \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \postEm[\Ndesign](\Par),
&&\normCstEm[\Ndesign]\Def \int_{\parSpace} \postEm[\Ndesign](\Par) d\Par
\label{eq:sample-approx}
\end{align}
which summarizes the uncertainty about the true posterior. 
While $\postEm[\Ndesign](\Par)$ depends 
only on the univariate surrogate prediction at $\Par$, $\llikEmRdm[\Ndesign]{\postDensNorm}(\Par)$
depends on the entire random function $\llikEmRdmDens[\Ndesign]$ due to its dependence on 
the (random) normalizing constant $\llikEmRdm[\Ndesign]{\normCst}$. The quantity
$\llikEmRdm[\Ndesign]{\postDensNorm}$ is referred to as the \textit{sample} approximation in
\citet{StuartTeck1, StuartTeck2,random_fwd_models,TeckHyperpar}.
To construct a deterministic approximation to $\postDens$, we can consider the expectation
\begin{equation}
\postApproxEP[\Ndesign](\Par) \Def \E_{\Ndesign}\left[\postNormEm[\Ndesign](\Par) \right], \label{eq:ep-approx}
\end{equation}
which we refer to as the \textit{expected posterior (EP)}, following the terminology in \citet{BurknerSurrogate}.
Under a nonparametric surrogate model (e.g., a GP), the expectation in \cref{eq:ep-approx} is with 
respect to an infinite-dimensional random element, and thus its existence is a non-trivial matter \citep{StuartTeck1}.
Assuming the expectation exists, the Monte Carlo scheme outlined in \cref{alg:ep} could in principle 
be used to draw samples from $\llikEmSampDensNorm$. 

\begin{algorithm}
    \caption{Direct sampling from $\llikEmSampDensNorm$}
    \label{alg:ep}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{sampleEP}{$\llikEmRdm[\Ndesign]{\postDensNorm}, \NSample, M$}     
        \For{$\sampleIndex \gets 1, \dots, \NSample$} 
        		\State $\llikEmRdm[\Ndesign]{\postDensNorm}^{(\sampleIndex)} \sim \law(\llikEmRdm[\Ndesign]{\postDensNorm})$ \Comment{Sample posterior trajectory}
		\State $\Par^{(\sampleIndex, 1)}, \dots, \Par^{(\sampleIndex, M)} \overset{iid}{\sim} \llikEmRdm[\Ndesign]{\postDensNorm}^{(\sampleIndex)}}$ \Comment{Sample parameters}
	\EndFor
	\State \Return $\{\Par^{(\sampleIndex, m)}\}_{1 \leq \sampleIndex \leq \NSample, \ 1 \leq m \leq M}$
	\EndFunction
    \end{algorithmic}
\end{algorithm}

A practical implementation 
would entail first sampling a trajectory of the surrogate model, which induces a sample 
trajectory of the unnormalized random density $\llikEmRdmDens[\Ndesign]$. Parameter samples
could then be drawn from the distribution implied by this unnormalized density using standard 
methods (e.g., MCMC).
The output of the algorithm thus represents a mixture over an 
ensemble of posterior distributions. Choosing $M = 1$  (drawing one sample per
trajectory of $\postNormEm[\Ndesign]$) implies that \cref{alg:ep} returns independent 
samples from $\postApproxEP[\Ndesign]$. For $M > 1$, the samples will be dependent.
For example, in the extreme case $\NSample = 1$ they will 
still have the correct marginal distribution $\postApproxEP[\Ndesign]$, but will 
only provide a summary of a single trajectory from $\postNormEm[\Ndesign]$ and thus 
in general provide a poor representation of this random density.

\Cref{alg:ep} is readily implementable in practice provided the ability to sample trajectories 
of $\postEm[\Ndesign]$, which will generally be possible for finite-dimensional 
parametric surrogate models. \citet{garegnani2021NoisyMCMC} and \citet{BurknerSurrogate}
both investigate \cref{alg:ep} in such settings. Under certain assumptions, the former paper 
bounds the Monte Carlo error in the sample-based approximation of $\postApproxEP[\Ndesign]$.

With a nonparametric emulator, the practical difficulty in 
implementing \cref{alg:ep} is the requirement to sample an infinite-dimensional 
trajectory from the surrogate model.
These difficulties are noted in \citet{VehtariParallelGP} as justification for pursuing alternative 
approaches. As far as we are aware, \citet{trainDynamics} is the only work to attempt an 
implementation of \cref{alg:ep} in the nonparametric context. Their method consists of approximating GP 
trajectories by sampling the surrogate only at a finite grid of points, and then approximating 
the trajectory as the GP mean, conditional on the sampled values at these points.
We note that an alternative approach could involve constructing a finite-dimensional approximation 
of the surrogate in order to provide the practical means to simulate (approximate) trajectories 
of $\postEm[\Ndesign]$. In the GP setting, approaches such as the Karhunen-Loeve expansion 
and random Fourier features could be leveraged for this purpose \citep{dimRedPolyChaos,samplingGPPosts}. 
As with the \citet{trainDynamics} method, this approach is only an approximate implementation of \cref{alg:ep}.

\subsection{Expected Likelihood}
Given the challenge of implementing \cref{alg:ep} with a nonparametric emulator, the majority of 
previous literature instead proposes deterministic approximations of the unnormalized
density. As opposed to the expected posterior method, these approximations are computed 
pointwise in $\Par$. This is a potential downside of this approach, as it does not leverage 
the full structure of the surrogate predictive distribution (e.g., the predictive covariance of a GP emulator).
However, the focus on unnormalized density approximation has several practical benefits, 
including that it does not require approximating surrogate trajectories, and is amenable to the application of 
standard Bayesian inference procedures using a single MCMC run.

We start by considering the approximation derived by computing the pointwise expectation 
of $\postEm(\Par)$ and then normalizing after-the-fact.
This yields the unnormalized density estimate
\begin{align}
\postApproxMarg[\Ndesign](\Par) \Def \E_{\Ndesign}\left[\postEm[\Ndesign](\Par) \right], \label{eq:post-approx-EL}
\end{align}
which we refer to as the \textit{expected likelihood (EL)}
\footnote{Note that we are technically considering expectations of the unnormalized posterior density approximation 
$\postEm[\Ndesign](\Par)$, but this is equivalent to taking expectations of the likelihood approximation.}
approximation, again borrowing terminology from
 \citet{BurknerSurrogate}. \citet{StuartTeck1} instead refer to this as the \textit{marginal} approximation, 
 and show the normalizing constant implied by \cref{eq:post-approx-EL} is given by
 \begin{equation}
 \int_{\parSpace} \E_{\Ndesign}\left[\postEm[\Ndesign](\Par) \right] d\Par 
 = \E_{\Ndesign} \int_{\parSpace} \postEm[\Ndesign](\Par) d\Par
 = \E_{\Ndesign}\left[\normCstEm \right],
 \end{equation} 
meaning the expected likelihood approximation is a ratio estimator of the form 
 $\postApproxNormMarg[\Ndesign](\Par) = \E_{\Ndesign}[\postEm[\Ndesign](\Par)] / \E_{\Ndesign}[\normCstEm]$.
 Note the difference with respect to the expected posterior approximation 
 $\postApproxEP[\Ndesign](\Par) = \E_{\Ndesign}[\postEm[\Ndesign](\Par) / \normCstEm]$, 
 which we highlight in \cref{tab:post-approx-comparison} in the forward model emulation setting.  
 The expected likelihood approximation $\postApproxMarg[\Ndesign]$ may be justified from a Bayesian
 decision-theoretic viewpoint as the minimizer of an $L^2$ risk. However, this does not 
 confer the same optimality on the normalized approximation $\postApproxNormMarg[\Ndesign]$;
 see \citep{SinsbeckNowak,StuartTeck2,VehtariParallelGP} for details. To our knowledge, the papers
 \citep{SinsbeckNowak,StuartTeck1} are two of the first to consider the expected likelihood approximation
 to propagate surrogate uncertainty.
 
 If the pointwise expectation 
 $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$ is computable in closed-form, then 
the expected likelihood approximation can be sampled using standard MCMC software.
In cases where this expectation is intractable, pseudo-marginal MCMC 
\citep{pseudoMarginalMCMC} may be employed, so long as 
samples can be drawn from the surrogate predictive distribution $\postEm[\Ndesign](\Par)$
at any input $\Par$. \citet{garegnani2021NoisyMCMC} investigates the pseudo-marginal 
approach, as well as the closely-related Monte Carlo within Metropolis-Hastings (MCwMH)
method \citep{noisyMCSurvey,stabilityNoisyMH}. MCwMH does not sample from 
$\postApproxNormMarg[\Ndesign]$ exactly, but can achieve 
much faster mixing relative to the pseudo-marginal method \citep{noisyMCMC}.
We discuss these methods, and related generalizations, further in \cref{sec:MH-approx}.

\citet{BurknerSurrogate} alternatively propose to replace  $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$
with a Monte Carlo estimate derived from surrogate samples. Their method differs from the 
pseudo-marginal and MCwMH algorithms in that the Monte Carlo samples are simulated once 
and then fixed throughout the MCMC run. It therefore allows for the application of standard 
MCMC methods, but only targets an approximation to $\postApproxNormMarg[\Ndesign]$.
This approach is well-suited to finite-dimensional surrogate models, but is not directly 
applicable in the nonparametric setting. 
In the nonparametric case, most previous literature 
has focused on the Gaussian settings from \cref{prop:fwd-em-Gaussian} and \cref{prop:llik-em-Gaussian}
due to their analytic tractability. We summarize these special cases below.

\subsubsection{Gaussian Setting: Forward Model Emulator}
\begin{prop} \label{prop:post-approx-fwd-EL-Gaussian}
Consider the setting in \cref{prop:fwd-em-Gaussian}, where both the forward model emulator 
and likelihood are Gaussian. Under these assumptions, the expected likelihood approximation
is given by
\begin{align}
\postApproxMarg[\Ndesign](\Par) 
&= \priorDens(\Par) \Gaussian\left(\obs \given \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par) \right). \label{eq:post-approx-fwd-EL-Gaussian}
\end{align}
\Cref{eq:post-approx-fwd-EL-Gaussian} can be seen to be the unnormalized posterior corresponding to the modified
Bayesian inverse problem
\begin{align}
&\obs = \emMean[\Ndesign]{\fwd}(\Par) + \eta_{\Ndesign}(\Par) + \noise
&&\noise \sim \Gaussian(0, \likPar)  \label{inv_prob_Gaussian_modified} \\
&\eta_{\Ndesign}(\Par) \sim \Gaussian(0, \emKer[\Ndesign]{\fwd}(\Par))
&&\Par \sim \priorDens  \nonumber
\end{align}
where $\noise$, $\Par$, and $\eta_{\Ndesign}(\Par)$ are pairwise a priori independent for all $\Par$.
\end{prop}

The approximate likelihood retains a Gaussian form, where the surrogate predictive mean
$\emMean[\Ndesign]{\fwd}$ has replaced the true forward model $\fwdEm[\Ndesign]$. The surrogate uncertainty
is incorporated via the addition of $\emKer[\Ndesign]{\fwd}(\Par)$ to the noise covariance $\likPar$.
We emphasize that even if the prior $\priorDens$ is Gaussian, the posterior approximation will typically be non-Gaussian
due to the nonlinearity of $\emMean[\Ndesign]{\fwd}(\Par)$ and the fact that the likelihood covariance depends on 
$\Par$ through $\emKer[\Ndesign]{\fwd}(\Par)$. The modified inverse problem viewpoint 
in \cref{inv_prob_Gaussian_modified} is noted in \citet{SinsbeckNowak} and \citet{StuartTeck1}; the former shows that
this viewpoint extends beyond the Gaussian setting, even when analytical expressions for $\postApproxMarg[\Ndesign](\Par) $ 
may not exist.
The closed-form expression in \cref{eq:post-approx-fwd-EL-Gaussian} is also noted in both
\citet{SinsbeckNowak, StuartTeck1} and has been used in many subsequent studies 
\citep{VehtariParallelGP,weightedIVAR,StuartTeck2,GP_PDE_priors,CES,idealizedGCM,
villani2024posteriorsamplingadaptivegaussian,hydrologicalModel}. 

\subsubsection{Gaussian Setting: Log-Density Emulator}
A closed form expression for $\postApproxMarg[\Ndesign](\Par)$ is available when using a log-likelihood
or log-posterior emulator with a Gaussian predictive distribution. We present the result for a log-likelihood 
emulator; in the log-posterior case the prior density term is removed.

\begin{prop} \label{prop:post-approx-llik-EL-Gaussian}
Consider the setting in \cref{prop:llik-em-Gaussian} with a Gaussian log-likelihood emulator. The 
expected likelihood approximation is then given by the unnormalized density 
\begin{align}
\postApproxMarg[\Ndesign](\Par) 
&= \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)}
= \postApproxMean(\Par) \Exp{\frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)}. \label{eq:post-approx-llik-EL-Gaussian}
\end{align}
\end{prop}

The density in \cref{eq:post-approx-llik-EL-Gaussian} can be viewed as inflating the mean approximation 
at points where the emulator is uncertain. 
It is notable that the uncertainty inflation factor $\Exp{\frac{1}{2}\emKer[\Ndesign]{\lpost}(\Par)}$ scales 
very quickly as the surrogate variance increases.
In practice, this sensitivity to the predictive variance can lead $\postApproxNormMarg[\Ndesign]$ to be highly 
multimodal, or heavily concentrated in small regions with high predictive variance. 
This pathology is noted in \cite{VehtariParallelGP}, who recommend against the use of the expected
likelihood approximation in their application of interest (log-likelihood emulation with noisy log-likelihood evaluations). 
This posterior approximation is analyzed theoretically in \citet{StuartTeck1,StuartTeck2,GP_PDE_priors,random_fwd_models,TeckHyperpar}.

\begin{table}[h]
\centering
\begin{tabular}{>{\centering\arraybackslash}p{4cm} >{\centering\arraybackslash}p{5cm} >{\centering\arraybackslash}p{5cm}}
\toprule
\textbf{Plug-In Mean} & \textbf{Expected Likelihood} & \textbf{Expected Posterior} \\
\midrule
$\displaystyle \frac{\postDens(\Par; \E_{\Ndesign}[\fwdEm[\Ndesign]])}{\normCst(\E_{\Ndesign}[\fwdEm[\Ndesign]])}$ & 
$\displaystyle \frac{\E_{\Ndesign}\left[\postDens(\Par; \fwdEm[\Ndesign])\right]}{\E_{\Ndesign}\left[\normCstEm[\Ndesign]\right]}$ & 
$\displaystyle \E_{\Ndesign}\left[\frac{\postDens(\Par; \fwdEm[\Ndesign])}{\normCstEm[\Ndesign]}\right]$ \\
\bottomrule
\end{tabular}
\caption{Comparison of the normalized density approximations implied by the plug-in mean, expected likelihood,
and expected posterior approximations in the forward model emulation setting.}
\label{tab:post-approx-comparison}
\end{table}

\subsection{Other Unnormalized Density Approximations}
Various alternative approximations have been proposed for the unnormalized posterior density. 
Instead of computing pointwise expectations, one can consider summarizing $\postEm[\Ndesign](\Par)$
by computing pointwise quantiles \citep{VehtariParallelGP,quantileApprox}. In the log-likelihood 
emulation setting with a GP emulator, \cite{VehtariParallelGP} considers the $\quantileProb$-quantile of 
the likelihood surrogate
\begin{equation}
\mathrm{Quantile}_{\quantileProb}(\Exp{\llikEm[\Ndesign](\Par)})
= \Exp{\emMean[\Ndesign]{\llik}(\Par) + \GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}},
\label{eq:post-approx-quantile}
\end{equation}
where $\GaussianCDF$ denotes the standard Gaussian distribution function. This expression is of the same form 
as the expected likelihood in \cref{eq:post-approx-llik-EL-Gaussian}, but the uncertainty inflation term 
$\Exp{\GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}}$ scales much more slowly than 
$\Exp{\frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)}$. The special case $\quantileProb = 1/2$ (i.e., the median) 
reduces to the mean approximation in \cref{eq:mean-approx_llik}, while values $\quantileProb > 1/2$ imply 
the density will be inflated in regions of higher surrogate uncertainty. \citet{FATES_CES} also utilize the 
approximation in \cref{eq:post-approx-quantile}, though they do not explicitly draw the connection to 
the quantile estimator.

In the forward model emulation setting, \citet{BurknerSurrogate} also consider an \textit{expected log-likelihood}
approximation of the form $\priorDens(\Par) \Exp{\E_{\Ndesign}[\llik(\Par; \fwdEm[\Ndesign])]}$ and draw 
a connection with power-scaled likelihoods. However, the authors ultimately recommend the expected 
posterior and expected likelihood as preferred alternatives to this method.

\subsection{Noisy MCMC Approximations} \label{sec:MH-approx}
In this section, we describe an alternative approach to surrogate-based inference
that focuses on constructing approximations to MCMC algorithms as opposed to approximations
of the unnormalized posterior density. Some of these methods provide algorithms for sampling 
from approximate posteriors already defined above, while others represent new approaches to inference.
The benefits of these approaches include ease of implementation, as well as the fact that
they only require the ability to draw samples from finite-dimensional projections of 
the surrogate predictive distribution. Therefore, these algorithms do not rely on distributional 
assumptions on the surrogate or likelihood, and can be used in the nonparametric surrogate setting.

To start, we recall the standard Metropolis-Hastings (MH) algorithm, which is defined by a proposal 
kernel with density $\propDens(\Par, \cdot)$. If the Markov chain is in the current state 
$\Par \in \parSpace$ then the next state is defined by sampling the proposal
 $\propPar \sim \propDens(\Par, \cdot)$, which is accepted with probability
\begin{align}
&\accProbMH(\Par, \propPar) \Def 
\min\left\{1, \frac{\postDens(\propPar)\propDens(\propPar, \Par)}{\postDens(\Par) \propDens(\Par, \propPar)} \right\}.
\label{MH_acc_prob_exact}
\end{align}
If accepted, the next state is defined to be $\propPar$, else it is set to the current state $\Par$. 
This procedure is summarized in \cref{alg:MH}. 

\begin{figure}[ht]
\centering
\begin{minipage}[t]{0.495\textwidth}
    \floatname{algorithm}{Alg.}
    \captionsetup{type=algorithm}
    \caption{Metropolis-Hastings}
    \label{alg:MH}
    \begin{algorithmic}[1]
    \Function{MH}{$\Par_0, \NMCMC$}     
        \For{$k \gets 0, \dots, \NMCMC$} 
            \State $\tilde{\Par} \sim q(\Par_{k}, \cdot)$
            \State $\alpha \gets \min\left\{1, \frac{\pi(\tilde{\Par}) q(\tilde{\Par}, \Par_k)}{\pi(\Par_k) q(\Par_k, \tilde{\Par})} \right\}$
            \State $b \sim \text{Bernoulli}(\alpha)$
            \If{$b = 1$}
                \State $\Par_{k+1} \gets \tilde{\Par}$ 
            \Else
                \State $\Par_{k+1} \gets \Par_k$
            \EndIf
        \EndFor
    \EndFunction
    \end{algorithmic}
\end{minipage}
\hfill
\begin{minipage}[t]{0.495\textwidth}
    \floatname{algorithm}{Alg.}
    \captionsetup{type=algorithm}
    \caption{Noisy Metropolis-Hastings}
    \label{alg:MH-noisy}
    \begin{algorithmic}[1]
    \Function{MH-Noisy}{$\Par_0, \NMCMC, \postSampleKernel$}
        \State $\hat{\pi}_{\text{curr}} \sim \mathrm{law}(\postEm[\Ndesign](\indexMCMC[0]{\Par}))$
        \For{$k \gets 0, \dots, \NMCMC$} 
            \State $\tilde{\Par} \sim q(\Par_k, \cdot)$
            \State $[\hat{\pi}_{\Par}, \hat{\pi}_{\tilde{\Par}}] \sim \postSampleKernel([\Par_k, \tilde{\Par}, \hat{\pi}_{\text{curr}}], \cdot)$
            \State $\hat{\alpha} \gets \min\left\{1, \frac{\hat{\pi}_{\tilde{\Par}} \cdot q(\tilde{\Par}, \Par_k)}{\hat{\pi}_{\Par} \cdot q(\Par_k, \tilde{\Par})} \right\}$
            \State $b \sim \text{Bernoulli}(\hat{\alpha})$
            \If{$b = 1$}
                \State $\Par_{k+1} \gets \tilde{\Par}$
                \State $\hat{\pi}_{\text{curr}} \gets \hat{\pi}_{\tilde{\Par}}$
            \Else
                \State $\Par_{k+1} \gets \Par_k$
            \EndIf
        \EndFor
    \EndFunction
    \end{algorithmic}
\end{minipage}
\caption{(Left) A standard Metropolis-Hastings MCMC algorithm with proposal density $\propDens(\Par, \codt)$.
(Right) A generic noisy Metropolis-Hastings algorithm. The choice of Markov kernel $\postSampleKernel$ 
defines particular algorithms, as described in \Cref{sec:MH-approx}.}
\end{figure}

We consider a family of algorithms that replace $\accProbMH(\Par, \propPar)$ by
substituting the exact densities $\postDens(\Par)$  and $\postDens(\propPar)$ 
with approximations sampled from a specified distribution. The particular choice of 
this distribution yields different algorithms, all of which are ``noisy'' in the sense that 
an additional Monte Carlo step has been injected within the standard MH scheme
\citep{noisyMCSurvey,noisyMCMC,stabilityNoisyMH}. In particular, we assume 
that the approximation of $[\postDens(\Par), \postDens(\propPar)]$ is sampled as
\begin{equation}
[\hat{\postDens}_{\Par}, \hat{\postDens}_{\propPar}] \sim \postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot),
\end{equation}
where $\postSampleKernel$ is a Markov kernel mapping from source $\parSpace^2 \times \R_+$
to target $\R^2_{+}$. The generic noisy MH algorithm is stated in \Cref{alg:MH-noisy}. 
We consider three special cases below.

\paragraph{Pseudo-Marginal.}
We first consider the choice 
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \delta_{\postDens^\prime} \otimes \mathrm{law}(\postEm[\Ndesign](\propPar)),
\end{equation}
which implies that at each iteration only the density value at the proposed point 
$\postEm[\Ndesign](\propPar)$  is sampled, while the value at the current point 
is recycled from the previous iteration. This is a pseudo-marginal algorithm 
targeting the stationary distribution $\postApproxNormMarg[\Ndesign]$ \citep{pseudoMarginalMCMC}.
In principle, it provides an exact MCMC scheme to sample from the expected likelihood 
approximation even when the expectation $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$ is intractable, 
a fact noted in \citet{StuartTeck1}. The key property ensuring invariance with respect to 
$\postApproxNormMarg[\Ndesign]$ is that the value $\hat{\postDens}_{\tilde{\Par}}$ sampled from 
$\postSampleKernel$ is an unbiased estimate of $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$.
Therefore, the method remains valid for estimators of the form 
\begin{align}
&\hat{\postDens}_{\tilde{\Par}}^{J} \Def \frac{1}{J} \sum_{j=1}^{J} \hat{\postDens}_{\tilde{\Par}}^{(j)},
&&\hat{\postDens}_{\tilde{\Par}}^{(j)} \overset{\mathrm{iid}}{\sim} \mathrm{law}(\postEm[\Ndesign](\propPar)). \label{eq:pm-unbiased-est}
\end{align}
It is well-known that pseudo-marginal methods
can suffer from slow mixing, but that efficiency can be improved by reducing the variance 
in the estimate of $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$; e.g., by increasing $J$ in
\cref{eq:pm-unbiased-est} \citep{pseudoMarginalMCMC,pseudoMarginalEfficiency}.
The pseudo-marginal approach to sampling $\postApproxNormMarg[\Ndesign]$ is studied 
in \citet{garegnani2021NoisyMCMC}.

\paragraph{Monte Carlo within Metropolis Hastings.}
We next consider the Markov kernel 
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \mathrm{law}(\postEm[\Ndesign](\Par)) \otimes \mathrm{law}(\postEm[\Ndesign](\propPar)),
\end{equation}
which independently re-samples the density values at both the current and proposed locations
at each iteration (the kernel does not depend on $\postDens^\prime$). Algorithms of this form are 
typically referred to as Monte Carlo within Metropolis Hastings (MCwMH), and have been studied as an
efficient alternative to the pseudo-marginal algorithm \citep{noisyMCMC,stabilityNoisyMH}.
The sampled density values can also be replaced with sample means as in \Cref{eq:pm-unbiased-est},
though the efficiency of MCwMH is generally less sensitive to the variance in the estimate
\citep{garegnani2021NoisyMCMC,stabilityNoisyMH}.
The MCwMH is typically referred to as inexact, in the sense that it does not admit 
$\postApproxNormMarg[\Ndesign]$ as an invariant distribution. However, in the present setting
$\postApproxNormMarg[\Ndesign]$ is itself an approximation of $\postDensNorm$ so we might
view MCwMH as an alternative method for propagating surrogate uncertainty, 
on equal footing with $\postApproxNormMarg[\Ndesign]$. This perspective is taken in 
\citet{surrogateNoisyMCMC}, while \citet{garegnani2021NoisyMCMC} studies MCwMH
as an approximation to $\postApproxNormMarg[\Ndesign]$. A variant of MCwMH
is employed in \citet{FerEmulation}.

\paragraph{Expected Acceptance Probability.}
Finally, we consider
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \mathrm{law}(\postEm[\Ndesign](\Par), \postEm[\Ndesign](\propPar)),
\end{equation}
implying that both density values are re-sampled each iteration, but now from
the joint distribution implied by the surrogate $\postEm[\Ndesign]$. 
We refer to this as the \textit{expected acceptance probability (E-Acc)} approximation,
as it can be viewed as marginalizing the MH acceptance probability with respect
to the surrogate. Indeed, by inserting $\postEm[\Ndesign]$
in place of $\postDens$ in $\accProbMH(\Par, \propPar)$, the surrogate induces a random 
approximation
\begin{equation}
\accProbMHEm[\Ndesign](\Par, \propPar) 
\Def \min\left\{1, \frac{\postEm[\Ndesign](\propPar)\propDens(\propPar, \Par)}{\postEm[\Ndesign](\Par) \propDens(\Par, \propPar)} \right\}
\label{eq:MH-prob-surrogate}
\end{equation}
of the MH acceptance probability. The E-Acc algorithm is identical to the standard 
MH scheme in \Cref{alg:MH} but with $\E_{\Ndesign}[\accProbMHEm[\Ndesign](\Par, \propPar)]$
replacing $\accProbMH(\Par, \propPar)$. \Cref{tab:acc-prob-comparison} compares the E-Acc 
approximation to the alternative approximations of $\accProbMH(\Par, \propPar)$ that are implicitly 
induced by approximations of the unnormalized posterior density. 
\citet{surrogateNoisyMCMC} propose the E-Acc algorithm, 
and explore connections to the expected posterior approximation.

\begin{table}[h]
\centering
\small % (optional) shrink slightly if needed
\renewcommand{\arraystretch}{1.6} % row height
\setlength{\tabcolsep}{10pt} % column separation
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}} % centered column of width #1

\begin{tabular}{lC{10cm}}
\toprule
\textbf{Posterior Approximation} & \textbf{MH Approximation} \\
\midrule
Plug-In Mean & 
\(\displaystyle \min\left\{1, \frac{\Exp{\E[\lpostEm[\Ndesign](\propPar)]}}{\Exp{\E[\lpostEm[\Ndesign](\Par)]}} \right\}\) \\ 
Expected Likelihood & 
\(\displaystyle \min\left\{1, \frac{\E \left[\Exp{\lpostEm[\Ndesign](\propPar)}\right]}{\E\left[\Exp{\lpostEm[\Ndesign](\Par)}\right]} \right\}\) \\
Expected Likelihood Ratio & 
\(\displaystyle \min\left\{1, \E\left[\frac{\Exp{\lpostEm[\Ndesign](\propPar)}}{\Exp{\lpostEm[\Ndesign](\Par)}}\right] \right\}\) \\
Expected Acc. Prob. & 
\(\displaystyle \E\left[\min\left\{1, \frac{\Exp{\lpostEm[\Ndesign](\propPar)}}{\Exp{\lpostEm[\Ndesign](\Par)}} \right\}\right]\) \\
\bottomrule
\end{tabular}
\caption{The approximations to the Metropolis-Hastings (MH) acceptance probability implied by different posterior approximations using a log-posterior surrogate $\lpostEm[\Ndesign]$. All expectations are with respect to 
the underlying surrogate predictive distribution. For brevity, the acceptance probabilities 
are presented for the case of a symmetric proposal distribution.}
\label{tab:acc-prob-comparison}
\end{table}

\subsection{Existence and Integrability} \label{sec:existence}
We have henceforth implicitly assumed that the surrogate-induced posterior approximations 
introduced above are well-defined. Consideration of this question points to important 
practical issues in constructing emulators for use in posterior approximation. See
\Cref{sec:recs} for practical recommendations. The primary concern
is that the tail behavior of the surrogate may lead to approximations of $\postDens$ that 
are not integrable. Such pathologies can easily arise if utilizing a stationary surrogate, where 
the predictive mean and variance stabilize at constant values as distance from the design points 
increases. This issue is especially a concern if emulating the log-posterior density, as the prior 
density is being modeled and hence cannot be relied upon to ensure integrability \citep{emPostDens}.
Table \todo illustrates simple examples where such pathologies can occur.
In order to avoid issues related to
tail behavior, previous work tends to restrict to the setting where $\parSpace$
is a compact subset of $\R^{\dimPar}$ \citep{StuartTeck1, VehtariParallelGP}.
Other applications have not directly addressed this issue, but in numerical experiments focus on 
prior distributions with compact support \citet{trainDynamics,FATES_CES} or truncate an unbounded
prior to achieve compact support \citep{gp_surrogates_random_exploration,FerEmulation}. 
\citet{emPostDens} address this issue in depth, describing how the use of stationary log-posterior
surrogates can lead to divergent MCMC chains when performing approximate posterior inference.
Theoretical treatments have also explored conditions to ensure existence over generic unbounded
domains \citep{random_fwd_models,garegnani2021NoisyMCMC}. We refer to 
\citep{StuartTeck1,StuartTeck2} for a comprehensive theoretical treatment in the GP setting.
Ideally, the surrogate ought to be constructed such that every realization of $\postEm[\Ndesign]$
is integrable (i.e., $\normCstEm$ exists almost surely). This assumption is implicit in \cref{alg:ep}, 
which assumes that each sampled trajectory of $\postEm[\Ndesign]$ induces a valid probability 
distribution from which samples can be drawn. This requirement can be weakened for other 
approximations; for example, in \cref{prop:llik-em-Gaussian} we observe the requirement is that
$\log\left[\priorDens(\Par)\right] + \emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)$
decays sufficiently quickly as $\norm{\Par} \to \infty$. 

\section{Sequential Design} \label{sec:seq_design}
\subsection{Asymptotic Consistency}
Note that the algorithms that are appropriate for certain problems will significantly depend on 
how expensive the simulator is.

These methods effectively combine posterior
inference and sequential design within a single algorithm.
The papers \citet{Li_2014,ConradLocalExactMCMC} 
adaptively construct local polynomial surrogates within an MCMC run, and are able to establish asymptotic 
exactness by showing the polynomial 
approximation error diminishes over time (\todo: verify this last point). The work \citep{ActiveLearningMCMC}
also conducts sequential design within an MCMC algorithm, using emulator predictions at points where the 
surrogate is confident, and running exact simulations at points that exceed a user-defined uncertainty tolerance.
In \citet{MCMC_GP_proposal}, an algorithm is proposed to accelerate Hamiltonian Monte Carlo by using a 
GP only for proposals, but running the exact simulation when computing the acceptance probability to 
ensure consistency. The delayed acceptance mechanism provides another common method for ensuring
asymptotic consistency \citep{DelayedAcceptance}.
These algorithms all leverage a surrogate in various ways with the goal of reducing the simulation load, while
maintaining asymptotic exactness.  We discuss sequential design strategies in \cref{sec:seq_design}.

\section{Practical Recommendations} \label{sec:recs}

\paragraph{Model the Global Trend.}
We now provide some practical guidelines motivated by these integrability considerations.
Intuitively, the surrogate ought to be constructed such that every realization of $\postEm[\Ndesign]$
is integrable (i.e., $\normCstEm$ exists almost surely). From a practical perspective, our general 
recommendation is to design the surrogate model to ensure that $\lpostEm[\Ndesign](\Par) \to -\infty$
almost surely as $\norm{\Par} \to \infty$. As a prior predictive check, we propose to   
assess the limiting behavior of the predictive distribution along different directions in parameter
space. For example, this could take the form of a graphical check by plotting quantiles of 
the predictive distribution at sequences of points spaced along the coordinate axes. 
See table \todo for an example. Performing this simple check does not require any additional 
evaluations of the expensive simulator, and can prevent wasting resources by uncovering 
issues with the surrogate during approximate inference (for example, \citet{emPostDens}
use divergent MCMC chains as a sort of surrogate diagnostic check).

In order to achieve the requirement $\lpostEm[\Ndesign](\Par) \to -\infty$, it is 
necessary to achieve a reasonable approximation of the global trend of the response 
surface that is being emulated over $\parSpace$. This is particularly important in the
log-density emulation setting. In this case, we generally recommend against the use of 
stationary surrogate models, though they can perform reasonably well in particular cases.
Instead, one natural approach is to fit a trend to capture the global structure, along with an 
additional model to capture local deviations from the trend. In the GP setting, this idea has 
been realized by combining quadratic mean functions with stationary kernels 
\citep{emPostDens,SinsbeckNowak,VehtariParallelGP,llikEmABC,ABCGP,pseudoMarginalGP}. In 
\citet{emPostDens} the quadratic trend is constructed via a sequential importance sampling
scheme, while in the other articles the quadratic coefficients are learned alongside the 
kernel hyperparameters. Various works have also proposed combining polynomial chaos
expansions and Gaussian processes \citep{SinsbeckNowak} (\todo: cite others).
\todo: cite Roshan Joseph paper here as well

\paragraph{The Design Should Oversample the Tails.}
\paragraph{Enforce Bound Constraints.}


\section{Numerical Case Study} \label{sec:case-study}

\section{Related Work} \label{sec:related-work}

\subsection{Computer Model Calibration} \label{sec:computer-model-calibration}
The challenge of emulating a black-box computer model has received widespread attention 
well beyond the scope of Bayesian inference. The design of surrogate models for Bayesian 
inverse problems may be informed by the vast literature from the computer experiments, 
engineering, applied math, machine learning, and statistics communities. As a starting point, 
we refer readers to \citet{gramacy2020surrogates,design_analysis_computer_experiments,SanterCompExp,UQpredCompSci} 
and the references therein. It is worth taking a moment to clarify the scope of our review 
with respect to related work in the computer experiments literature in particular.
Indeed, early work in this field addressed the challenge of learning 
model parameters  $\Par$ from observational data via the use of a surrogate for $\fwd$, a problem 
commonly referred to as \textit{computer model calibration} (see \citet{computerModelCalibrationReview}
for a review). When cast as a problem of Bayesian inference, the calibration problem falls within the 
framework considered in this article. However, much of this early calibration work focused on 
the added challenge of learning a discrepancy term between the computer model $\fwd$ and the 
true underlying system \citep{ModelDiscrepancy,emPostDens,OakleyllikEm}. 
For example, the pioneering work \citet{KOH} considers jointly learning 
a forward model emulator, calibration parameters, and a discrepancy function within a single 
Bayesian model. By contrast, we do not consider discrepancy modeling in this review, and moreover
focus on the alternative modular workflow, where the surrogate is fit offline 
without seeing the calibration data $\obs$ \citep{modularization}. 
\footnote{This point is specific to the forward model emulation setting. Log-density emulators 
do depend on $\obs$, since the log-likelihood is a function of the data.}
This modular two-stage approach naturally leads to 
the question of how to propagate the surrogate uncertainty in the calibration stage, which is one 
of the central questions motivating this review.

\subsection{Probabilistic Numerics} \label{sec:prob-numerics}

\subsection{Approximate Bayesian Computation} \label{sec:abc}

\subsection{Other}
Multifidelity methods? Active subspaces?

% Appendix 
\section{Appendix}

\subsection{Marginal Approximation with Gaussian Likelihood: Forward Model Emulation}
The closed-form computations related to the marginal approximation with a Gaussian 
likelihood follow from standard results regarding the convolution of Gaussian densities.  

\begin{prop} \label{Gaussian_convolution}
Let $\Gaussian(A \mu, \likPar)$ and $\Gaussian(m, C)$ be Gaussian distributions on $\R^{\dimObs}$ and $\R^{\dimPar}$, 
respectively, with $A \in \R^{\dimObs \times \dimPar}$ and $\likPar, C$ symmetric, positive definite matrices. Then 
\begin{align*}
\int_{\R^{\dimPar}} \Gaussian(\obs | A \mu, \likPar) \Gaussian(\mu | m, C) d\mu
&= \Gaussian(\obs | Am, \likPar + ACA^\top). 
\end{align*}
\end{prop}

\begin{proof} 
\todo
\end{proof}

We next prove a lemma that will be used in the proofs of \Cref{prop:Gaussian_marginal_moments} 
and \Cref{lemma:fwd-Gaussian-density-dist} below. 
\begin{lemma} \label{lemma:squared_Gaussian_density}
Let $\Gaussian(m, C)$ be a Gaussian distribution on $\R^{\dimObs}$ with $C$ a symmetric, positive-definite 
matrix. Then, for $\obs \in \R^{\dimObs}$, 
\begin{align*}
\Gaussian(\obs | m, C)^2 
&= 2^{-\dimObs/2} \det(2\pi C)^{-1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right). 
\end{align*}
\end{lemma}

\begin{proof}
\begin{align*}
\Gaussian(\obs | m, C)^2 
&= \det(2\pi C)^{-1} \Exp{-\frac{1}{2} (\obs - m)^\top \left[\frac{1}{2}C \right]^{-1}(\obs - m)} \\
&= \det(2\pi C)^{-1} \det(2\pi (1/2)C)^{1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right) \\
&= 2^{-\dimObs/2} \det(2\pi C)^{-1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right). 
\end{align*}
\end{proof}

\begin{prop} \label{prop:Gaussian_marginal_moments}
Assume $\obs | \mu \sim \Gaussian(A \mu, \likPar)$ and $\mu \sim \Gaussian(m, C)$, where $\mu \in \R^{\dimPar}$, 
$A \in \R^{\dimObs \times \dimPar}$, and $\likPar$, $C$ are both symmetric, positive definite. Then 
\begin{align}
\E\left[\Gaussian(\obs | A \mu, \likPar) \right] &= \Gaussian(\obs | Am, \likPar + ACA^\top) \\
\Var\left[\Gaussian(\obs | A \mu, \likPar) \right] 
&= \frac{\Gaussian\left(\obs | Am, \frac{1}{2} \likPar + ACA^\top \right)}{2^{\dimObs/2} \det(2\pi \likPar)^{1/2}} - 
\frac{\Gaussian\left(\obs | Am, \frac{1}{2}[\likPar + ACA^\top] \right)}{2^{\dimObs/2} \det(2\pi[\likPar + ACA^\top])^{1/2}}
\end{align}
\end{prop}

\begin{proof} 
The first result follows immediately from \Cref{Gaussian_convolution}. For the variance, we have 
\begin{align}
\Var\left[\Gaussian(\obs | A \mu, \likPar) \right] 
&= \E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right] - \E\left[\Gaussian(\obs | A \mu, \likPar) \right]^2 \nonumber \\
&= \E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right] - \Gaussian(\obs | Am, \likPar + ACA^\top)^2. \label{two_terms_variance}
\end{align}
Starting with the first term, we apply \Cref{lemma:squared_Gaussian_density} and 
\Cref{Gaussian_convolution}, respectively, to obtain 
\begin{align*}
\E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right]
&= 2^{-\dimObs/2} \det(2\pi \likPar)^{-1/2} \E\left[\Gaussian\left(\obs | A\mu, \frac{1}{2}\likPar \right)\right] \\
&= 2^{-\dimObs/2} \det(2\pi \likPar)^{-1/2} \Gaussian\left(\obs | Am, \frac{1}{2}\likPar + ACA^\top \right).
\end{align*}
For the second term in \ref{two_terms_variance}, another application of \Cref{lemma:squared_Gaussian_density} gives
\begin{align*}
\Gaussian(\obs | Am, \likPar + ACA^\top)^2
&= 2^{-\dimObs/2} \det(2\pi[\likPar + ACA^\top])^{-1/2} \Gaussian\left(\obs | Am, \frac{1}{2}[\likPar + ACA^\top]\right).
\end{align*}
Plugging these expressions back into \ref{two_terms_variance} completes the proof. 
\end{proof}


\subsection{Marginal Acceptance Probability}
In this section we derive an expression for 
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right], 
\end{align}
as considered in \ref{acc_prob_joint_marg}. We start by noting 
\begin{align}
\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)
&\sim \LN\left(\log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par), \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right] \right),
\label{acc_ratio_LN}
\end{align}
where 
\begin{align*}
C \Def \frac{\priorDens(\propPar) \propDens(\propPar, \Par)}{\priorDens(\Par) \propDens(\Par, \propPar)}.
\end{align*}
Thus, the required computation reduces to computing the expectation of $\min\left\{1, Y \right\}$, where $Y$ is a log-normally distributed 
random variable. 

\begin{lemma} \label{lemma:exp_max_one_LN}
Let $Y \sim \LN(m, s^2)$. Then, 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right] &= \GaussianCDF\left(\frac{m}{s} \right) + \GaussianCDF\left(-\frac{m + s^2}{s} \right) e^{m + \frac{1}{2}s^2}
\end{align}
\end{lemma}

\begin{proof}
We have 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right]
&= \int_{0}^{\infty} \min\{1, y\} \LN(y | m, s^2) dy \nonumber \\
&= \int_{-\infty}^{\infty} \min\{1, e^x\} \Gaussian(x | m, s^2) dx \nonumber \\
&= \int_{0}^{\infty} \Gaussian(x | m, s^2) dx +  \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx. \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx.  
\label{second_integral} 
\end{align}
For the integral in \ref{second_integral} we combine the exponential terms and complete the square. 
This yields 
\begin{align*}
&\int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx \\
&= \Exp{-\frac{m^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2} \left[\frac{x^2}{2} - 2\left(\frac{m}{s^2} + 1\right)x  \right]} dx \\
&= \Exp{-\frac{m^2}{2s^2} + \frac{[m + s^2]^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2s^2}(x - [m+s^2])^2} dx \\
&= \Exp{m + \frac{1}{2}s^2} \GaussianCDF\left(-\frac{m+s^2}{s}\right).
\end{align*}
Plugging this back into \ref{second_integral} completes the proof. 
\end{proof}

We now apply \Cref{lemma:exp_max_one_LN} to obtain the expression for the marginal acceptance probability. 
\begin{prop} \label{prop:joint-marg-accept-prob}
\begin{align*}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right] 
= w_1 + w_2 \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar), 
\end{align*}
where 
\begin{align*}
w_1 &= \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \Prob\left(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq \Exp{-\Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]}\right).
\end{align*}
\end{prop}

\begin{proof}
We recall from \ref{acc_ratio_LN} that $\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \sim \LN(m, s^2)$, with 
\begin{align*}
m &= \log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par) \\
s^2 &= \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]. 
\end{align*}
We thus have 
\begin{align*}
e^{m + \frac{1}{2}s^2} = \E_{\llikEm}\left[\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \right] 
=  \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar). 
\end{align*}
It remains to verify the expressions for the weights $w_1$ and $w_2$. Letting $Z \sim \Gaussian(0,1)$, 
we apply \Cref{lemma:exp_max_one_LN} to obtain
\begin{align*}
w_1 &= \GaussianCDF(m/s) = \Prob(Z \leq m/s) = \Prob(\Exp{m+sZ} \geq 1) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \GaussianCDF(-(m+s^2)/s) = \Prob(\Exp{m + sZ} \leq e^{-s}) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq e^{-s}).
\end{align*}
\end{proof}

\subsection{Transition Kernels of Approximate MCMC Algorithms} \label{transition_kernel_derivations}
We derive the expression for the \textit{MCWMH-joint} transition 
kernel $\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}$ given in \ref{MCWMH-joint-kernel}. 
Let $\Par \in \parSpace$ and $A \subset \parSpace$ a Borel set. 
We recall that the kernel for a standard MH algorithm is given by 
\begin{align}
\MarkovKernel(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar
+ [1 - \avgAccProbMH(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
where $\avgAccProbMH(\Par)$ is the overall acceptance probability, averaged over all proposals,
\begin{align}
\avgAccProbMH(\Par)
&= \int_{\parSpace} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar. 
\end{align}
Denote $\parMat \Def \{\Par, \propPar\}$. 
The \textit{MCWMH-joint} algorithm replaces the exact log-likelihood evaluations 
$\llik(\parMat) \Def [\llik(\Par), \llik(\propPar)]^\top$ used to define $\propDens(\Par, \propPar)$ by 
sampled approximate values 
\begin{align}
\llikSamp_{\parMat} \Def [\llikSamp, \llikSampProp]^\top \sim \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)). 
\end{align}
We let $\Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})$ denote the approximate acceptance probability defined using 
the sampled values $\llikSamp_{\parMat}$. The probability of accepting a state in the set $A$, conditional on the sample $\llikSamp_{\parMat}$, 
is thus 
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat})
&=  \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat}) d\propPar. 
\end{align}
The unconditional probability follows from the law of total probability:
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par)
&= \int_{\R^2} \Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat}) \Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \\
&= \int_{\R^2} \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\propPar \ d\llikSamp_{\parMat} \\
&= \int_{A} \propDens(\Par, \propPar) \int_{\R^2}  \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \ d\propPar \label{flip_integral_order} \\
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar
\end{align}
where \ref{flip_integral_order} follows from Tonelli's theorem, and using the definition of $\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)$
in \ref{acc_prob_joint_marg}. Setting $A \Def \parSpace$ in the above integral yields the overall acceptance probability 
\begin{align}
\llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par) \Def \int_{\parSpace} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar. 
\end{align}
These quantities thus give 
\begin{align}
\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar
+ [1 - \llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
which follows from same derivations for the standard MH kernel \ref{MH-kernel} with the marginal acceptance probabilities substituted for
the original ones. The transition kernel for the \textit{MCWMH-ind} algorithm follows immediately by marginalizing with respect 
to $\Gaussian(\emMean[\Ndesign]{\llik}\left(\parMat), \diag\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar) \right\}\right)$
in place of the full joint distribution. 

\subsection{Sequential Design Calculations}
We start by stating an identity for the inversion of partitioned matrices, which is very useful in updating GPs by 
conditioning on new design points. The generic result can be found in the lecture notes \cite{MinkaMatrixLectures}, 
but we specialize the statement to the GP setting.  

\subsubsection{Useful Lemmas}

\begin{prop} \label{partitioned-matrix-inverse}
Let $\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$ be a GP prior, with 
$\funcEm[\Ndesign] \Def \funcPrior | [\funcPrior(\designIn) = \func(\designIn)] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$  
the GP predictive distribution after conditioning on the design $(\designIn, \func(\designIn))$. Let 
$\designBatchIn$ be a set of $\Nbatch$ new design points. Define 
$\kerMat[\Ndesign] \Def \gpKerPrior(\designIn)$, $\kerMat[\Nbatch] \Def \gpKerPrior(\designBatchIn)$,
and $\kerMat[\Ndesign,\Nbatch] \Def \gpKerPrior(\designIn[\Ndesign], \designBatchIn)$.  
Then, letting 
$\designIn[\Naugment] \Def \designIn \cup \designBatchIn$, the inverse of the kernel matrix 
evaluated on the augmented design satisfies 
\begin{align}
\gpKerPrior(\designIn[\Naugment])^{-1}
&= \begin{pmatrix} \kerMat[\Ndesign] & \kerMat[\Ndesign,\Nbatch] \\
\kerMat[\Ndesign,\Nbatch]^\top & \kerMat[\Nbatch] \end{pmatrix}^{-1}
=  \begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix},
\end{align}
where 
\begin{align}
\tilde{K} = \kerMat[\Ndesign]^{-1} + \kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Nbatch,\Ndesign] \kerMat[\Ndesign]^{-1}.
\end{align}
Thus, assuming $\kerMat[\Ndesign]^{-1}$ has already been computed, $\gpKerPrior(\designIn[\Naugment])^{-1}$ can be constructed in an additional 
$\BigO(\Nbatch^3 + \Nbatch \Ndesign^2 + \Nbatch^2 \Ndesign)$ operations. 
\end{prop}

We repeatedly use the fact that $\funcPrior|[\funcPrior(\designIn[\Naugment]) = \func(\designIn[\Naugment])]$ and 
$\funcEm[\Ndesign] | [\funcEm[\Ndesign](\designBatchIn) = \func(\designBatchIn)]$ are equal in distribution ; 
i.e., conditioning the GP prior on the entire design 
is equivalent to sequentially conditioning on subsets of the design. For the sake of completeness, we provide the rigorous justification for this below.

\begin{lemma} \label{lemma:gp-condition-order}
Let $\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$ be a GP prior. Consider a set of $\Naugment$ design points $\{\designIn[\Naugment], \funcVal[\Naugment]\}$
partitioned as $\designIn[\Naugment] = \designIn[\Ndesign] \cup \designBatchIn$ and $\funcVal[\Naugment] = \funcVal[\Ndesign] \cup \funcVal[\Nbatch]$.
Let $\funcEm[\Ndesign] \Def \func | [\func(\designIn[\Ndesign]) = \funcVal[\Ndesign]] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$. 
Then the random process $\funcPrior | [\funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]]$ is equal in distribution to the random process
$\funcEm[\Ndesign] | [\funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]]$. 
\end{lemma} 

\begin{proof} 
Since both processes in question are Gaussian it suffices to check that 
\begin{align*}
\E[\funcPrior(\parMat) | \funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]] 
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]] \\
\Cov[\funcPrior(\parMat) | \funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]] 
&= \Cov[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]],
\end{align*}
for an arbitrary finite set of inputs $\parMat \subset \parSpace$. The quantities on the lefthand side are 
$\gpMean[\Naugment](\parMat)$ and $\gpKer[\Naugment](\parMat)$, by definition. We begin by expanding 
the expression $\gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}$, noting that we 
are borrowing the notation from \Cref{partitioned-matrix-inverse}. Applying the partitioned matrix 
inversion identity from \Cref{partitioned-matrix-inverse} yields 
\begin{align*}
\gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}
&= \begin{pmatrix} \gpKerPrior(\parMat, \designIn[\Ndesign]) &  \gpKerPrior(\parMat, \designIn[\Nbatch]) \end{pmatrix}
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix}.
\end{align*}
Denoting $\funcVal[\Ndesign]^\prime \Def \funcVal[\Ndesign] - \gpMeanPrior(\designIn[\Ndesign])$ and 
$\funcVal[\Nbatch]^\prime \Def \funcVal[\Nbatch] - \gpMeanPrior(\designIn[\Nbatch])$, the predictive mean $\gpMean[\Naugment](\parMat)$ is thus given by 
\begin{align*}
\gpMean[\Naugment](\parMat)
&= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}[\funcVal[\Naugment] - \gpMeanPrior(\designIn[\Naugment])] \\
&= \gpMeanPrior(\parMat) + \begin{pmatrix} \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix}^\top
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix} 
\begin{pmatrix}  \funcVal[\Ndesign]^\prime \\  \funcVal[\Nbatch]^\prime \end{pmatrix} \\
&= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn[\Ndesign])\kerMat[\Ndesign]^{-1}\funcVal[\Ndesign]^\prime + 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \funcVal[\Nbatch]^\prime - 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1} \funcVal[\Ndesign]^\prime \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch]^\prime - \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1} \funcVal[\Ndesign]^\prime] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch]^\prime - \gpMean[\Ndesign](\designIn[\Nbatch]) + \gpMeanPrior(\designIn[\Nbatch])] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch] - \gpMean[\Ndesign](\designIn[\Nbatch])] \\
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]] 
\end{align*}
where we have used the fact that the predictive covariance of the GP $\funcEm[\Ndesign]$ gives 
\[
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) = \gpKerPrior(\parMat, \designIn[\Nbatch]) - 
\gpKerPrior(\parMat, \designIn[\Ndesign])\kerMat[\Ndesign]^{-1} \gpKerPrior(\designIn[\Ndesign], \parMat).
\]
The covariance calculation proceeds similarly by replacing $\funcVal[\Ndesign]^\prime$ and $\funcVal[\Nbatch]^\prime$ with 
$\gpKerPrior(\designIn[\Ndesign], \parMat)$ and $\gpKerPrior(\designIn[\Nbatch], \parMat)$, respectively. We obtain 
\begin{align*}
\gpKer[\Naugment](\parMat)
&= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}[\funcVal[\Naugment] - \gpMeanPrior(\designIn[\Naugment])] \\
&= \gpKerPrior(\parMat) - \begin{pmatrix} \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix}^\top
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix} 
\begin{pmatrix}  \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix} \\
&= \gpKer[\Ndesign](\parMat) - 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} [\gpKerPrior(\designIn[\Nbatch], \parMat) - \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1}\gpKerPrior(\designIn[\Ndesign], \parMat)] \\
&= \gpKer[\Ndesign](\parMat) -  \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \gpKer[\Ndesign](\designIn[\Nbatch], \parMat) \\
&= \Cov[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]].
\end{align*}

\end{proof}

\subsubsection{Uncertainty in GP Predictive Mean Due to Unobserved Response}

\begin{proof} [Proof of \Cref{lemma:pred-mean-dist}]
Though the result is only required for a single input $\Par$, it is no more difficult to establish for a set of inputs $\parMat$. 
We recall that 
\begin{align*}
\gpMean[\Ndesign](\parMat | \designBatchFunc) 
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm(\designBatchIn) = \designBatchFunc] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} [\designBatchFunc - \gpMean[\Ndesign](\designBatchIn)],
\end{align*}
following from the GP predictive equations \ref{kriging_eqns}. Since, $\designBatchFunc|\parMat \sim \Gaussian(\gpMean[\Ndesign](\designBatchIn), \gpKer[\Ndesign](\designBatchIn))$, 
we see that $\gpMean[\Ndesign](\parMat | \designBatchFunc)$ is a linear function of a Gaussian random variable. It is thus Gaussian distributed, with mean and covariance
\begin{align*}
\E_{\designBatchFunc}[\gpMean[\Ndesign](\parMat | \designBatchFunc)]
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} [\gpMean[\Ndesign](\designBatchIn) - \gpMean[\Ndesign](\designBatchIn)] = \gpMean[\Ndesign](\parMat) \\
\Cov_{\designBatchFunc}[\gpMean[\Ndesign](\parMat | \designBatchFunc)]
&= \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1}  \gpKer[\Ndesign](\designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} \gpKer[\Ndesign](\designBatchIn, \parMat) \\
&=  \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} \gpKer[\Ndesign](\designBatchIn, \parMat) \\
&= \gpKer[\Ndesign](\parMat) - \gpKer[\Naugment](\parMat).
\end{align*}
\end{proof}

\subsubsection{Integrated Conditional Variance Calculations: Log-Likelihood Emulation}

\begin{proof} [Proof of \Cref{lemma:evar}]
We start by noting that 
\begin{align*}
\Var[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \Var[\priorDens(\Par) \Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \\
&= \priorDens(\Par)^2 \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik],
\end{align*}
so we will focus on the likelihood emulator, ignoring the prior for now. Since 
\begin{align*}
\Exp{\llikEm(\Par)} | [\llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \sim 
\LN(\emMean[\Ndesign]{\llik}(\Par| \designBatchLlik), \emKer[\Naugment]{\llik}(\Par)),
\end{align*}
we apply the formula for a log-normal variance to obtain 
\begin{align}
\Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \cst \Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)}, \label{formula_plug_in}
\end{align}
where 
\begin{align*}
\cst \Def \left[\Exp{\emKer[\Naugment]{\llik}(\Par)} -1 \right] \Exp{\emKer[\Naugment]{\llik}(\Par)}
\end{align*}
is not a function of the random variable $\designBatchLlik$. \Cref{lemma:pred-mean-dist} gives 
\begin{align*}
\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)
&\sim \LN(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)), 
\end{align*}
which implies 
\begin{align*}
\Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)}
&\sim \Gaussian(2\emMean[\Ndesign]{\llik}(\Par), 4[\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)]).
\end{align*}
Applying the formula for a log-normal mean thus yields 
\begin{align*}
\E_{\designBatchLlik} \left[\Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)} \right]
&= \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \Exp{2[\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)]} \\
&=  \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \varInflation(\Par; \designBatchIn), 
\end{align*}
where $\varInflation(\Par; \designBatchIn)$ is defined in \ref{var_inflation_factor}. Plugging this expression back 
into \ref{formula_plug_in} gives 
\begin{align*}
\E_{\designBatchLlik} \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \cst \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \varInflation(\Par; \designBatchIn) \\
&= \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \emMean[\Ndesign]{\llik}(\Par)] \varInflation(\Par; \designBatchIn).
\end{align*}
Multiplying both sides by $\priorDens(\Par)$ completes the proof, with the closed-form expression for the first term following 
immediately from the formula for a log-normal variance. 
\end{proof}

\subsubsection{Integrated Conditional Variance Calculations: Forward Model Emulation}

\begin{proof} [Proof of \Cref{prop:evar-fwd-emulation}]
We begin by noting that the squared prior density can be pulled out of the expectation as
\begin{align*}
\E_{\designBatchFwd} \Var[\postDens(\Par; \fwdEmCond{\designBatchFwd}) | \designBatchFwd]
&= \priorDens^2(\Par) \ \E_{\designBatchFwd}  \Var[\Gaussian\left(\obs |  \fwdEmCond{\designBatchFwd}(\Par), \likPar \right) | \designBatchFwd]. 
\end{align*}
The variance on the righthand side can be expanded as 
\begin{align}
\Var[\Gaussian\left(\obs |  \fwdEmCond{\designBatchFwd}(\Par), \likPar \right) | \designBatchFwd]
&= \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par) \right)}{\det(2\pi \likPar)^{1/2}}
- \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right]  \right)}{\det(2\pi [\likPar + \emKer[\Naugment]{\fwd}(\Par)])^{1/2}}
\end{align}
following from an application of \Cref{prop:fwd-em-Gaussian}. The denominators in the above expression can be 
pulled out of the outer expectation and are seen to equal the denominators in the desired expression. We thus complete the proof by 
computing the expectation of the numerators with respect to 
$\designBatchFwd | \designBatchIn \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\designBatchIn), \emKer[\Ndesign]{\fwd}(\designBatchIn))$. 
These expectations can be computed by noting \Cref{lemma:pred-mean-dist} and then applying \Cref{prop:Gaussian_marginal_moments}.
This yields,
\begin{align*}
\E_{\designBatchFwd}\left[\Gaussian\left(\obs \bigg| \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par) \right)\right]
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), 
\left[\frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par)\right] + \left[\emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)\right] \right) \\
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par) \right)
\end{align*}
and
\begin{align*}
\E_{\designBatchFwd}\left[\Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right]\right)\right] 
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2} \left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right] + \left[\emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)\right] \right) \\
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \left[\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right] - \frac{1}{2} \emKer[\Naugment]{\fwd}(\Par)\right),
\end{align*}
which completes the derivation of \Cref{fwd_evar1}. To obtain \Cref{fwd_evar2} we rearrange the covariances of the above expressions to obtain 
\begin{align*}
\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par) &= 
\left[\likPar +  \emKer[\Ndesign]{\fwd}(\Par) \right] - \frac{1}{2}\likPar 
= \CovComb(\Par) - \frac{1}{2}\likPar \\
\left[\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right] - \frac{1}{2} \emKer[\Naugment]{\fwd}(\Par)
&= \left[\likPar + \emKer[\Ndesign]{\fwd}(\Par) \right] - \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par) \right] 
= \CovComb(\Par) - \frac{1}{2} \CovComb[\Naugment](\Par). 
\end{align*}

\end{proof}


% Questions and TODOs
\section{Questions and TODOs}
\subsection{Questions}
\begin{enumerate}
\item How to reliably use a log-normal emulator for Bayesian inference? 
	\begin{enumerate}
	\item Improve GP calibration (e.g., quadratic mean) 
	\item Use more robust statistics (e.g., interquartile range)
	\item Truncate proposal or prior. 
	\end{enumerate}
\item How to deal with highly concentrated/correlated posteriors? 
	\begin{enumerate}
	\item MALA or other samplers. 
	\end{enumerate}
\end{enumerate}

\subsection{Need to add}
\begin{enumerate}
\item Summary of results from noisy MCMC literature.
\item Try working out results that compare the ratio of the approx density at two points across different approximations; 
alternatively could consider deriving these results for the normalized densities. 
\item Include result that sample and marginal approx agree at the design points.
\item Include existence results (check existence result from that new paper)
\item Viewing noisy MCMC approaches as approximations to the sample-based posterior.
\item Different views of noisy MCMC approaches, including extending the state space. How does this alg compare to the 
marginal and mcwmh-ind algs?
\item Add some sort of theoretical result that demonstrates that the marginal approximation is extremely sensitive to the GP 
variance. Based on numerical experiments, seems like this result should be given with respect to the dynamic range of the 
log-likelihood. Also of course depends on how fast the GP variance grows away from the design points, so perhaps should 
consider fill distance or something like this as well.
\item Numerical experiment that considers the different ways to weight the integrated uncertainty criteria (i.e., targeting 
the unnormalized posterior density vs. using the approx posterior samples as weights).
\item Posterior consistency results for the noisy MCMC emulators; combine the noisy MCMC results with GP approximation 
results.
\item Evaluating calibration of GP-approximated posteriors relative to calibration of the underlying GP emulator.
\item Constrained GPs
\item Pathwise sampling approach to approximate the sample-based approximation.
\item Compare noisy MCMC vs. deterministic version that considers integrating over the acceptance prob. 
\item Analyze effect of incorporating GP covariance structure; does it result in posteriors closer to the sample-based posterior? 
\item Compare marginal and sample-based approx.
\item Compare marginal approx in log-likelihood vs. forward model setting. 
\item Analyze distribution of likelihood under forward model emulation [I think this is the exponential of a folded Gaussian random variable]. 
How does its tail compare to the lognormal tail? 
\end{enumerate}

\subsection{Emulator Ideas}
\begin{enumerate}
\item Sum of quadratic kernel, Gaussian kernel, and some sort of flat/linear kernel (i.e. something with very long lengthscales) to capture the 
part of the response surface that "flattens" out. 
\end{enumerate}

\subsection{Notation}
\begin{enumerate}
\item How to clean up notation for all of the different approximations being considered here? 
\end{enumerate}

\subsection{Numerical experiments:}
My plan is to have emulation in dynamical settings (ODEs) as the unifying theme here. VSEM can provide the core example but could also consider 
others, such as Lorenz-63 (see Hwanwoo Kim, Daniel Sanz-Alonso paper for the ODEs they consider). When introducing the dynamical setting,
cite Stuart/Schneider Earth System modeling 2.0 paper. Provide various examples of observation operators: time-averages (moments) of state 
variables, identity operator, many shorter time-averages (e.g., weekly/monthly averages), multi-objective settings of calibrating to multiple state 
variables. I should probably include Lorenz-63 to have a more familiar example to many audiences. 

\begin{enumerate}
\item 1D example with 1D output for basic illustration. 
	\begin{enumerate}
	\item VSEM with single varied parameter. 
	\item For 1D output, consider long time average of of a single state variable. This will allow us to compare forward model and log-likelihood emulation directly.
	\item Gaussian likelihood. 
	\item Compare emulator distributions, log-likelihood emulator distributions, and likelihood emulator distributions, and various posterior approximations. 
	\item Validation metrics: RMSE, MAE, CRPS, Log-Score, Coverage. 
	\end{enumerate}
\item 2D examples for basic illustration and consideration of different posterior characteristics: Gaussian, banana, unidentifiable, concentration level, bimodal. 
\item Extension of 2D densities to 6-10 dims (see Vehtari paper numerical experiments) 
\item Sequential design performance at different levels of posterior concentration. 
\item Integrated uncertainty criterion: incorporating the current posterior via the integrand or the measure?
\item Large batch, few iteration sequential design experiment. 
(4 different combinations we could consider) 
\end{enumerate}

\begin{enumerate}
\item 1D example for basic illustration. 
\item 2D examples for basic illustration and consideration of different posterior characteristics: Gaussian, banana, unidentifiable, concentration level, bimodal. 
\item Extension of 2D densities to 6-10 dims (see Vehtari paper numerical experiments) 
\item Sequential design performance at different levels of posterior concentration. 
\item Integrated uncertainty criterion: incorporating the current posterior via the integrand or the measure?
\item Large batch, few iteration sequential design experiment. 
(4 different combinations we could consider) 
\end{enumerate}

\subsection{Potential examples:}
\begin{enumerate}
\item Banana, unimodal, bimodal, unidentifiable
\item Heat equation (see Sinsbeck and Nowak) 
\item VSEM
\end{enumerate}

\subsection{Things to consider trying}
\begin{enumerate}
\item Perhaps give some context by discussing connections to Bayesian optimization and to log-likelihood approximation used in 
simulation-based inference. 
\item Hyperparameter marginalization: need to look into opportunities for closed-form hyperparameter marginalization during 
sequential design phase. 
\item Developing a design criterion that better aligns with the MHWMC procedure; e.g., something that targets the likelihood ratio. 
\item Implementing Higdon basis function approach for comparison. 
\item Nonnegative constrained GPs (may be able to do this by modifying the kergp optimization code and using nloptr's option to add constraints) 
\item GP-accelerated MALA 
\end{enumerate}

\subsection{Limitations of existing literature}
\begin{enumerate}
\item Very little discussion of case where likelihood parameters are unknown. 
\item Lack of emphasis on batch design (with some exceptions).
\item Little guidance on which approximation/design criterion to choose.
\item Vehtari fixes the marginal approx, and focuses instead on varying the design criterion, but notes that sampling from the marginal approx is problematic. 
\end{enumerate}

\subsection{Conjectures}
\begin{enumerate}
\item The MCWMH algorithms will perform better than sampling from the marginal approx in the log-likelihood emulation setting, especially when the GP is very uncertain. 
\end{enumerate}

\subsection{Consideration}
\begin{enumerate}
\item Literature typically focuses on convergence of the approx posterior. But in cases with very expensive computer models, one might have to stop pre-convergence. 
In these settings the comparison between the approximate posteriors becomes even more important. 
\end{enumerate}



\bibliography{post_approx_with_GP_emulators} 
% \bibliographystyle{ieeetr}

\end{document}







