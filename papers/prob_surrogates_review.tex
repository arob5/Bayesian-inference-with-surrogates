\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{caption, subcaption} % Captions and sub-figures. 
\usepackage{fancyvrb} % For writing using verbatim font inline. 
\usepackage{float}
% \usepackage[demo]{graphicx}

% Bibliography
\bibliographystyle{plainnat}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Tables. 
\usepackage{multirow}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{{./../output/plots/}}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{rec}{Recommendation}

\crefname{prop}{Proposition}{Propositions}
\Crefname{prop}{Proposition}{Propositions}
\crefname{definition}{Definition}{Definitions}
\Crefname{definition}{Definition}{Definitions}
\crefname{lemma}{Lemma}{Lemmas}
\Crefname{lemma}{Lemma}{Lemmas}
\crefname{thm}{Theorem}{Theorems}
\Crefname{thm}{Theorem}{Theorems}
\crefname{corollary}{Corollary}{Corollaries}
\Crefname{corollary}{Corollary}{Corollaries}
\crefname{rec}{Recommendation}{Recommendations}
\Crefname{rec}{Recommendation}{Recommendations}

% Title and author
\title{Probabilistic Surrogates for Bayesian Inverse Problems: Posterior Approximation and Sequential Design}
\author{Andrew Roberts}

\begin{document}

\maketitle

% Introduction 
\section{Introduction}
In the modern era, data-driven and mechanistic modeling have become closely intertwined, with hybrid modeling strategies 
increasingly used to understand and predict the behavior of complex systems. Statistical techniques have become 
the standard for uncertainty quantification in such contexts, but challenges persist in adapting statistical inference procedures
to handle the computational burden imposed by expensive computer simulations. 
Within the Bayesian paradigm, there is thus growing interest in methods of Bayesian computation tailored to settings 
where evaluation of the (unnormalized) posterior density incurs significant computational expense. 
This situation commonly arises across various disciplines in the physical and engineering sciences, whereby evaluating 
the likelihood implies performing a complex computer simulation; e.g., \citep{ESM_modeling_2pt0,FerEmulation}. 
Such computer models often derive from systems of differential equations, with the goal being to estimate (i.e., calibrate) 
unknown parameters characterizing the physical model.
To address this computational bottleneck, a wide body of research has emerged that seeks to characterize the Bayesian 
posterior distribution over model parameters while minimizing the number of required posterior density evaluations. 
A popular class of methods involves replacing either the computer model, log-likelihood function, or
log-posterior density with a statistical surrogate (also called an emulator or meta-model), which induces a computationally 
cheap approximation of the posterior density.
The surrogate can be trained offline using a small set of expensive model runs (often performed in 
parallel), and plugged in as an approximation to facilitate the application of 
standard inference schemes such as Markov chain Monte Carlo (MCMC) 
\citep{modularization,BurknerSurrogate,BurknerTwoStep}.

The present work is focused on the use of \textit{stochastic} emulators in accelerating Bayesian inference,
implying that the true posterior density is modeled as a realization of a random function.
Such probabilistic surrogates acknowledge the additional uncertainty introduced by 
the approximation. We focus on the questions of how to leverage this uncertainty in constructing posterior
approximations and in designing goal-oriented sequential design (i.e., active learning) algorithms
to iteratively improve the surrogate-induced posterior.
The canonical example of a probabilistic surrogate is a Gaussian process \citet{gpML}, though 
standard Bayesian parametric models \citep{BurknerSurrogate} and neural network-based
approaches \citep{deepEnsembles,epistemicNN} are common alternatives.
Gaussian processes (GPs) have been widely used as emulators in calibrating complex
computer codes \citep{design_analysis_computer_experiments,SanterCompExp,gramacy2020surrogates}
and are increasingly being used to accelerate Bayesian inference
\citep{KOH,StuartTeck1,StuartTeck2,VehtariParallelGP,gp_surrogates_random_exploration,
quantileApprox,FerEmulation,KandasamyActiveLearning2015,CES,Kandasamy_2017,
SinsbeckNowak,Surer2023sequential,llikRBF,ranjan2016inverse,weightedIVAR,hydrologicalModel,
VillaniAdaptiveGP,GP_PDE_priors,random_fwd_models,ActiveLearningMCMC,
MCMC_GP_proposal,trainDynamics}.
This focus on GPs facilitates convenient analytical computations for both posterior density approximation and 
active learning, and allows for improved interpretability. On the other hand, the desire for analytical tractability 
limits modeling flexibility. Many existing methodologies are limited to the Gaussian setting: requiring emulators with 
Gaussian predictive distributions, or imposing a Gaussian assumption on the likelihood being approximated
\citep{StuartTeck1,Surer2023sequential,weightedIVAR,hydrologicalModel}.
Small deviations from these assumptions commonly break the closed-form computations enjoyed in this limited regime.
Therefore, in addition to providing a comprehensive review of the Gaussian setting, we emphasize recent work on 
developing general-purpose probabilistic workflows that relax the Gaussian requirements.

% Goals
\subsection{Goals}
This review is motivated by a rapidly increasing body of literature using probabilistic surrogate models
to accelerate Bayesian inference. Similar methodological work has been pursued across various disciplines, 
including Bayesian statistics, computational science and engineering, and applied mathematics, in 
addition to applied work spanning a range of scientific applications. Much of this research appears to 
be siloed within its respective discipline; one of our aims is to synthesize this related work in an effort to break down
these barriers. Our review centers on two common problems problems faced in most applications: (i.) how to
use a stochastic surrogate to approximate the Bayesian posterior, and (ii.) how to efficiently refine 
an existing surrogate.

Our second goal is to highlight different approaches to incorporating surrogate models within a Bayesian
workflow. We draw a distinction between two general strategies common in the literature:
\textit{forward model emulation}, whereby (some function of) the expensive computer model is approximated 
by an emulator, and \textit{log-density emulation}, 
where a surrogate approximates either the log-likelihood or unnormalized log-posterior density directly. 
The latter approach is a special case of the former, and both strategies ultimately induce an 
approximation of the posterior density. However, there are significant practical differences to consider
when deciding which method to use for a particular application.  
We discuss the relative merits and downsides of each approach, as well as typical use cases.

A third goal is both to synthesize the large subset of literature focusing on the use of GP emulators, 
while also highlighting generic workflows that do not require particular distributional assumptions.
Our general viewpoint allows surrogates to have generic predictive distributions, which may only
be accessible through samples. We treat the GP setting as a special case, and provide streamlined 
derivations and interpretations of quantities that are available in closed-form in this setting.

% Paper Organization
\subsection{Paper Organization}
\Cref{sec:background} begins with background on Bayesian inverse problems and Gaussian processes.
\Cref{sec:surrogate-models} provides an overview of probabilistic surrogates, highlighting distinctions
between parametric and nonparametric emulators and describing the techniques of forward model 
emulation and log-density emulation. \Cref{sec:post-approx} addresses the question of how to 
use a probabilistic surrogate to approximate the posterior distribution, highlighting the importance
of propagating surrogate uncertainty. \Cref{sec:seq-design} reviews active learning strategies for 
goal-oriented surrogate refinement. We offer a list of practical recommendations in \Cref{sec:recs},
which are emphasized in a numerical case study presented in \Cref{sec:case-study}.
In \Cref{sec:related-work} we highlight related work in computer model calibration, probabilistic numerics,
and multifidelity methods (\todo: this third one may change). We conclude in \Cref{sec:conclusion}. 
All derivations and proofs are provided in the appendix.

% Background
\section{Background} \label{sec:background}
In this section we introduce the Bayesian inference setting of interest, with an emphasis on Bayesian inverse problems
stemming from mechanistic dynamical models. We provide a concrete example of parameter estimation for ordinary 
differential equations, and conclude the section with a brief overview of Gaussian processes.
 
 \subsection{Bayesian Inverse Problems}
We focus on the Bayesian inference setting in which pointwise evaluations of the unnormalized posterior
density 
\begin{equation}
\postDens(\Par) \Def \priorDens(\Par) p(\obs \given \Par) \label{post_dens_generic}
\end{equation}
are available, but expensive owing to the cost of computing the likelihood $p(\obs \given \Par)$. This setting 
commonly arises in the Bayesian approach to inverse problems \citep{Stuart_BIP}. 
Consider a \textit{forward model} $\fwd: \parSpace \subseteq \R^{\dimPar} \to \R^{\dimObs}$ describing 
some system of interest, parameterized by input parameters $\Par \in \parSpace$. In addition, suppose that 
we have noisy observations $\obs \in \obsSpace \subseteq \R^{\dimObs}$ of the output signal that $\fwd(\Par)$ 
seeks to approximate. The \textit{inverse problem} concerns learning the parameter values $\Par$ such
 that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating} the model so that it agrees with the observations. 
 The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on $\obs \given \Par$. We assume that this distribution 
admits a density with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs \given \Par) = \Exp{\llik(\Par)}$. The notation $\llik(\Par)$ suppresses the dependence 
on $\obs$, as the observed data will be fixed throughout. We start by focusing inference only on the 
calibration parameter $\Par$, assuming that other likelihood parameters (e.g., noise covariance) 
are fixed.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution
\begin{align}
\postDensNorm(\Par) \Def p(\Par \given \obs) = \frac{1}{\normCst}\postDens(\Par) = \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant given by 
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. We will also find it useful to introduce the notation
\begin{equation}
\lpost(\Par) \Def \log \postDens(\Par) = \log \priorDens(\Par) + \llik(\Par) \label{eq:lpost}
\end{equation}
for the logarithm of the unnormalized posterior density.
When relevant, we will make explicit the dependence on the forward 
model by writing $\llik(\Par; \fwd)$, $\lpost(\Par; \fwd)$, and $\postDens(\Par; \fwd)$. 

In addition to considering 
generic likelihoods, we give special attention to the additive Gaussian noise model
\begin{align}
&\obs = \fwd(\Par) + \noise,
&&\noise \sim \Gaussian(0, \likPar) \label{inv_prob_Gaussian} 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par; \fwd) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) 
= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)), \label{llik_Gaussian}
\end{align}
as this model features prominently in the inverse problems literature.
In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are serial in nature, often requiring $\sim 10^5 - 10^7$ iterations, with each 
iteration involving an evaluation of the unnormalized posterior density $\postDens(\Par; \fwd)$. 
In the inverse problem context, this computational requirement is often prohibitive when the forward model 
evaluations $\fwd(\Par)$ incur significant computational cost, as each density evaluation requires the 
expensive computation $\fwd(\Par)$. Motivated by this problem, a large body of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\lpost(\Par)$ (note that approximating the former induces 
an approximation of the latter). We focus on surrogate models that take the form of statistical 
regression models approximating the maps $\Par \mapsto \fwd(\Par)$ or $\Par \mapsto \lpost(\Par)$.
\footnote{This is in contrast to other surrogate modeling strategies (e.g., reduced-order modeling)
that exploit the specific structure of the forward model; see \citet{multifidelityReview} for a detailed review.}
We refer to methods that explicitly model the former map as \textit{forward model emulation}, and those that 
model the latter as \textit{log-density emulation}. We also include methods that emulate the log-likelihood
map $\Par \mapsto \llik(\Par)$ in this latter category (see \Cref{sec:llik_vs_lpost}).
Throughout this review, we will typically view the functions
$\fwd(\Par)$, $\llik(\Par)$, and $\lpost(\Par)$ as computationally expensive black-boxes. In applications,
the forward model often takes the form of a costly computer simulation, so we will occasionally refer to 
queries to these functions as simulator evaluations.
The following section provides a concrete example of such a simulation
model stemming from the numerical solution of differential equations. 
 
\subsection{Motivating Example: Parameter Estimation for ODEs} \label{dynamical_models}
We now demonstrate how the problem of estimating the parameters of differential equation models 
 can be cast within the Bayesian inverse problem framework. Our primary motivating applications 
 stem from the use of dynamical models in Earth system modeling  \citep{ESM_modeling_2pt0,paramLSM}.
Consider a parameter-dependent initial value problem 
\begin{align}
\frac{d}{d\Time} \state(\Time, \Par) &= \odeRHS(\state(\Time, \Par), \Par), &&x(\timeStart) = \stateIC, \label{ode_ivp}
\end{align}
describing the time evolution of $\dimState$ state variables 
$\state(\Time, \Par) \Def \left[\indexState{\state}(\Time, \Par)\right]_{\stateIndex=1}^{\dimState}$
with dynamics depending on $\Par \in \parSpace$. As our focus will be on estimating these parameters 
from observations, we consider the parameter-to-state map
\begin{align}
\Par &\mapsto \left\{\state(\Time, \Par) :  \Time \in [\timeStart, \timeEnd] \right\}.
\end{align}
In practice, the solution is typically approximated via a numerical discretization of the form 
\begin{align}
\solutionOp: \Par &\mapsto \left[\indexTime[0]{\state}(\Par), \dots, \indexTime[\NTimeStep]{\state}(\Par) \right]^\top. \label{eq:ode-solution-op}
\end{align}
Here, $\solutionOp: \parSpace \to \R^{\NTimeStep \times \dimState}$ represents the map induced by a numerical solver, 
and $\indexTime[0]{\state}(\Par), \dots, \indexTime[\NTimeStep]{\state}(\Par)$ are approximations of the state 
values $\state(\Time, \Par)$ at a finite set of time points in $[\timeStart, \timeEnd]$. 
Going forward, we focus on the discrete-time operator $\solutionOp$, neglecting discretization error
for simplicity. Finally, suppose we have observed data $\obs \in \obsSpace \subseteq \R^{\dimObs}$ that we model as a
noise-corrupted function of the state trajectory. This is formalized by the definition of an observation operator 
$\obsOp: \R^{\NTimeStep \times \dimState} \to \obsSpace$ mapping from the state trajectory to a 
$\dimObs$-dimensional observable quantity. We assume the observations are generated as 
\begin{align}
\obs &= (\obsOp \circ \solutionOp)(\ParTrue) + \noise \label{ode_inv_prob} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber 
\end{align}
for some ``true'' parameter value $\ParTrue \in \parSpace$. Observe that this falls within the generic 
inverse problem framework with forward model $\fwd \Def \obsOp \circ \solutionOp$. 
Generalizations can be considered to settings with non-additive or non-Gaussian noise, or the 
presence of model discrepancy. \Cref{sec:case-study} presents a numerical case study using 
a problem of this form, motivated by applications to near-term forecasting of the 
terrestrial carbon cycle.

 \subsection{Gaussian Processes} \label{gp_review}
 In this section, we provide a brief review of Gaussian processes; for in-depth treatments, we refer 
 readers to \cite{gramacy2020surrogates, StuartTeck2, gpML}. We also discuss basic extensions
 to Gaussian process models for multi-output functions, which will be relevant in various examples
we discuss later.
 
 \subsubsection{Single Output Gaussian Processes}
 A Gaussian process (GP) can be thought of as a
random function $\funcPrior: \parSpace \to \R$, defined by the property that 
the random vector $[\funcPrior(\Par_1), \dots, \funcPrior(\Par_\Ndesign)]^\top$ has
a joint Gaussian distribution for any finite set of inputs $\parMat \Def \{\Par_1, \dots, \Par_\Ndesign\} \subset \parSpace$. 
A GP is defined by its mean function $\gpMeanPrior: \parSpace \to \R$ and positive-definite kernel
(i.e., covariance function) $\gpKerPrior: \parSpace \times \parSpace \to \R$ and is thus denoted by 
$\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$. Throughout this paper, we will vectorize notation as follows. Letting 
$\tilde{\parMat} \Def [\tilde{\Par}_1, \dots, \tilde{\Par}_M]  \subset \parSpace$ be a second set of inputs, we define 
$\funcPrior(\parMat) \Def [\funcPrior(\Par_1), \dots, \funcPrior(\Par_\Ndesign)]^\top \in \R^{\Ndesign}$, 
$\gpKerPrior(\parMat, \tilde{\parMat}) \Def \{\gpKerPrior(\Par_{\idxDesign}, \tilde{\Par}_m)\}_{\substack{1 \leq \idxDesign \leq \Ndesign \\ 1 \leq m \leq M}} \in \R^{\Ndesign \times M}$, 
and $\gpKerPrior(\parMat) \Def \gpKerPrior(\parMat, \parMat) \in \R^{\Ndesign \times \Ndesign}$. 
We similarly use this vectorized notation for other functions; e.g., $\gpMeanPrior(\parMat)$, $\fwd(\parMat)$, and $\llik(\parMat)$.
Given this notation, 
the defining property of the GP can be written as 
\begin{align}
\funcPrior(\parMat) &\sim \Gaussian(\gpMeanPrior(\parMat), \gpKerPrior(\parMat)).
\end{align}
For our purposes, the GP $\funcPrior$ will represent a prior distribution for a \textit{deterministic} function 
$\func: \parSpace \to \R$, such that the randomness in $\funcPrior$ encodes our epistemic 
uncertainty about $\func$ \citep{epistemicAleatoric}.
The underlying ideas explored here can also be extended to the setting where evaluations of $\func$ 
are corrupted by noise \citep{stochasticComputerModels,VehtariParallelGP,OakleyllikEm}. 
If we observe noiseless function evaluations $\func(\designIn)$ at a set of inputs $\designIn$, then we can consider
the conditional distribution $\funcPrior|[\funcPrior(\designIn)=\func(\designIn)]$. It is well-known
\citep{gpML} that the conditional is also a GP
\begin{align}
\funcEm[\Ndesign] \Def \funcPrior|[\funcPrior(\designIn)=\func(\designIn)]\sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign]), \label{generic_gp_conditional}
\end{align}
with mean and kernel given by
\begin{align}
\gpMean[\Ndesign](\parMat) &= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} [\func(\designIn) - \gpMeanPrior(\designIn)] \label{kriging_eqns} \\
\gpKer[\Ndesign](\parMat) &= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} \gpKerPrior(\designIn, \parMat). \nonumber
\end{align}

We will refer to observed input locations $\designIn$ as \textit{design points}, and the conditional
distribution $\GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$ as the \textit{predictive distribution}.
The predictive mean can be seen to interpolate the design points (i.e., $\gpMean[\Ndesign](\designIn) = \func(\designIn[\Ndesign])$), with 
the predictive variance vanishing at these inputs (i.e., $\gpKer[\Ndesign](\designIn) = 0$). 
To break these interpolation properties,
a constant can be added to the diagonal of the kernel matrix so that $(\gpKerPrior(\designIn) + \nuggetSD^2 I)^{-1}$ replaces 
$\gpKerPrior(\designIn)^{-1}$ in \cref{kriging_eqns}. In the present context of deterministic forward models, 
this interpolation property is typically desirable. Nevertheless, a small, positive fixed constant $\nuggetSD^2$ is still 
typically included for numerical stability. 

Appropriate choices for the prior mean $\gpMeanPrior$ and covariance $\gpKerPrior$ depend on the 
particular application, with common choices for the mean including a constant or a linear model of the form 
$\gpMeanPrior(\Par) = \beta^\top \phi(\Par)$, for some feature map $\phi$. A typical default choice of 
covariance function is the exponentiated quadratic (also called squared exponential, Gaussian, or radial basis function)
kernel,
\begin{align}
\gpKerPrior(\Par, \tilde{\Par}) &= 
\margSD^2 \Exp{- \sum_{\idxParDim=1}^{\dimPar} \left(\frac{\Par_{\idxParDim} - \tilde{\Par}_{\idxParDim}}{\lengthscale_{\idxParDim}} \right)^2}, \label{Gaussian_kernel}
\end{align}
defined by the choice of positive hyperparameters $\margSD, \lengthscale_{1}, \dots, \lengthscale_{\dimPar}$. 
This is an example of a stationary (i.e., shift-invariant) kernel since $\gpKerPrior(\Par, \tilde{\Par})$ only depends 
on its arguments through their difference $\Par - \tilde{\Par}$. 
A common approach to set the mean and kernel hyperparameters is the empirical Bayes method of fixing
their values at their maximum (marginal) likelihood estimates, which conveniently retains the Gaussian
predictive distribution in \cref{generic_gp_conditional} at the expense of failing to acknowledge uncertainty
in the hyperparameters. The fully Bayesian alternative significantly increases computation, 
and results in a non-Gaussian predictive distribution after marginalizing the hyperparameters. 

\subsubsection{Multiple Output Gaussian Processes}
We now consider the extension to multi-output functions $\funcPrior: \parSpace \to \R^{\dimObs}$. 
There is a large literature on multi-output kernels \citep{multiOutputKernels}, but we focus here on the 
simplest case in which each output is modeled separately as an independent GP. 
In this setting, the notation $\gpKerPrior(\Par)$ is now interpreted as the $\dimObs \times \dimObs$ 
diagonal matrix collecting the variance of each independent GP. Similarly, for a set of inputs 
$\parMat \in \R^{M \times \dimPar}$, $\gpKerPrior(\parMat)$ is now the 
$M\dimObs \times M\dimObs$ block diagonal matrix constructed from the covariances of 
each independent GP.
 
This basic multi-output strategy becomes problematic when the output dimension $\dimObs$ 
is large or the outputs are highly interdependent, rendering the independent GP approach infeasible or ill-advised.
A popular alternative is to approximate the function output as a linear combination of a small number
of basis vectors, with the coefficients modeled as independent GPs; that is, 
\begin{align}
\funcPrior(\Par) &= \sum_{\idxBasis=1}^{\dimBasis} \basisWeight_{\idxBasis}(\Par) \basisVec_{\idxBasis} + \noise_{\basisVec} \label{basis_func_model} \\
\basisWeight_{\idxBasis} &\overset{\textrm{ind}}{\sim} \GP(\emMean[0]{(\idxBasis)}, \emKer[0]{(\idxBasis)}), \label{basis_func_GPs} \\
 \noise_{\basisVec} &\sim \Gaussian(0, \sigma^2_{\basisVec}), \label{basis_func_noise}
\end{align}
where the $\basisVec_{\idxBasis}$ are fixed basis vectors.
These basis vectors are commonly constructed via a singular value decomposition, though other 
bases are also possible \citep{HigdonBasis, emulate_functional_output, functionValuedModels, PODemulation}.
Under this model, the univariate GPs $\basisWeight_{\idxBasis}$ are fit and conditioned independently, then 
plugged back into \cref{basis_func_model} to construct the predictive distribution. The linear Gaussian 
structure of \cref{basis_func_model} implies that this approach retains a Gaussian predictive distribution
for $\funcEm[\Ndesign]$, conditional on the hyperparameters.

% Initial Design and Surrogate Models
\section{Initial Design and Surrogate Models} \label{sec:surrogate-models}
We begin our review by summarizing the surrogate modeling workflow in the context of solving
Bayesian inverse problems. We highlight commonly-used surrogates, emphasizing
models that provide a probabilistic representation of emulator uncertainty in addition to 
standard point predictions. We conclude the section with discussions of the tradeoffs
between the forward and log-density emulation strategies. Our aim is not to investigate any specific 
model in depth; see \Cref{sec:recs,sec:case-study} for practical recommendations.

\subsection{Initial Design}
Before discussing specific emulators, we comment on the general process of surrogate fitting.
We focus exclusively on a modular workflow \citep{modularization}, such that the surrogate is
 trained only on data produced
by the simulator, as opposed to jointly learning the surrogate with the parameters in a 
unified Bayesian model (e.g., as in the seminal work \citet{KOH}). Fitting the surrogate requires 
generating training data by evaluating the simulator at a chosen set of input parameters
$\designIn \Def \{\Par_1, \dots, \Par_{\Ndesign}\}$. We refer to $\designIn$ as the \textit{initial design}.
Depending on which map is being emulated, the training data for the emulator will take the form 
$\{\designIn, \fwd(\designIn)\}$, $\{\designIn, \llik(\designIn)\}$, or $\{\designIn, \lpost(\designIn)\}$. 
In the computer experiments literature, 
the initial design is typically selected to satisfy some sort of ``space-filling'' criterion; canonical examples 
include latin hypercube designs and Sobol sequences \citep{initDesignReview}. In the present setting, a natural 
approach is to sample $\designIn$ from the prior $\priorDens$, either via simple Monte Carlo or using 
algorithms that seek to minimize a specific notion of discrepancy \citep{supportPoints, SteinPoints}.
As we will demonstrate in experiments, fitting emulators on prior designs may simultaneously produce 
reasonable approximations in a prior-averaged sense, and poor approximations in a posterior-averaged
sense. This is due to a common phenomenon in Bayesian inverse problems, whereby the posterior is highly 
concentrated relative to the prior \citep{StuartTeck2,PCEBIP}. Even in parameter spaces of moderate dimension, 
it is common that the initial design may contain few or no points in the region of high posterior mass. 
This is one of the central challenges of surrogate modeling in this setting, and motivates the use of sequential 
design algorithms to iteratively construct the design in order to target high-posterior regions (see \Cref{sec:seq-design}).

Another approach to this problem is to first run an alternative algorithm (e.g., an approximate 
sampler or optimizer) to produce design points in regions of high posterior mass, which can then 
be used as part of an initial design to fit an emulator. This general strategy is exemplified by the 
\textit{calibrate, emulate, sample} workflow \citep{CES,idealizedGCM,CESSoftware,FATES_CES,adaptiveMultimodal}, 
which uses Ensemble Kalman methods to produce the initial design. In similar spirit, the earlier work 
\citet{emPostDens} utilizes a sequence of importance sampling steps to construct a Gaussian 
approximation of the posterior, which is then sampled from to produce an initial surrogate design.
We do not discuss these methods further in this review, but note that continued methodological 
development investigating such workflows may prove a fruitful avenue for future research. 

\subsection{Probabilistic Surrogates}
Our focus in this review is on probabilistic (i.e., random/stochastic) surrogates, which 
are emulator models that provide a probabilistic description of approximation uncertainty.
We review common models falling within this category, highlighting the distinction
between parametric and nonparametric surrogates, and discuss surrogate strategies within
the Bayesian inference context.

\subsubsection{Common Models}

\paragraph{Gaussian Processes.}
Gaussian processes (GPs) have emerged as a canonical surrogate model, and provide the 
basis for examples given throughout this review (refer to \Cref{gp_review} for GP
background and notation). GPs have been extensively applied in a wide variety of 
surrogate modeling tasks, including as emulators for solving Bayesian inverse problems.
The Gaussian predictive distribution of a 
GP facilitates many convenient closed-form computations, which we will derive and present 
throughout this review. Extensions of the GP methodology can produce more flexible 
emulators with non-Gaussian predictive distributions. For example, fully Bayesian 
GPs \citep{fullyBayesianGPs} and deep GPs \citep{deepGPVecchia,deepGPAL}
typically yield predictions in the form of (infinite) mixtures of Gaussians.

\paragraph{Polynomials.}
Surrogates that consist of linear combinations of polynomial basis functions are 
commonly used in the engineering and applied math communities. Polynomial 
chaos expansions (PCE) are a particular example whereby a polynomial 
basis is used to approximate a random variable $\func(\Par)$ as a function of
a random input $\Par \sim \priorDens$. In particular, the expansion is of the form
\begin{equation}
\funcEm[\dimBasis](\Par) = \sum_{\idxBasis=1}^{\dimBasis} c_{\idxBasis} \basisVec_{\idxBasis}(\Par),
\end{equation}
where $\basisVec_{1}, \dots, \basisVec_{\dimBasis}$ are orthogonal polynomials with respect 
to the distribution $\priorDens$ on the inputs. Once constructed, a PCE is often used to 
approximate statistical moments of $\func(\Par)$. More relevant to our context,
the map $\funcEm[\dimBasis](\Par)$ can also be used as a surrogate for $\func(\Par)$.
With fixed coefficients $c_{\idxBasis}$, this map is deterministic and thus PCEs do not fall
within our definition of probabilistic surrogates. We nonetheless highlight them here
due to their popularity, the fact that they can be converted into random surrogates by 
considering Bayesian treatments of the coefficients 
\citep{BayesianPCE1,BayesianPCE2,BurknerSurrogate},
and their use in conjunction with probabilistic surrogates (e.g., as the mean function
of a GP; \citet{PCEGPWind,PCEGP2,SinsbeckNowak}). PCE surrogates have been
employed to accelerate Bayesian inversion in various applications 
\citep{dimRedPolyChaos,BurknerSurrogate,PCEBIP}.

\paragraph{Neural Networks.}
There has been growing interest in the use of neural networks as surrogate models, 
which are well-suited to the regime where both the input dimension $\dimPar$ 
and computational budget $\Ndesign$ are large. Bayesian treatment of neural 
network parameters provides probabilistic predictions, but in practice significant 
approximations are required for inference \citep{BayesOptNN}. One popular simplification
treats only the final neural network layer in a Bayesian fashion 
\citep{BayesLastLayer,BayesOptBayesLastLayer}. Motivated by these difficulties,
there has also been growing interest in surrogate models with predictive distributions 
not necessarily rooted in the Bayesian philosophy. This includes deep ensembles, 
which summarize uncertainty via an ensemble of neural network 
models \citep{deepEnsembles,Lueckmann2018LikelihoodfreeIW},
and epistemic neural networks \citep{epistemicNN,BayesOptEpistemicNN}. 

\subsubsection{Parametric vs. Nonparametric Models}
The distinction between parametric and nonparametric surrogate models will 
prove significant at several points throughout this review. We illustrate this
distinction for a generic random surrogate $\funcEm[\Ndesign]$ for some 
function $\func$. 

\paragraph{Parametric Model.}
We refer to $\funcEm[\Ndesign]$ as parametric 
or finite-dimensional if the randomness in $\funcEm[\Ndesign]$ stems from 
a finite-dimensional parameter $\theta_{\Ndesign}$; i.e., 
$\funcEm[\Ndesign](\cdot) = g(\cdot; \theta_{\Ndesign})$ for some 
random vector $\theta_{\Ndesign}$ and non-random function $g$.
In this case, it is feasible to sample trajectories (sample paths) of 
$\funcEm[\Ndesign]$ via 
\begin{align}
&\func_{\mathrm{samp}}(\cdot) \Def g(\cdot, \theta_{\mathrm{samp}}),
&&\theta_{\mathrm{samp}} \sim \mathrm{law}(\theta_{\Ndesign}).
\end{align}
The sampled function $\func_{\mathrm{samp}}(\cdot)$ can now 
be evaluated at any input value. A standard example is a linear 
regression model of the form
\begin{align}
\funcEm[\Ndesign](\cdot) &= \sum_{\idxBasis=1}^{\dimBasis} (\theta_{\Ndesign})_{\idxBasis} \basisVec_{\idxBasis}(\cdot).
\label{eq:finite-basis}
\end{align}
for some fixed basis vectors $\basisVec_{1}, \dots, \basisVec_{\dimBasis}$.
 
\paragraph{Nonparametric Model.} For a nonparametric model, no such finite-dimensional 
parameter $\theta_{\Ndesign}$ exists. In this setting, a generic view of a 
surrogate model is as a map from finite sets of inputs $\parMat$ to a predictive distribution 
over the associated responses $\func(\parMat)$. This viewpoint has formed the basis 
for the generic implementation of surrogate models in software packages such as 
BoTorch \citep{botorch}. It is therefore feasible to sample the finite-dimensional distributions
$\funcEm[\Ndesign](\parMat)$ for finite input sets $\parMat$, but sampling trajectories
$\func_{\mathrm{samp}}(\cdot) \sim \mathrm{law}(\funcEm[\Ndesign])$ would require 
infinite computing resources. Practical techniques to sample approximate trajectories have been
extensively studied when $\funcEm[\Ndesign]$ is a GP, including work in 
Bayesian inverse problems \citep{dimRedPolyChaos,functionSpaceMCMC} 
and Bayesian optimization \citep{pathwiseConditioning,samplingGPPosts}.
Common approaches construct finite-rank approximations of the form  
in \Cref{eq:finite-basis}.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth} % GP dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/fwd_dist_fwdem.png}}
         \caption{$\fwdEm[\Ndesign]$}
         \label{fig:fwd_dist_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % llik dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/llik_dist_fwdem.png}}
         \caption{$\llik(\cdot; \fwdEm[\Ndesign])$}
         \label{fig:llik_dist_fwd}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % lik dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lik_dist_fwdem.png}}
         \caption{$\Exp{\llik(\cdot; \fwdEm[\Ndesign])}$}
         \label{fig:lik_dist_fwdem}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth} % GP dist [llik] (equal to llik dist so excluding)
     \includegraphics[width=\textwidth]{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % llik dist [llik] 
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/llik_dist_llikem.png}}
         \caption{$\llikEm[\Ndesign]$}
         \label{fig:llik_dist_llikem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % lik dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lik_dist_llikem.png}}
         \caption{$\Exp{\llikEm[\Ndesign]}$}
         \label{fig:lik_dist_llikem}
     \end{subfigure}
        \caption{Comparison of forward model and log-likelihood emulation in the simple one-dimensional problem 
        ($\dimPar = \dimObs = 1$) described in $\todo$. The top and bottom rows correspond to the forward model 
        emulation and log-likelihood emulation settings, respectively. The columns correspond to (1) the forward model emulator;
        (2) the log-likelihood emulator induced by the underlying GP; 
        (3) the likelihood emulator induced by the underlying GP. When directly emulating the 
        log-likelihood, the underlying GP is itself the log-likelihood emulator, and there is no forward model emulator.
        The black line is the ground truth and the red points are the design points used for emulator training. 
        The blue line is the mean of the respective emulators, and the shaded area corresponds to 90\% credible 
        intervals for the predictive distributions.}
        \label{fig:em_dist_1d}
\end{figure}

\subsection{Surrogate Targets for Bayesian Inference}
We now begin our discussion of emulation strategies geared towards the goal of Bayesian inversion,
which differ based on the function of $\Par$ that is targeted for emulation.
We group these strategies into the broad categories of \textit{forward model emulation}
and \textit{log-density emulation}. This dichotomy is also explored in 
\citet{StuartTeck1,GP_PDE_priors,random_fwd_models}, and briefly noted in 
\citet{Surer2023sequential,trainDynamics,ActiveLearningMCMC,emPostDens}.

\subsubsection{Forward Model Emulation}
One natural approach is to fit an emulator approximating the forward model map $\Par \mapsto \fwd(\Par)$,
which entails fitting a predictive model to a design $\{\designIn, \fwd(\designIn)\}$. 
We let $\fwdEm[\Ndesign]$ denote a stochastic surrogate that has been fit to this design,
thus representing a distribution over functions that summarizes the uncertainty about the 
numerically-inaccessible function $\fwd$. We let $\emMean[\Ndesign]{\fwd}(\Par)$ and 
$\emKer[\Ndesign]{\fwd}(\Par)$ denote the mean and variance of the random 
variable $\fwdEm[\Ndesign](\Par)$, even when $\fwdEm[\Ndesign]$ is not a GP.
Plugging the random function $\fwdEm[\Ndesign]$ in place of $\fwd$ in the log-likelihood 
induces random approximations of the likelihood and posterior. In general, we use 
$\llikEm[\Ndesign]$ to denote a stochastic approximation to the log-likelihood, induced
by an underlying emulator fit to $\Ndesign$ design points. We write 
$\llikEm[\Ndesign](\Par) = \llik(\Par; \fwdEm[\Ndesign])$ when it is necessary to specify that
the log-likelihood approximation is induced by a forward model emulator. We similarly use 
the notation $\postEm(\Par) = \postDens(\Par; \fwdEm[\Ndesign])$ and 
$\lpostEm(\Par) = \lpost(\Par; \fwdEm[\Ndesign])$ for the induced unnormalized 
posterior density and log posterior density approximations. The induced distributions for 
these quantities depend on the predictive distribution of the forward model emulator, as
well as the structure of the likelihood function and prior density. The first row of \cref{fig:em_dist_1d} 
demonstrates how a GP forward model emulator induces predictive distributions 
for the log-likelihood and likelihood. The specific structure of the surrogate, which depends heavily on the 
characteristics of $\fwd$, is not the focus of this review. However, we do note that 
a common challenge in emulating forward models stems from the fact that the observation 
space (i.e., the output space of $\fwd(\Par)$) can be very high-dimensional. 
Such complications commonly arise in settings with spatial or temporal structure, 
such as epidemic modeling \citep{FadikarAgentBased},
engineering design \citep{PODemulation}, ecological forecasting 
\citep{emPostDens,DagonCLM}, and climate modeling \citep{ESM_modeling_2pt0,idealizedGCM}.
One popular approach to deal with this challenge is to approximate the forward model output
as a linear combination of a small number of basis vectors, and then emulate the scalar coefficients 
of these vectors \citep{HigdonBasis,FadikarAgentBased,PODemulation}. In \Cref{basis_func_GPs}, 
we have already defined such a model in the GP setting.
Alternatively, surrogates 
can be fit to low-dimensional summaries of the high-dimensional output, such as spatial or 
temporal averages \citep{ESM_modeling_2pt0,idealizedGCM,CLMBayesianCalibration,CLMSurrogates}.
A great deal of other approaches have been proposed, including emulators designed 
specifically for dynamical models \citep{GP_dynamic_emulation, Bayesian_emulation_dynamic, Liu_West_dynamic_emulation, dynamic_nonlinear_simulators_GP}.
Log-density emulation, described in \Cref{log_density_emulation}, offers another avenue for 
dimension reduction when the observation space is high-dimensional.

\paragraph{Gaussian Setting.}
We now consider the special case where both the likelihood and $\fwdEm[\Ndesign]$ are Gaussian.
These two distributional assumptions combine to allow closed-form computations, and thus have 
been considered frequently in the literature \citep{StuartTeck1,GP_PDE_priors,hydrologicalModel,Surer2023sequential,VillaniAdaptiveGP,weightedIVAR,idealizedGCM,CES}.

\begin{prop} \label{prop:fwd-em-Gaussian}
Assume that $\fwdEm[\Ndesign]$ is a forward model emulator with Gaussian predictive distribution; 
that is, $\fwdEm[\Ndesign](\Par) \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par))$
for all $\Par \in \parSpace$. Then, in the additive Gaussian likelihood setting, the induced unnormalized 
posterior density surrogate 
$\postDens(\Par; \fwdEm[\Ndesign]) = \priorDens(\Par)\Gaussian(\obs| \fwdEm[\Ndesign](\Par), \likPar)$
has mean and variance given by
\begin{align*}
\E_{\fwdEm}\left[\postDens(\Par; \fwdEm[\Ndesign]) \right] 
&= \priorDens(\Par) \Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par)) \\
\Var_{\fwdEm}\left[\postDens(\Par; \fwdEm[\Ndesign]) \right]
&= \priorDens^2(\Par) \left[\frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)  \right)}{\det(2\pi \likPar)^{1/2}}
- \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\left[\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right]  \right)}{\det(2\pi [\likPar + \emKer[\Ndesign]{\fwd}(\Par)])^{1/2}}\right]
\end{align*}
\end{prop}

The canonical example of a Gaussian surrogate is a GP, which is defined by specifying a prior distribution  
$\fwdPrior \sim \GP(\emMeanPrior{\fwd}, \emKerPrior{\fwd})$ for $\fwd$. Conditioning on the design yields 
the predictive distribution 
$\fwdEm[\Ndesign]  \Def \fwdPrior|[\fwdPrior(\designIn) = \fwd(\designIn)] \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$, 
which constitutes the forward model emulator.
Unless otherwise indicated, we assume the mean and kernel hyperparameters are
fixed, so that the GP predictive distribution remains Gaussian. This is a common assumption in the literature, 
and facilitates the presentation of analytical expressions (such as \Cref{prop:fwd-em-Gaussian})
that aid intuition. We will make clear when such assumptions are being 
leveraged, and later highlight methodology that is agnostic to the predictive distribution of the surrogate model.

In the typical case that the observation space has dimension $\dimObs > 1$, then we interpret 
$\emMeanPrior{\fwd}, \emKerPrior{\fwd}$ as multi-output mean and covariance functions, respectively.
If $\dimObs$ is reasonably small, then it may be appropriate to simply fit an independent GP to 
each scalar output variable. However, if $\dimObs$ is large or the outputs are highly structured, 
then the independent GP approach may be infeasible.  
In such contexts, a popular 
alternative is to leverage the basis reduction strategy in \cref{basis_func_model,basis_func_GPs,basis_func_noise}.
The surrogate predictive distribution remains Gaussian under this model, and thus \cref{prop:fwd-em-Gaussian} still applies in this setting. The particular setup of a Gaussian likelihood with a GP forward model emulator using the basis vector approach is explored in \citet{Surer2023sequential}. 

\subsubsection{Log-Density Emulation} \label{log_density_emulation}
Observe in \cref{post_dens} that the expensive forward model evaluations appear in the posterior density only through the 
likelihood. Thus, for the purposes of posterior inference, it is sufficient to directly approximate either
$\Par \mapsto \llik(\Par)$ or $\Par \mapsto \lpost(\Par)$ without obtaining an approximation of the forward model.
This implies fitting a surrogate model to the design $\{\designIn, \llik(\designIn)\}$ or $\{\designIn, \lpost(\designIn)\}$,
which results in emulators $\llikEm[\Ndesign]$ and $\lpostEm[\Ndesign]$, respectively.
As with forward model emulators, we generically use $\emMean[\Ndesign]{\llik}(\Par)$ and $\emKer[\Ndesign]{\llik}(\Par)$ 
to denote the mean and variance of $\llikEm[\Ndesign](\Par)$. We likewise write $\emMean[\Ndesign]{\lpost}(\Par)$ and 
$\emKer[\Ndesign]{\lpost}(\Par)$ for the mean and variance of $\lpostEm[\Ndesign](\Par)$.
We refer to these two approaches collectively as \textit{log-density emulation}.
As with forward model surrogates, log-density surrogates induce a random approximation $\postEm[\Ndesign]$ of the 
unnormalized posterior density; we write $\postDens(\Par; \llikEm[\Ndesign])$ and $\postDens(\Par; \lpostEm[\Ndesign])$
when it is necessary to emphasize the quantity that is directly emulated.
The second row of \cref{fig:em_dist_1d} demonstrates how a GP log-likelihood emulator induces a predictive 
distribution for the likelihood.
The log-likelihood emulation approach is considered in 
\citet{VehtariParallelGP,FATES_CES,trainDynamics,quantileApprox,ActiveLearningMCMC,FerEmulation,
StuartTeck1,random_fwd_models,GP_PDE_priors,OakleyllikEm}.
Log-likelihood emulation has also been utilized for Bayesian quadrature for numerical 
integration \citep{BayesQuadrature,BayesQuadRatios} and approximate Bayesian 
computation \citep{llikEmABC}.
The closely-related unnormalized log-posterior density (i.e., joint parameter-data density) emulation
strategy is used in \citet{emPostDens,Kandasamy_2017,llikRBF,gp_surrogates_random_exploration,landslideCalibration}.
We colloquially refer to this latter approach as \textit{log-posterior emulation} for brevity.
A related method is used in \citep{wang2018adaptive,adaptiveMultimodal}, in which surrogates are constructed 
to approximate a sequence of functions designed to be more regular than the log-likelihood.   
In general, emulating densities on the log scale is typically preferred as a way to improve numerical stability,
enforce non-negativity in the density approximation, and yield a smoother target function for emulation.
Perhaps the most significant benefit of this approach is the reduction to a scalar-valued output quantity for emulation,
as opposed to the potentially high-dimensional output space of the forward model. This notion is referred 
to as \textit{scalarization} in \citet{ranjan2016inverse, trainDynamics}.
On the other hand, log-density emulation has several downsides. Even on the log scale, log-densities can be fast-varying and 
exhibit large dynamic range, proving troublesome for surrogate models that assume stationarity \cite{wang2018adaptive}.
A simple toy example is used in \citep{Surer2023sequential} to demonstrate such modeling challenges, and the 
authors conclude that forward model emulation is preferred in their application of interest. A second potential weakness 
stems from the fact that log-densities often have known bound constraints, which may be difficult to enforce 
for certain surrogate models. \citet{quantileApprox} investigates the enforcement of 
bound constraints in GP-based log-likelihood emulation. In our numerical experiments, we demonstrate that ignoring
such constraints can yield very poor posterior approximations [\todo: maybe include a 1d example in the
recommendations section?]. A third challenge follows from the fact that 
the likelihood parameters (e.g., $\likPar$ in \Cref{inv_prob_Gaussian}) are typically not known and must also be 
learned from data. An obvious solution is to extend the input space of the emulator to include the likelihood parameters
\citep{llikRBF,emPostDens}, but the resulting response surface may prove more challenging to emulate. 
This approach also increases both the input dimensionality of the emulator, as well as the number of design points required to achieve 
a reasonable fit. Various approaches are explored in \citet{llikRBF} for dealing with nuisance likelihood parameters
without requiring additional runs of the expensive simulator. Alternatively, specific likelihood 
choices may admit a sufficient statistic which can be emulated independently of the likelihood parameters. 
An example of this approach is given in the discussion of the Gaussian setting below.

\paragraph{Log-Likelihood vs. Log-Posterior Emulation.} \label{sec:llik_vs_lpost}
We take a moment to comment on the the distinction between emulating the log-likelihood versus
the log-posterior. The literature summary above indicates that both approaches are commonly used;
we are not aware of any cases in which the performance of the two methods is compared. 
Indeed, the choice appears inconsequential, as one emulator can easily be converted into the other 
by adding or subtracting the log-prior density, which represents a deterministic shift to the predictive 
distribution. For example, given a fit log-likelihood emulator $\llikEm[\Ndesign]$, a log-posterior emulator 
can be constructed via
\begin{equation}
\lpostEm[\Ndesign](\Par) \Def \llikEm[\Ndesign](\Par) + \log \priorDens(\Par),
\end{equation}
which simply adjusts the predictive mean. Despite the similarity, there are potential tradeoffs from 
a modeling standpoint. One benefit of emulating the log-posterior is the qualitative knowledge 
that the tails must decay, which can (and should) be leveraged in designing an appropriate 
surrogate model. The tail behavior of $\llik$ is not always well-known a priori, which can 
present more of a modeling challenge. On the other hand, the downside of emulating $\lpost$ is
that the prior is being approximated, and therefore cannot be relied on to decay in the tails.
These tradeoffs are related to the challenge of ensuring the emulator induces a well-defined
posterior approximation, a topic we discuss in \Cref{sec:existence}. 

\paragraph{Gaussian Setting.} \label{ldens_Gaussian}
The predictive distribution of $\postEm[\Ndesign]$ induced
by a Gaussian log-density emulator is easily characterized in closed-form. 
The following result is the analog of \Cref{prop:fwd-em-Gaussian} in the forward model 
emulation setting.

\begin{prop} \label{prop:llik-em-Gaussian}
Assume that $\lpostEm[\Ndesign]$ is a log-posterior emulator with Gaussian predictive distribution; 
that is, $\lpostEm[\Ndesign](\Par) \sim \Gaussian(\emMean[\Ndesign]{\lpost}(\Par), \emKer[\Ndesign]{\lpost}(\Par))$
for all $\Par \in \parSpace$. Then the induced unnormalized posterior density surrogate 
$\postEm[\Ndesign](\Par) = \Exp{\lpostEm[\Ndesign](\Par)}$ is given by
\begin{align}
\postEm[\Ndesign](\Par) &\sim \text{LN}\left(\emMean[\Ndesign]{\lpost}(\Par), \emKer[\Ndesign]{\lpost}(\Par)\right),
\end{align}
which implies 
\begin{align}
\E\left[\postEm[\Ndesign](\Par)\right] &= \Exp{\emMean[\Ndesign]{\lpost}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\lpost}(\Par)}, \\
\Var\left[\postEm[\Ndesign](\Par)\right] &= \left[\Exp{\emKer[\Ndesign]{\lpost}(\Par)} - 1\right] \Exp{2\emMean[\Ndesign]{\lpost}(\Par) + \emKer[\Ndesign]{\lpost}(\Par)}.
\end{align}
\end{prop}

The assumptions in \Cref{prop:llik-em-Gaussian} are satisfied when either the 
log-likelihood or log-posterior are emulated by GPs. In the former case,
this follows from specifying a prior 
$\llikEm[0] \sim \GP(\emMean[0]{\llik}, \emKer[0]{\llik})$ and then defining 
the surrogate as the conditional process
$\llikEm[\Ndesign] \Def \llikPrior | [\llikPrior(\designIn) = \llik(\designIn)] \sim \GP(\emMean[\Ndesign]{\llik}, \emKer[\Ndesign]{\llik})$.
In these cases, the induced unnormalized log-posterior density approximation 
$\postEm[\Ndesign]$ is the exponential of a GP, which we refer to as a 
\textit{log-normal process}.

Note that \Cref{prop:llik-em-Gaussian} is agnostic to the specific form of the 
likelihood, unlike \Cref{prop:fwd-em-Gaussian}. However, particular likelihood
structures may be leveraged to enable improved emulation in certain settings. For example, 
\citet{FerEmulation} consider a Gaussian likelihood with block-diagonal covariance structure, yielding
the log-likelihood
\begin{align}
\llik(\Par) 
&= \log \Gaussian(\obs \given \fwd(\Par), \likPar) \nonumber \\
&= - \frac{1}{2} \sum_{j=1}^{J} \left[\dimObs_j \log\left(2\pi\sigma_j^2\right) +
 \frac{1}{\sigma_j^2} \norm{\obs_j - \fwd_j(\Par)}_2^2 \right].
\end{align}
In this case, the maps $\Par \mapsto \norm{\obs_j - \fwd_j(\Par)}_2^2$ can be emulated 
independently of the likelihood parameter $\likPar$, enabling straightforward inference of the 
likelihood parameters along with $\Par$.
Approaches such as this can be considered a variation of log-density emulation, or an instance of 
forward model emulation with a particular definition of $\fwd$.
 
\section{Posterior Approximation} \label{sec:post-approx}
With a fit emulator in hand, we now turn to the question of how best to leverage the surrogate in approximating the 
posterior distribution of the Bayesian inverse problem. We view this as a question of uncertainty propagation. 
In realistic settings with restrictive computational budgets, the surrogate predictive distribution may retain a high degree of 
epistemic uncertainty stemming from the sparsity of the design points; our interest is in constructing posterior 
approximations that incorporate this uncertainty. We note that such approximations 
may serve various downstream goals. If computational constraints prevent the acquisition of additional 
design points, then one must accept a fixed posterior approximation.
In such cases, the surrogate uncertainty should be acknowledged to prevent 
overconfidence when drawing scientific conclusions. In other settings, it may be feasible to refine the surrogate 
via additional queries to the simulator. An approximate posterior constructed using the current 
surrogate may serve as a guide to determine the new input parameters at which the simulator will be 
evaluated. In this case, propagating surrogate uncertainty is key in weighing the exploration vs. exploitation 
tradeoff to select new design points (see \cref{sec:seq-design}).
This uncertainty propagation viewpoint aligns closely with the perspective taken in 
\citep{BurknerSurrogate,BurknerTwoStep}, 
who consider several MCMC-based methods for propagating surrogate uncertainty in a Bayesian framework. 
Alternative lines of research (reviewed in \cref{sec:seq-design}) have instead emphasized iterative schemes 
designed to achieve asymptotic convergence to the exact posterior. The tradeoff is that such approaches 
still typically require a large number of simulator evaluations--often performed serially-- and thus remain 
impractical in settings with computationally-intensive simulators.
Motivated by this fact, our focus in this section is on approximate posterior inference and uncertainty propagation
with a fixed surrogate.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth} % lpost dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lpost_dist_fwdem_trim.png}}
         \caption{$\lpost(\cdot; \fwdEm[\Ndesign])$}
         \label{fig:lpost_dist_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % post norm approx [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/post_norm_approx_fwdem.png}}
         \caption{$\postDensNorm$ approx}
         \label{fig:post_norm_approx_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % lpost dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lpost_dist_llikem_trim.png}}
         \caption{$\lpost(\cdot; \llikEm[\Ndesign])$}
         \label{fig:lpost_dist_llikem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % post norm approx [llik] 
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/post_norm_approx_llikem.png}}
         \caption{$\postDensNorm$ approx}
         \label{fig:post_norm_approx_llikem}
     \end{subfigure}
        \caption{A continuation of the example in \cref{fig:em_dist_1d}. The top and bottom rows correspond
        to the forward model and log-likelihood emulation settings, respectively. The first column summarizes
        the distributions of the log-posterior approximations induced by the underlying emulators, with 
        the predictive mean in blue and the shaded region containing 90\% of the mass. The second column 
        plots compares various (normalized) posterior approximations derived from the log-posterior 
        emulator. Note that even though  the emulators interpolate the design points, the posterior
        approximations do not owing to the normalization.}
        \label{fig:post_norm_approx_1d}
\end{figure}

\subsection{Plug-In Mean}
Before discussing methods to propagate surrogate uncertainty, we establish the baseline method of simply 
ignoring this uncertainty. A deterministic emulator can be defined by utilizing only the predictive mean function
of the random surrogate model. Plugging the predictive mean in place of the quantity it is emulating induces 
a deterministic approximation $\postApproxNormMean[\Ndesign]$ of the posterior distribution 
$\postDensNorm$, which we refer to as the \textit{plug-in mean}, or simply \textit{mean}, approximation. 
In the forward model and log-likelihood emulation settings, this implies the unnormalized density approximations 
\begin{align}
&\postApproxMean[\Ndesign](\Par; \fwdEm[\Ndesign]) \Def \postDens(\Par; \emMean[\Ndesign]{\fwd}), \label{eq:mean-approx-fwd}
&&\postApproxMean[\Ndesign](\Par; \llikEm[\Ndesign]) \Def \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par)}, \label{eq:mean-approx-llik}
\end{align}
with the normalizing constants $\normCstEm(\emMean[\Ndesign]{\fwd})$ and 
$\normCstEm(\emMean[\Ndesign]{\llik})$ defined by integrating the unnormalized densities over $\parSpace$
(existence of these integrals is discussed in \cref{sec:existence}).
The mean approximation has been applied in various contexts
\citep{VehtariParallelGP,trainDynamics,emPostDens,BurknerSurrogate,CLMBayesianCalibration,Lueckmann2018LikelihoodfreeIW} 
and analyzed theoretically 
\citep{StuartTeck1,StuartTeck2,random_fwd_models,TeckHyperpar,gp_surrogates_random_exploration}.
If the emulator predictive mean is known to be highly accurate then this approximation may be reasonable,
but in general ignoring the surrogate uncertainty can lead to posterior approximations that are 
both inaccurate and overly confident \citep{BurknerSurrogate}. 

\subsection{Expected Posterior}
We now transition to focus on posterior approximations that incorporate the surrogate uncertainty.
Consider a random surrogate $\postEm[\Ndesign]$ for the unnormalized posterior density, 
induced by some underlying emulator model (e.g., a forward model or log-density emulator).
We will use the notation $\E_{\Ndesign}$ to indicate expectations with respect to the 
distribution of the underlying surrogate model; e.g., the law of $\fwdEm[\Ndesign]$ or $\lpostEm[\Ndesign]$.
By normalizing $\postEm[\Ndesign]$, we obtain a random density
\footnote{More generally, this is a random measure. We work with densities to avoid measure-theoretic technicalities.}
\begin{align}
&\postNormEm[\Ndesign](\Par) 
\Def \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \postEm[\Ndesign](\Par),
&&\normCstEm[\Ndesign]\Def \int_{\parSpace} \postEm[\Ndesign](\Par) d\Par
\label{eq:sample-approx}
\end{align}
which summarizes the uncertainty about the true posterior. 
While $\postEm[\Ndesign](\Par)$ depends 
only on the univariate surrogate prediction at $\Par$, $\llikEmRdm[\Ndesign]{\postDensNorm}(\Par)$
depends on the entire random function $\llikEmRdmDens[\Ndesign]$ due to its dependence on 
the (random) normalizing constant $\llikEmRdm[\Ndesign]{\normCst}$. The quantity
$\llikEmRdm[\Ndesign]{\postDensNorm}$ is referred to as the \textit{sample} approximation in
\citet{StuartTeck1, StuartTeck2,random_fwd_models,TeckHyperpar}.
To construct a deterministic approximation to $\postDens$, we can consider the expectation
\begin{equation}
\postApproxEP[\Ndesign](\Par) \Def \E_{\Ndesign}\left[\postNormEm[\Ndesign](\Par) \right], \label{eq:ep-approx}
\end{equation}
which we refer to as the \textit{expected posterior (EP)}, following the terminology in \citet{BurknerSurrogate}.
Under a nonparametric surrogate model (e.g., a GP), the expectation in \cref{eq:ep-approx} is with 
respect to an infinite-dimensional random element, and thus its existence is a non-trivial matter \citep{StuartTeck1}.
Assuming the expectation exists, the Monte Carlo scheme outlined in \cref{alg:ep} could in principle 
be used to draw samples from $\llikEmSampDensNorm$. 

\begin{algorithm}
    \caption{Direct sampling from $\llikEmSampDensNorm$}
    \label{alg:ep}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{sampleEP}{$\llikEmRdm[\Ndesign]{\postDensNorm}, \NSample, M$}     
        \For{$\sampleIndex \gets 1, \dots, \NSample$} 
        		\State $\llikEmRdm[\Ndesign]{\postDensNorm}^{(\sampleIndex)} \sim \law(\llikEmRdm[\Ndesign]{\postDensNorm})$ \Comment{Sample posterior trajectory}
		\State $\Par^{(\sampleIndex, 1)}, \dots, \Par^{(\sampleIndex, M)} \overset{iid}{\sim} \llikEmRdm[\Ndesign]{\postDensNorm}^{(\sampleIndex)}$ \Comment{Sample parameters}
	\EndFor
	\State \Return $\{\Par^{(\sampleIndex, m)}\}_{1 \leq \sampleIndex \leq \NSample, \ 1 \leq m \leq M}$
	\EndFunction
    \end{algorithmic}
\end{algorithm}

A practical implementation 
would entail first sampling a trajectory of the surrogate model, which induces a sample 
trajectory of the unnormalized random density $\llikEmRdmDens[\Ndesign]$. Parameter samples
could then be drawn from the distribution implied by this unnormalized density using standard 
methods (e.g., MCMC).
The output of the algorithm thus represents a mixture over an 
ensemble of posterior distributions. Choosing $M = 1$  (drawing one sample per
trajectory of $\postNormEm[\Ndesign]$) implies that \cref{alg:ep} returns independent 
samples from $\postApproxEP[\Ndesign]$. For $M > 1$, the samples will be dependent.
For example, in the extreme case $\NSample = 1$ they will 
still have the correct marginal distribution $\postApproxEP[\Ndesign]$, but will 
only provide a summary of a single trajectory from $\postNormEm[\Ndesign]$ and thus 
in general provide a poor representation of this random density.

\Cref{alg:ep} is readily implementable in practice provided the ability to sample trajectories 
of $\postEm[\Ndesign]$, which will generally be possible for finite-dimensional 
parametric surrogate models. \citet{garegnani2021NoisyMCMC,BurknerSurrogate}, and \citet{BurknerTwoStep}
investigate \cref{alg:ep} in such settings. Under certain assumptions, \citet{garegnani2021NoisyMCMC} 
bounds the Monte Carlo error in the sample-based approximation of $\postApproxEP[\Ndesign]$.
\citet{BurknerTwoStep} proposes an importance sampling approximation to reduce the 
required number of sampled trajectories.

With a nonparametric emulator, the practical difficulty in 
implementing \cref{alg:ep} is the requirement to sample an infinite-dimensional 
trajectory from the surrogate model.
These difficulties are noted in \citet{VehtariParallelGP} as justification for pursuing alternative 
approaches. As far as we are aware, \citet{trainDynamics} is the only work to attempt an 
implementation of \cref{alg:ep} in the nonparametric context. Their method consists of approximating GP 
trajectories by sampling the surrogate only at a finite grid of points, and then approximating 
the trajectory as the GP mean, conditional on the sampled values at these points.
We note that an alternative approach could involve constructing a finite-dimensional approximation 
of the surrogate in order to provide the practical means to simulate (approximate) trajectories 
of $\postEm[\Ndesign]$. In the GP setting, approaches such as the Karhunen-Loeve expansion 
and random Fourier features could be leveraged for this purpose \citep{dimRedPolyChaos,samplingGPPosts}. 
As with the \citet{trainDynamics} method, this approach is only an approximate implementation of \cref{alg:ep}.

\subsection{Expected Likelihood}
Given the challenge of implementing \Cref{alg:ep} with a nonparametric emulator, the majority of 
previous literature instead proposes deterministic approximations of the unnormalized
density. As opposed to the expected posterior method, these approximations are computed 
pointwise in $\Par$. This is a potential downside of this approach, as it does not leverage 
the full structure of the surrogate predictive distribution (e.g., the predictive covariance of a GP emulator).
However, the focus on unnormalized density approximation has several practical benefits, 
including that it does not require approximating surrogate trajectories and is amenable to the application of 
standard Bayesian inference procedures using a single MCMC run.

We start by considering the approximation derived by computing the pointwise expectation 
of $\postEm(\Par)$ and then normalizing after-the-fact.
This yields the unnormalized density estimate
\begin{align}
\postApproxMarg[\Ndesign](\Par) \Def \E_{\Ndesign}\left[\postEm[\Ndesign](\Par) \right], \label{eq:post-approx-EL}
\end{align}
which we refer to as the \textit{expected likelihood (EL)}
\footnote{Note that we are technically considering expectations of the unnormalized posterior density approximation 
$\postEm[\Ndesign](\Par)$, but this is equivalent to taking expectations of the likelihood approximation and
then multiplying by the prior density.}
approximation, again borrowing terminology from
 \citet{BurknerSurrogate}. \citet{StuartTeck1} instead refer to this as the \textit{marginal} approximation, 
 and show the normalizing constant implied by \cref{eq:post-approx-EL} is given by
 \begin{equation}
 \int_{\parSpace} \E_{\Ndesign}\left[\postEm[\Ndesign](\Par) \right] d\Par 
 = \E_{\Ndesign} \int_{\parSpace} \postEm[\Ndesign](\Par) d\Par
 = \E_{\Ndesign}\left[\normCstEm \right],
 \end{equation} 
meaning the expected likelihood approximation is a ratio estimator of the form 
 $\postApproxNormMarg[\Ndesign](\Par) = \E_{\Ndesign}[\postEm[\Ndesign](\Par)] / \E_{\Ndesign}[\normCstEm]$.
 Note the difference with respect to 
 $\postApproxEP[\Ndesign](\Par) = \E_{\Ndesign}[\postEm[\Ndesign](\Par) / \normCstEm]$, 
 which we highlight in \cref{tab:post-approx-comparison} in the forward model emulation setting.  
 The expected likelihood approximation $\postApproxMarg[\Ndesign]$ may be justified from a Bayesian
 decision-theoretic viewpoint as the minimizer of an $L^2$ risk. However, this does not 
 confer the same optimality on the normalized approximation $\postApproxNormMarg[\Ndesign]$;
 see \citep{SinsbeckNowak,StuartTeck2,VehtariParallelGP} for details. To our knowledge, the papers
 \citep{SinsbeckNowak,StuartTeck1} are two of the first to consider the expected likelihood approximation
 to propagate surrogate uncertainty.
 
 If the pointwise expectation 
 $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$ is computable in closed-form, then 
the expected likelihood approximation can be sampled using standard MCMC software.
In cases where this expectation is intractable, pseudo-marginal MCMC 
\citep{pseudoMarginalMCMC} may be employed, so long as 
samples can be drawn from the surrogate predictive distribution $\postEm[\Ndesign](\Par)$
at any input $\Par$. The pseudo-marginal approach is discussed in \Cref{sec:MH-approx}.
\citet{BurknerSurrogate} alternatively propose to replace  $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$
with a fixed Monte Carlo estimate derived from surrogate samples (i.e., a sample average approximation). 
Their method differs from the 
pseudo-marginal algorithm in that the Monte Carlo samples are simulated once 
and then fixed throughout the MCMC run. This allows for the application of standard 
MCMC methods, but only targets an approximation to $\postApproxNormMarg[\Ndesign]$
and is only directly implementable in the parametric setting.
In the nonparametric case, most previous literature 
has focused on the Gaussian settings from \cref{prop:fwd-em-Gaussian} and \cref{prop:llik-em-Gaussian}
due to their analytic tractability. We summarize these special cases below.

\subsubsection{Gaussian Setting: Forward Model Emulator}
\begin{prop} \label{prop:post-approx-fwd-EL-Gaussian}
Consider the setting in \cref{prop:fwd-em-Gaussian}, where both the forward model emulator 
and likelihood are Gaussian. Under these assumptions, the expected likelihood approximation
is given by
\begin{align}
\postApproxMarg[\Ndesign](\Par) 
&= \priorDens(\Par) \Gaussian\left(\obs \given \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par) \right). \label{eq:post-approx-fwd-EL-Gaussian}
\end{align}
Moreover, \Cref{eq:post-approx-fwd-EL-Gaussian} is the unnormalized posterior corresponding to the modified
Bayesian inverse problem
\begin{align}
&\obs = \emMean[\Ndesign]{\fwd}(\Par) + \eta_{\Ndesign}(\Par) + \noise
&&\noise \sim \Gaussian(0, \likPar)  \label{inv_prob_Gaussian_modified} \\
&\eta_{\Ndesign}(\Par) \sim \Gaussian(0, \emKer[\Ndesign]{\fwd}(\Par))
&&\Par \sim \priorDens  \nonumber
\end{align}
where $\noise$, $\Par$, and $\eta_{\Ndesign}(\Par)$ are pairwise a priori independent for all $\Par$.
\end{prop}

The approximate likelihood retains a Gaussian form, where the surrogate predictive mean
$\emMean[\Ndesign]{\fwd}$ has replaced the true forward model $\fwdEm[\Ndesign]$. The surrogate uncertainty
is incorporated via the addition of $\emKer[\Ndesign]{\fwd}(\Par)$ to the noise covariance $\likPar$.
We emphasize that even if the prior $\priorDens$ is Gaussian, the posterior approximation will typically be non-Gaussian
due to the nonlinearity of $\emMean[\Ndesign]{\fwd}(\Par)$ and the fact that the likelihood covariance depends on 
$\Par$ through $\emKer[\Ndesign]{\fwd}(\Par)$. The modified inverse problem viewpoint 
in \cref{inv_prob_Gaussian_modified} is noted in \citet{SinsbeckNowak} and \citet{StuartTeck1}; the former shows that
this viewpoint extends beyond the Gaussian setting, even when analytical expressions for $\postApproxMarg[\Ndesign](\Par) $ 
may not exist.
The closed-form expression in \cref{eq:post-approx-fwd-EL-Gaussian} is also noted in both
\citet{SinsbeckNowak, StuartTeck1} and has been used in many subsequent studies 
\citep{VehtariParallelGP,weightedIVAR,StuartTeck2,GP_PDE_priors,CES,idealizedGCM,
villani2024posteriorsamplingadaptivegaussian,hydrologicalModel}. 

\subsubsection{Gaussian Setting: Log-Density Emulator}
A closed form expression for $\postApproxMarg[\Ndesign](\Par)$ is available when using a log-likelihood
or log-posterior emulator with a Gaussian predictive distribution. We present the result for a log-likelihood 
emulator; in the log-posterior case the prior density term is removed.

\begin{prop} \label{prop:post-approx-llik-EL-Gaussian}
Consider the setting in \cref{prop:llik-em-Gaussian} with a Gaussian log-likelihood emulator. The 
expected likelihood approximation is then given by the unnormalized density 
\begin{align}
\postApproxMarg[\Ndesign](\Par) 
&= \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)}
= \postApproxMean(\Par) \Exp{\frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)}. \label{eq:post-approx-llik-EL-Gaussian}
\end{align}
\end{prop}

The density in \cref{eq:post-approx-llik-EL-Gaussian} can be viewed as inflating the mean approximation 
at points where the emulator is uncertain. 
It is notable that the uncertainty inflation factor $\Exp{\frac{1}{2}\emKer[\Ndesign]{\lpost}(\Par)}$ scales 
very quickly as the surrogate variance increases.
In practice, this sensitivity to the predictive variance can lead $\postApproxNormMarg[\Ndesign]$ to be highly 
multimodal, or heavily concentrated in small regions with high predictive variance. 
This pathology is noted in \cite{VehtariParallelGP}, who recommend against the use of the expected
likelihood approximation in their application of interest (log-likelihood emulation with noisy log-likelihood evaluations). 
This posterior approximation is analyzed theoretically in \citet{StuartTeck1,StuartTeck2,GP_PDE_priors,random_fwd_models,TeckHyperpar}.

\begin{table}[h]
\centering
\begin{tabular}{>{\centering\arraybackslash}p{4cm} >{\centering\arraybackslash}p{5cm} >{\centering\arraybackslash}p{5cm}}
\toprule
\textbf{Plug-In Mean} & \textbf{Expected Likelihood} & \textbf{Expected Posterior} \\
\midrule
$\displaystyle \frac{\postDens(\Par; \E_{\Ndesign}[\fwdEm[\Ndesign]])}{\normCst(\E_{\Ndesign}[\fwdEm[\Ndesign]])}$ & 
$\displaystyle \frac{\E_{\Ndesign}\left[\postDens(\Par; \fwdEm[\Ndesign])\right]}{\E_{\Ndesign}\left[\normCstEm[\Ndesign]\right]}$ & 
$\displaystyle \E_{\Ndesign}\left[\frac{\postDens(\Par; \fwdEm[\Ndesign])}{\normCstEm[\Ndesign]}\right]$ \\
\bottomrule
\end{tabular}
\caption{Comparison of the normalized density approximations implied by the plug-in mean, expected likelihood,
and expected posterior approximations in the forward model emulation setting.}
\label{tab:post-approx-comparison}
\end{table}

\subsection{Other Unnormalized Density Approximations}
Various alternative approximations have been proposed for the unnormalized posterior density. 
Instead of computing pointwise expectations, one can consider summarizing $\postEm[\Ndesign](\Par)$
by computing pointwise quantiles \citep{VehtariParallelGP,quantileApprox}. In the log-likelihood 
emulation setting with a GP emulator, \cite{VehtariParallelGP} considers the $\quantileProb$-quantile of 
the likelihood surrogate
\begin{equation}
\mathrm{Quantile}_{\quantileProb}(\Exp{\llikEm[\Ndesign](\Par)})
= \Exp{\emMean[\Ndesign]{\llik}(\Par) + \GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}},
\label{eq:post-approx-quantile}
\end{equation}
where $\GaussianCDF$ denotes the standard Gaussian distribution function. This expression is of the same form 
as the expected likelihood in \cref{eq:post-approx-llik-EL-Gaussian}, but the uncertainty inflation term 
$\Exp{\GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}}$ scales more slowly than 
$\Exp{\frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)}$. The special case $\quantileProb = 1/2$ (i.e., the median) 
reduces to the mean approximation in \cref{eq:mean-approx-llik}, while values $\quantileProb > 1/2$ imply 
the density will be inflated in regions of higher surrogate uncertainty. \citet{FATES_CES} also utilize the 
approximation in \cref{eq:post-approx-quantile}, though they do not explicitly draw the connection to 
the quantile estimator.

In the forward model emulation setting, \citet{BurknerSurrogate} also consider an \textit{expected log-likelihood}
approximation of the form $\priorDens(\Par) \Exp{\E_{\Ndesign}[\llik(\Par; \fwdEm[\Ndesign])]}$ and draw 
a connection with power-scaled likelihoods. However, the authors ultimately recommend the expected 
posterior and expected likelihood as preferred alternatives to this method.

\subsection{Noisy MCMC Approximations} \label{sec:MH-approx}
In this section, we describe an alternative approach to surrogate-based inference
that focuses on constructing approximations to MCMC algorithms as opposed to approximations
of the unnormalized posterior density. Some of these methods provide algorithms for sampling 
from approximate posteriors already defined above, while others represent new approaches to inference.
The benefits of these approaches include ease of implementation and generality--they only require 
the ability to draw samples from the predictive distribution at finite sets of inputs.
Therefore, these algorithms do not rely on distributional 
assumptions on the surrogate or likelihood, and can be used in the nonparametric surrogate setting.

To start, we recall the standard Metropolis-Hastings (MH) algorithm, which is defined by a proposal 
kernel with density $\propDens(\Par, \cdot)$. If the Markov chain is in the current state 
$\Par \in \parSpace$ then the next state is defined by sampling the proposal
 $\propPar \sim \propDens(\Par, \cdot)$, which is accepted with probability
\begin{align}
&\accProbMH(\Par, \propPar) \Def 
\min\left\{1, \frac{\postDens(\propPar)\propDens(\propPar, \Par)}{\postDens(\Par) \propDens(\Par, \propPar)} \right\}.
\label{MH_acc_prob_exact}
\end{align}
If accepted, the next state is defined to be $\propPar$, else it is set to the current state $\Par$. 
This procedure is summarized in \cref{alg:MH}. 

\begin{figure}[ht]
\centering
\begin{minipage}[t]{0.495\textwidth}
    \floatname{algorithm}{Alg.}
    \captionsetup{type=algorithm}
    \caption{Metropolis-Hastings}
    \label{alg:MH}
    \begin{algorithmic}[1]
    \Function{MH}{$\Par_0, \NMCMC$}     
        \For{$k \gets 0, \dots, \NMCMC$} 
            \State $\tilde{\Par} \sim q(\Par_{k}, \cdot)$
            \State $\alpha \gets \min\left\{1, \frac{\pi(\tilde{\Par}) q(\tilde{\Par}, \Par_k)}{\pi(\Par_k) q(\Par_k, \tilde{\Par})} \right\}$
            \State $b \sim \text{Bernoulli}(\alpha)$
            \If{$b = 1$}
                \State $\Par_{k+1} \gets \tilde{\Par}$ 
            \Else
                \State $\Par_{k+1} \gets \Par_k$
            \EndIf
        \EndFor
    \EndFunction
    \end{algorithmic}
\end{minipage}
\hfill
\begin{minipage}[t]{0.495\textwidth}
    \floatname{algorithm}{Alg.}
    \captionsetup{type=algorithm}
    \caption{Noisy Metropolis-Hastings}
    \label{alg:MH-noisy}
    \begin{algorithmic}[1]
    \Function{MH-Noisy}{$\Par_0, \NMCMC, \postSampleKernel$}
        \State $\hat{\pi}_{\text{curr}} \sim \mathrm{law}(\postEm[\Ndesign](\indexMCMC[0]{\Par}))$
        \For{$k \gets 0, \dots, \NMCMC$} 
            \State $\tilde{\Par} \sim q(\Par_k, \cdot)$
            \State $[\hat{\pi}_{\Par}, \hat{\pi}_{\tilde{\Par}}] \sim \postSampleKernel([\Par_k, \tilde{\Par}, \hat{\pi}_{\text{curr}}], \cdot)$
            \State $\hat{\alpha} \gets \min\left\{1, \frac{\hat{\pi}_{\tilde{\Par}} \cdot q(\tilde{\Par}, \Par_k)}{\hat{\pi}_{\Par} \cdot q(\Par_k, \tilde{\Par})} \right\}$
            \State $b \sim \text{Bernoulli}(\hat{\alpha})$
            \If{$b = 1$}
                \State $\Par_{k+1} \gets \tilde{\Par}$
                \State $\hat{\pi}_{\text{curr}} \gets \hat{\pi}_{\tilde{\Par}}$
            \Else
                \State $\Par_{k+1} \gets \Par_k$
            \EndIf
        \EndFor
    \EndFunction
    \end{algorithmic}
\end{minipage}
\caption{(Left) A standard Metropolis-Hastings MCMC algorithm with proposal density $\propDens(\Par, \cdot)$.
(Right) A generic noisy Metropolis-Hastings algorithm. The choice of Markov kernel $\postSampleKernel$ 
defines particular algorithms, as described in \Cref{sec:MH-approx}.}
\end{figure}

We consider a family of algorithms that replace $\accProbMH(\Par, \propPar)$ by
substituting the exact densities $\postDens(\Par)$  and $\postDens(\propPar)$ 
with approximations sampled from a specified distribution. The particular choice of 
this distribution yields different algorithms, all of which are ``noisy'' in the sense that 
an additional Monte Carlo step has been injected within the standard MH scheme
\citep{noisyMCSurvey,noisyMCMC,stabilityNoisyMH}. In particular, we assume 
that the approximation of $[\postDens(\Par), \postDens(\propPar)]$ is sampled as
\begin{equation}
[\hat{\postDens}_{\Par}, \hat{\postDens}_{\propPar}] \sim \postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot),
\end{equation}
where $\postSampleKernel$ is a Markov kernel mapping from source $\parSpace^2 \times \R_+$
to target $\R^2_{+}$. The generic noisy MH algorithm is stated in \Cref{alg:MH-noisy}. 
We consider three special cases below.

\paragraph{Pseudo-Marginal.}
We first consider the choice 
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \delta_{\postDens^\prime} \otimes \mathrm{law}(\postEm[\Ndesign](\propPar)),
\end{equation}
which implies that at each iteration only the density value at the proposed point 
$\postEm[\Ndesign](\propPar)$  is sampled, while the value at the current point 
is recycled from the previous iteration. This is a pseudo-marginal algorithm 
targeting the stationary distribution $\postApproxNormMarg[\Ndesign]$ \citep{pseudoMarginalMCMC}.
In principle, it provides an exact MCMC scheme to sample from the expected likelihood 
approximation even when the expectation $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$ is intractable, 
a fact noted in \citet{StuartTeck1}. The key property ensuring invariance with respect to 
$\postApproxNormMarg[\Ndesign]$ is that the value $\hat{\postDens}_{\tilde{\Par}}$ sampled from 
$\postSampleKernel$ is an unbiased estimate of $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$.
Therefore, the method remains valid for estimators of the form 
\begin{align}
&\hat{\postDens}_{\tilde{\Par}}^{J} \Def \frac{1}{J} \sum_{j=1}^{J} \hat{\postDens}_{\tilde{\Par}}^{(j)},
&&\hat{\postDens}_{\tilde{\Par}}^{(j)} \overset{\mathrm{iid}}{\sim} \mathrm{law}(\postEm[\Ndesign](\propPar)). \label{eq:pm-unbiased-est}
\end{align}
It is well-known that pseudo-marginal methods
can suffer from slow mixing, but that efficiency can be improved by reducing the variance 
in the estimate of $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$; e.g., by increasing $J$ in
\cref{eq:pm-unbiased-est} \citep{pseudoMarginalMCMC,pseudoMarginalEfficiency}.
The pseudo-marginal approach to sampling $\postApproxNormMarg[\Ndesign]$ is studied 
in \citet{garegnani2021NoisyMCMC}.

\paragraph{Monte Carlo within Metropolis Hastings.}
We next consider the Markov kernel 
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \mathrm{law}(\postEm[\Ndesign](\Par)) \otimes \mathrm{law}(\postEm[\Ndesign](\propPar)),
\end{equation}
which independently re-samples the density values at both the current and proposed locations
at each iteration (the kernel does not depend on $\postDens^\prime$). Algorithms of this form are 
typically referred to as Monte Carlo within Metropolis Hastings (MCwMH), and have been studied as an
efficient alternative to the pseudo-marginal algorithm \citep{noisyMCMC,stabilityNoisyMH}.
The sampled density values can also be replaced with sample means as in \Cref{eq:pm-unbiased-est},
though the efficiency of MCwMH is generally less sensitive to the variance in the estimate
\citep{garegnani2021NoisyMCMC,stabilityNoisyMH}.
The MCwMH is typically referred to as inexact, in the sense that it does not admit 
$\postApproxNormMarg[\Ndesign]$ as an invariant distribution. However, in the present setting
$\postApproxNormMarg[\Ndesign]$ is itself an approximation of $\postDensNorm$ so we might
view MCwMH as an alternative method for propagating surrogate uncertainty
on equal footing with $\postApproxNormMarg[\Ndesign]$. This perspective is taken in 
\citet{surrogateNoisyMCMC}, while \citet{garegnani2021NoisyMCMC} studies MCwMH
as an approximation to $\postApproxNormMarg[\Ndesign]$. A variant of MCwMH
is employed in \citet{FerEmulation}.

\paragraph{Expected Acceptance Probability.}
Finally, we consider
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \mathrm{law}(\postEm[\Ndesign](\Par), \postEm[\Ndesign](\propPar)),
\end{equation}
implying that both density values are re-sampled each iteration, but now from
the joint distribution implied by the surrogate $\postEm[\Ndesign]$. 
We refer to this as the \textit{expected acceptance probability (E-Acc)} approximation,
as it can be viewed as marginalizing the MH acceptance probability with respect
to the surrogate. Indeed, by inserting $\postEm[\Ndesign]$
in place of $\postDens$ in $\accProbMH(\Par, \propPar)$, the surrogate induces a random 
approximation
\begin{equation}
\accProbMHEm[\Ndesign](\Par, \propPar) 
\Def \min\left\{1, \frac{\postEm[\Ndesign](\propPar)\propDens(\propPar, \Par)}{\postEm[\Ndesign](\Par) \propDens(\Par, \propPar)} \right\}
\label{eq:MH-prob-surrogate}
\end{equation}
of the MH acceptance probability. The E-Acc algorithm can thus be viewed as a modification of the
MH scheme in \Cref{alg:MH} with $\E_{\Ndesign}[\accProbMHEm[\Ndesign](\Par, \propPar)]$
replacing $\accProbMH(\Par, \propPar)$. Many of the other posteriors introduced above can also
be viewed as invoking particular approximations of $\accProbMH(\Par, \propPar)$, as summarized
in \Cref{tab:acc-prob-comparison}. \citet{surrogateNoisyMCMC} propose the E-Acc algorithm, 
and explore connections to the expected posterior approximation.

\begin{table}[h]
\centering
\small % (optional) shrink slightly if needed
\renewcommand{\arraystretch}{1.6} % row height
\setlength{\tabcolsep}{10pt} % column separation
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}} % centered column of width #1

\begin{tabular}{lC{10cm}}
\toprule
\textbf{Posterior Approximation} & \textbf{MH Approximation} \\
\midrule
Plug-In Mean & 
\(\displaystyle \min\left\{1, \frac{\Exp{\E[\lpostEm[\Ndesign](\propPar)]}}{\Exp{\E[\lpostEm[\Ndesign](\Par)]}} \right\}\) \\ 
Expected Likelihood & 
\(\displaystyle \min\left\{1, \frac{\E \left[\Exp{\lpostEm[\Ndesign](\propPar)}\right]}{\E\left[\Exp{\lpostEm[\Ndesign](\Par)}\right]} \right\}\) \\
Expected Likelihood Ratio & 
\(\displaystyle \min\left\{1, \E\left[\frac{\Exp{\lpostEm[\Ndesign](\propPar)}}{\Exp{\lpostEm[\Ndesign](\Par)}}\right] \right\}\) \\
Expected Acc. Prob. & 
\(\displaystyle \E\left[\min\left\{1, \frac{\Exp{\lpostEm[\Ndesign](\propPar)}}{\Exp{\lpostEm[\Ndesign](\Par)}} \right\}\right]\) \\
\bottomrule
\end{tabular}
\caption{The approximations to the Metropolis-Hastings (MH) acceptance probability implied by different posterior approximations using a log-posterior surrogate $\lpostEm[\Ndesign]$. All expectations are with respect to 
the underlying surrogate predictive distribution. For brevity, the acceptance probabilities 
are presented for the case of a symmetric proposal distribution.}
\label{tab:acc-prob-comparison}
\end{table}

\subsection{Existence and Integrability} \label{sec:existence}
We have henceforth implicitly assumed that the surrogate-induced posterior approximations 
introduced above are well-defined. Consideration of this question points to important 
practical issues in constructing emulators for use in posterior approximation (see
\Cref{sec:recs}). The primary concern
is that the tail behavior of the surrogate may lead to approximations of $\postDens$ that 
are not integrable. Such pathologies can easily arise if utilizing a stationary surrogate, where 
the predictive mean and variance stabilize at constant values as distance from the design points 
increases. This issue is especially a concern if emulating the log-posterior density, as the prior 
density is being modeled and hence cannot be relied upon to ensure integrability \citep{emPostDens}.
Table \todo illustrates simple examples where such pathologies can occur.
In order to avoid issues related to
tail behavior, previous work tends to restrict to the setting where $\parSpace$
is a compact subset of $\R^{\dimPar}$ \citep{StuartTeck1, VehtariParallelGP}.
Other applications have not directly addressed this issue, but in numerical experiments focus on 
prior distributions with compact support \citet{trainDynamics,FATES_CES} or truncate an unbounded
prior to achieve compact support \citep{gp_surrogates_random_exploration,FerEmulation}. 
\citet{emPostDens} address this issue in depth, describing how the use of stationary log-posterior
surrogates can lead to divergent MCMC chains when performing approximate posterior inference.
Theoretical treatments have also explored conditions to ensure existence over generic unbounded
domains \citep{random_fwd_models,garegnani2021NoisyMCMC}. We refer to 
\citep{StuartTeck1,StuartTeck2} for a comprehensive theoretical treatment in the GP setting.
Ideally, the surrogate ought to be constructed such that every realization of $\postEm[\Ndesign]$
is integrable (i.e., $\normCstEm$ exists almost surely). This assumption is implicit in \Cref{alg:ep}, 
which assumes that each sampled trajectory of $\postEm[\Ndesign]$ induces a valid probability 
distribution from which samples can be drawn. This requirement can be weakened for other 
approximations; for example, in \Cref{prop:llik-em-Gaussian} we observe the requirement is that
$\log\left[\priorDens(\Par)\right] + \emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)$
decays sufficiently quickly as $\norm{\Par} \to \infty$. 

\section{Sequential Design} \label{sec:seq-design}
Up until this point, we have considered a fixed surrogate model trained using a set of
$\Ndesign$ simulator evaluations. We now discuss algorithms for iteratively 
updating the emulator using additional rounds of simulator runs.
Given a limited computational budget, these new evaluation points should be carefully chosen in 
a goal-oriented fashion. The cycle of selecting design points, querying the simulator, 
and updating the surrogate represents a particular \textit{sequential design},
or \textit{active learning}, problem. We focus on active learning strategies
tailored to the goal of approximating the posterior distribution $\postDens$. The key
idea underlying such strategies is that achieving an accurate posterior 
approximation does not require an accurate emulator over the entirety of $\parSpace$;
rather, the emulator should be accurate in the subset of $\parSpace$ where the 
posterior is large, while also containing enough global information to know where
the posterior is small. Various versions of this local-global tradeoff have been articulated
across several studies \citep{StuartTeck2, gp_surrogates_random_exploration,SinsbeckNowak,Surer2023sequential,adaptiveMultimodal}. 
This research has demonstrated that traditional active learning algorithms targeting global improvements 
in the underlying emulators (e.g., $\fwdEm[\Ndesign]$ or $\lpostEm[\Ndesign]$) are often sub-optimal with 
respect to the goal of improving the induced posterior approximation.

A second theme intertwined with the local-global tradeoff is the idea that effective 
sequential design should target promising regions (according to the current surrogate)
but also explore regions where the surrogate is uncertain. 
This ``exploration vs. exploitation'' balancing act is familiar in fields such as 
Bayesian optimization \citep{reviewBayesOpt}, Bayesian quadrature 
\citep{BayesQuadrature,BayesQuadratureAL,BayesQuadRatios,quadratureLogGP}, 
reinforcement learning \citep{BadiaRL,LiuRL}, and bandit algorithms \citep{banditsEmpirical,LattimoreBandits}, 
among others. For GP surrogates, a variety of design criteria (i.e., acquisition functions) have been 
proposed that seek to strike this balance in the context of solving Bayesian inverse problems \citep{SinsbeckNowak,Surer2023sequential,KandasamyActiveLearning2015,weightedIVAR,
VehtariParallelGP,VillaniAdaptiveGP}. We review these criteria below, placing particular 
emphasis on the batch sequential setting, where multiple design points are acquired simultaneously.
We conclude this section by reviewing methods that fall outside of the standard active learning 
framework.

To remain agnostic to the quantity being emulated, we state algorithms with respect to
a generic underlying emulator $\funcEm[\Ndesign]$, which approximates some function
$\func$ (e.g., the forward model or log-likelihood).

\subsection{Sequential Design Loop}
Given a current surrogate fit to design inputs $\designIn[\Ndesign] \in \parSpace^{\Ndesign}$,
our goal is to select a new batch of inputs $\designBatchIn \in \parSpace^{\Nbatch}$
to form the augmented design $\designIn[\Naugment] \Def \designIn[\Ndesign] \cup \designBatchIn$.
After running the simulator at the new points $\designBatchIn$ and updating the emulator, we obtain
an updated posterior surrogate $\postEm[\Naugment]$. We therefore seek to select $\designBatchIn$
to yield the best possible improvement in the approximate posterior, which is naturally cast as 
an optimization problem
\begin{equation}
\designBatchIn \in \argmin_{\parMat \in \parSpace^{\Nbatch}} \acq[](\parMat)
\label{eq:acq-opt}
\end{equation}
with respect to some objective function $\acq[]: \parSpace^{\Nbatch} \to \R$, typically
called the \textit{acquisition function} or \textit{design criterion}.
Depending on computational resources, this optimization can be repeated over a sequence of 
rounds $\Nrounds$, yielding the sequential design loop summarized in \Cref{alg:seq-des-loop}.
\begin{algorithm}
    \caption{Sequential Design Loop}
    \label{alg:seq-des-loop}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{SeqDesign}{$\func, \funcPrior, \designIn[\Ndesign], \Nrounds$}     
    	\State $\hat{\func} \gets \mathrm{fit}(\funcPrior \given \designIn[\Ndesign], \func(\designIn[\Ndesign]))$
	\Comment{Fit to initial design}	
        \For{$\designIndex \gets 1, \dots, \Nrounds$} \Comment{Sequential design loop}
        		\State $\designBatchIn^{(\designIndex)} \gets \argmin_{\parMat \in \parSpace^{\Nbatch}} \acq[]^{(\designIndex)}(\parMat)$ 
		\State $\hat{f} \gets \mathrm{update}(\hat{f} \given \designBatchIn^{(\designIndex)}, \func(\designBatchIn^{(\designIndex)}))$
	\EndFor
	\EndFunction
    \end{algorithmic}
\end{algorithm}
We state the algorithm for a generic probabilistic model $\funcPrior$. 
This model is first fit to an initial design $\{\designIn[\Ndesign], \func(\designIn[\Ndesign])\}$ and then 
updated at each iteration $\designIndex$ by augmenting the existing design with newly 
acquired points $\{\designBatchIn^{(\designIndex)}, \func(\designBatchIn^{(\designIndex)})\}$.
\footnote{One could of course consider varying the batch size $\Nbatch$ across the $\Ndesign$ iterations but to simplify notation
we keep $\Nbatch$ constant.}
For GPs, these updates involve conditioning and potentially also re-fitting hyperparameters. 
The special case $\Nbatch = 1$ corresponds to the 
pure sequential design setting in which the function $\func$ is evaluated at each acquired point before
considering the subsequent acquisition. The more challenging batch sequential design setting
$\Nbatch > 1$ requires selection of inputs without observing the responses for the other points
in the batch. In practice, batch selection may be a necessity when simulations are costly but can 
be run in parallel. Though it accommodates batch acquisitions, \Cref{alg:seq-des-loop} is
still ``myopic'' in the sense that it disregards the potential for acquisitions in future rounds. 
Non-myopic strategies have been considered
through a dynamic programming lens, though they typically come at the cost of significant 
computational expense \citep{SURThesis, supermartingaleSUR}.

\subsection{Goal-Oriented Acquisition Functions}
We now make \Cref{alg:seq-des-loop} concrete by introducing several acquisition functions
that explicitly target the goal of posterior approximation. We draw a distinction between 
acquisition criteria restricted to the pure sequential ($\Nbatch = 1$) setting
and those naturally defined in the batch setting. It is still possible to perform 
batch acquisition with criteria of the former type, as discussed in \Cref{sec:batch-heuristic}.

\subsubsection{Single Point Criteria}
When optimizing for a single design point, a natural strategy is to simply select the input
where the uncertainty in $\postEm[\Ndesign](\Par)$ is largest. If predictive variance
is used as the measure of uncertainty, this yields the \textit{maximum variance} criterion
\begin{equation}
\acq[](\Par) \Def -\Var_{\Ndesign}[\postEm[\Ndesign](\Par)], \label{eq:acq-maxvar}
\end{equation}
which is negated to align with our convention of minimizing acquisition functions.
This is a natural modification of standard active learning criteria that target inputs 
where $\Var_{\Ndesign}[\funcEm[\Ndesign](\Par)]$ is maximal \citep[Section 6.2.1]{gramacy2020surrogates}.
The predictive variance of $\postEm[\Ndesign](\Par)$
is given in closed-form in \Cref{prop:fwd-em-Gaussian,prop:llik-em-Gaussian} for 
forward model and log-density emulators in the Gaussian settings. For example, 
when $\lpostEm[\Ndesign](\Par) \sim \Gaussian(\emMean[\Ndesign]{\lpost}(\Par), \emKer[\Ndesign]{\lpost}(\Par))$
we have
\begin{equation}
\Var\left[\postEm[\Ndesign](\Par; \lpostEm[\Ndesign])\right] &= 
\left[\Exp{\emKer[\Ndesign]{\lpost}(\Par)} - 1\right] \Exp{2\emMean[\Ndesign]{\lpost}(\Par) + \emKer[\Ndesign]{\lpost}(\Par)},
\end{equation}
which demonstrates that the criterion favors inputs where both the expected log-posterior
$\emMean[\Ndesign]{\lpost}(\Par)$ and epistemic uncertainty $\emKer[\Ndesign]{\lpost}(\Par)$ are large, encoding an exploration-exploitation
tradeoff. The maximum variance criterion is utilized in \citet{Lueckmann2018LikelihoodfreeIW,Kandasamy_2017}. In the GP log-density emulator setting (\Cref{prop:llik-em-Gaussian})
$\postEm[\Ndesign](\Par)$ is log-normal. Given the heavy tails and asymmetry of a log-normal random variable, 
\citep{VehtariParallelGP,wang2018adaptive} argue that the variance provides a misleading measure
of uncertainty in this setting, instead favoring the entropy or interquartile range. 
As an alternative to these maximum uncertainty criteria, \citet{gp_surrogates_random_exploration}
suggest explicitly decoupling exploration and exploitation by selecting one point 
that maximizes $\emMean[\Ndesign]{\lpost}(\Par)$ and randomly sampling a second point from 
$\priorDens$. 

\subsubsection{Multipoint Criteria}
We next consider acquisition functions that can be evaluated at batches of multiple inputs. In particular,
we focus on a class of acquisition functions that we call \textit{expected conditional uncertainty (ECU)} 
criteria. Criteria falling within this general class have been explored in various contexts 
\citep{ALC,Roy2001,Mercer_kernels_IVAR,Binois_2018,deepGPAL,ALExpErrReduction,parallelSURExcursionSet}
\footnote{Many names have been used for such criteria, including expected error reduction, 
active learning Cohn, integrated mean squared prediction error, and integrated variance.},
but we focus on examples geared specifically towards surrogate-induced posterior approximation.

ECU acquisitions arise from the following logic: we would like to select an input batch 
$\parMat \in \parSpace^{\Nbatch}$ that induces an updated surrogate $\postEm[\Naugment]$ with the largest 
possible reduction in uncertainty on average over $\parSpace$. In general, we cannot compute this uncertainty
since we cannot construct $\funcEm[\Naugment]$ without observing the response $\func(\parMat)$. We therefore
instead consider the random element 
$\funcEm[\Naugment]^{\gamma} \Def \funcEm[\Ndesign] \given [\funcEm[\Ndesign](\parMat) = \gamma]$, with 
the unknown response $\func(\parMat)$ modeled by the random vector 
$\gamma \sim \mathrm{law}(\funcEm[\Ndesign](\parMat))$. Computing ECU therefore implies averaging over 
both $\parSpace$ and $\gamma$. If we again choose variance as our measure of uncertainty, we have
\begin{equation}
\acq[](\parMat) \Def 
\int_{\parSpace} \E_{\Ndesign} \Var_{\Naugment}[\postDens(\Par; \funcEm[\Naugment]^{\gamma}) \given \gamma] \ \weightDens(\Par) d\Par,
 \label{eq:acq-intvar}
\end{equation}
where the expectation is with respect to $\gamma$, and the variance is with respect to $\funcEm[\Naugment]^{\gamma}$
conditional on $\gamma$. Here, $\weightDens$ is a measure on $\parSpace$ that we are free to choose.
Outside of special cases \citep{Binois_2018,MakTargetedVar,Koermer2024} the outer integral (over $\parSpace$) 
in ECU criteria is not tractable. This is especially the case in our present setting,
and thus we focus on the sample average approximation \citep{Mercer_kernels_IVAR},
\begin{align}
&\acq[](\parMat) \Def \frac{1}{J}
\sum_{j=1}^{J} \E_{\Ndesign} \Var_{\Naugment}[\postDens(\Par_j; \funcEm[\Naugment]^{\gamma}) \given \gamma],
&&\Par_j \overset{\mathrm{iid}}{\sim} \weightDens[\Ndesign].
 \label{eq:acq-intvar-saa}
\end{align}
The $\Par_j$ are sampled at the beginning of the sequential design round and then fixed, so that
\Cref{eq:acq-intvar-saa} is viewed as a deterministic objective function. The expected variance 
terms in the summands of \Cref{eq:acq-intvar-saa} admit closed forms in the Gaussian 
settings of \Cref{prop:fwd-em-Gaussian,prop:llik-em-Gaussian}.

\paragraph{Gaussian setting: Log-Density Emulator.} 

\citet{VehtariParallelGP} derive a closed form expression for the expected variance term under a Gaussian
log-likelihood emulator. Owing to concerns regarding log-normal tails (noted above) they instead recommend
a similar criterion using the interquartile range in place of the variance. We nonetheless present the variance 
criterion below as it has an intuitive interpretation.

\begin{prop} \label{prop:acq-intvar-ldens-Gaussian}
Consider a Gaussian log-likelihood emulator 
$\llikEm[\Ndesign](\Par) \sim \Gaussian(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par))$
as in \Cref{prop:llik-em-Gaussian}. Then the ECU-variance criterion in \Cref{eq:acq-intvar-saa} reduces to
\begin{equation}
\acq[](\parMat) = \frac{1}{J}
\sum_{j=1}^{J} \Var\left[\postEm(\Par_j) \given \llikEm(\parMat) = \emMean[\Ndesign]{\llik}(\parMat) \right] \varInflation(\Par_j; \parMat)   
\label{eq:acq-intvar-saa-ldens}
\end{equation}
where
\begin{align*}
\Var\left[\postEm(\Par) \given \llikEm(\parMat) = \emMean[\Ndesign]{\llik}(\parMat) \right]
&= \priorDens(\Par)^2 \Exp{2\emMean[\Ndesign]{\llik}\left(\Par\right) + \emKer[\Naugment]{\llik}(\Par)} 
\left[\Exp{\emKer[\Naugment]{\llik}(\Par)} - 1 \right] \\
\varInflation(\Par_j; \parMat)
&= \Exp{2\left(\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)\right)}
\end{align*}
\end{prop}

Notice that the first term in \Cref{eq:acq-intvar-saa-ldens} is the variance of $\postEm[\Ndesign](\Par_j)$ conditioned on
$\{\parMat, \emMean[\Ndesign]{\llik}(\parMat)\}$; i.e., the current GP prediction is treated as if it were the true response.
The second term accounts for the uncertainty in the true response; in particular, the penalty
$\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)$ is large when the input batch $\parMat$ is highly 
``influential.''

\paragraph{Gaussian setting: Forward Model Emulator.}

\citet{SinsbeckNowak} propose an ECU-variance criterion for a forward model emulator, and note that 
the expected variance term is computable in closed-form when both the emulator and likelihood are Gaussian.
\citet{Surer2023sequential} derive a similar result in the special case of the multi-output basis vector 
model in \Cref{basis_func_model}.

\begin{prop} \label{prop:acq-intvar-fwd-Gaussian}
Consider a Gaussian forward model emulator 
$\fwdEm[\Ndesign](\Par) \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par))$
as in \Cref{prop:fwd-em-Gaussian}, and moreover assume the likelihood is Gaussian.
Then the ECU-variance criterion in \Cref{eq:acq-intvar-saa} reduces to
\begin{align*}
\acq[](\parMat) = \frac{1}{J}
\sum_{j=1}^{J} \priorDens^2(\Par_j) \left[\frac{\Gaussian\left(\obs \given \emMean{\fwd}(\Par_j), \CovComb(\Par_j) - \frac{1}{2}\likPar \right)}{2^{\dimObs/2} \det(2\pi \likPar)^{1/2}} - 
\frac{\Gaussian\left(\obs \given \emMean{\fwd}(\Par_j), \CovComb(\Par_j) - \frac{1}{2}\CovComb[\Naugment](\Par_j) \right)}{2^{\dimObs/2} \det(2\pi \CovComb[\Naugment](\Par_j))^{1/2}} \right],
\end{align*}
where $\CovComb(\Par) \Def \likPar + \emKer{\fwd}(\Par)$ and $\CovComb[\Naugment](\Par) \Def \likPar + \emKer[\Naugment]{\fwd}(\Par)$.   
\label{eq:acq-intvar-saa-fwd}
\end{prop}

\citet{weightedIVAR} consider an alternative ECU-variance criterion in the Gaussian forward model emulator setting:
\begin{align}
&\acq[](\parMat) \Def \frac{1}{J}
\sum_{j=1}^{J} \emKer[\Naugment]{\fwd}(\Par_j),
&&\Par_j \sim \postApproxNormMarg.
 \label{eq:acq-intvar-lartaud}
\end{align}
The summands target uncertainty in $\fwdEm[\Ndesign]$ rather than $\postEm[\Ndesign]$, while the weighting measure
$\weightDens = \postApproxNormMarg$ is chosen to target the goal of posterior approximation. This has the 
benefit of being much easier to compute, since the GP variance $\emKer[\Naugment]{\fwd}$ is independent of the 
unseen response $\fwd(\parMat)$. \citet{weightedIVAR} establish asymptotic convergence results for this criterion
using the stepwise uncertainty reduction framework \citep{BectSUR}. 

\subsection{Batch Design Strategies}
\subsubsection{Multipoint Optimization}
Federov exchange, stochastic optimization, continuous optimization, SAA,
subsampling approach (or put this in its own section?)

\subsubsection{Greedy Heuristic Methods} \label{sec:batch-heuristic}
\subsubsection{Sampling-Based Design}
\citet{FerEmulation,hydrologicalModel,quantileApprox}
\citet{adaptiveMultimodal} interleaves ensemble Kalman updates within the adaptive algorithm.

Connection to Thompson sampling

\subsection{Other Approaches}
These methods effectively combine posterior
inference and sequential design within a single algorithm.
The papers \citet{Li_2014,ConradLocalExactMCMC} 
adaptively construct local polynomial surrogates within an MCMC run, and are able to establish asymptotic 
exactness by showing the polynomial 
approximation error diminishes over time (\todo: verify this last point). The work \citep{ActiveLearningMCMC}
also conducts sequential design within an MCMC algorithm, using emulator predictions at points where the 
surrogate is confident, and running exact simulations at points that exceed a user-defined uncertainty tolerance.
In \citet{MCMC_GP_proposal}, an algorithm is proposed to accelerate Hamiltonian Monte Carlo by using a 
GP only for proposals, but running the exact simulation when computing the acceptance probability to 
ensure consistency. The delayed acceptance mechanism provides another common method for ensuring
asymptotic consistency \citep{DelayedAcceptance}.
These algorithms all leverage a surrogate in various ways with the goal of reducing the simulation load, while
maintaining asymptotic exactness. We discuss sequential design strategies in \cref{sec:seq-design}.

\section{Practical Recommendations} \label{sec:recs}
We now provide a set of practical recommendations for building surrogate models, constructing
posterior approximations, and sequentially refining the surrogate. \Cref{sec:case-study}
highlights these recommendations through a numerical case study.

\begin{rec} \label{rec:prop-uncertainty}
The choice surrogate model and uncertainty propagation method should be conducted jointly.
\end{rec}

In general, we emphasize the importance of propagating surrogate uncertainty in 
the posterior approximation, but are not convinced of a single ``correct'' uncertainty 
propagation method. Instead we find a few different approaches to be reasonable, but
emphasize that the empirical success of any particular method will be closely tied to the
surrogate model itself. The expected posterior approximation is conceptually appealing
as a direct summary of the random posteriir $\postEmNorm{\Ndesign}$, but is often 
challenging to implement and computationally expensive. 
Therefore, we tend to prefer the noisy MCMC methods defined in \Cref{sec:MH-approx},
which show empirical promise, require only a single MCMC run, and are applicable to 
generic probabilistic surrogates.
However, in choosing these methods one needs to appreciate how the behavior of the posterior
approximation will vary based on the properties of the surrogate predictive distribution.
For example, in log-density emulation the expected likelihood approximation can concentrate in very 
small regions of parameter space where the surrogate is uncertain, in which case a pseudo-marginal 
MCMC run will almost certainly fail. However, in this case we have found that the simple adjustment 
of incorporating a bound constraint in the emulator can solve this problem and produce reasonable 
results. The E-acc method tends to be more robust with respect to 
heavy-tailed surrogate predictions, but requires stronger requirements to ensure 
existence (\Cref{sec:existence}). 

\begin{rec} \label{rec:multiscale}
Treat density emulation as a multiscale problem.
\end{rec}

In many applications, it is common for log-likelihoods and log-posteriors to exhibit various
properties that render log-density emulation a challenging task. They tend to have very 
large dynamic ranges over the support of the prior distribution, even when their range 
may be relatively small over the posterior support. In addition, their values may drop sharply 
in certain directions while asymptoting in others. In general, we find that traditional stationary 
surrogates perform quite poorly in approximating such response surfaces. 



One must therefore take care to control the tails of the emulator in order to produce well-defined
posterior approximations. This situation is not uncommon in applications, as it may simply reflect the 
fact that the simulator predictions asymptote in certain directions of parameter space. 
In general, we find a stationary GP performs quite poorly in such contexts. One could argue that 
the placement of design points in the tails of the prior distribution could alleviate this issue, 
as it would force the GP predictions downward. Although the GP would revert to its prior
as the distance from the design points increases, the hope is  
that an MCMC algorithm would never reach such regions, as the design points in the tail
would effectively create a ``moat'' of low probability.
Moreover, if emulating the log-likelihood then one could also rely on the prior to drive the posterior 
density emulator to zero.


We now provide some practical guidelines motivated by these integrability considerations.
Intuitively, the surrogate ought to be constructed such that every realization of $\postEm[\Ndesign]$
is integrable (i.e., $\normCstEm$ exists almost surely). From a practical perspective, our general 
recommendation is to design the surrogate model to ensure that $\lpostEm[\Ndesign](\Par) \to -\infty$
almost surely as $\norm{\Par} \to \infty$. As a prior predictive check, we propose to   
assess the limiting behavior of the predictive distribution along different directions in parameter
space. For example, this could take the form of a graphical check by plotting quantiles of 
the predictive distribution at sequences of points spaced along the coordinate axes. 
See table \todo for an example. 

In order to achieve the requirement $\lpostEm[\Ndesign](\Par) \to -\infty$, it is 
necessary to achieve a reasonable approximation of the global trend of the response 
surface that is being emulated over $\parSpace$. This is particularly important in the
log-density emulation setting. In this case, we generally recommend against the use of 
stationary surrogate models, though they can perform reasonably well in particular cases.
Instead, one natural approach is to fit a trend to capture the global structure, along with an 
additional model to capture local deviations from the trend. In the GP setting, this idea has 
been realized by combining quadratic mean functions with stationary kernels 
\citep{emPostDens,SinsbeckNowak,VehtariParallelGP,llikEmABC,ABCGP,pseudoMarginalGP}. In 
\citet{emPostDens} the quadratic trend is constructed via a sequential importance sampling
scheme, while in the other articles the quadratic coefficients are learned alongside the 
kernel hyperparameters. Various works have also proposed combining polynomial chaos
expansions and Gaussian processes \citep{SinsbeckNowak} (\todo: cite others).
\todo: cite Roshan Joseph paper here as well

\begin{rec} \label{rec:oversample-tails}
The design should oversample the tails.
\end{rec}
As discussed in \Cref{sec:seq-design}, it is natural to aim to place design points in regions
where $\postDensNorm(\Par)$ is large. However, it is often critical to also include sufficiently many
points in regions where $\postDensNorm(\Par)$ is small, and at least initially, where 
$\priorDens(\Par)$ is small as well. Intuitively, such points are required to 
learn the global trend of the response surface (see \Cref{rec:multiscale}) and to ``pin down''
regions of negligible posterior mass. In constructing the initial design, we often find it helpful
to sample design points from an over-dispersed version of $\priorDens$. In the 
case study in \Cref{sec:case-study} we sample the initial design from the prior, but also
explicitly include additional points in the tails of the prior. We also suggest that sequential
acquisition schemes are designed to ensure sufficient exploration. This can be encoded
in an acquisition function (e.g., via the choice of $\weightDens[\Ndesign]$ in \Cref{eq:acq-intvar})
or via the explicit selection of exploratory points.  
For example, at each active learning iteration \citet{gp_surrogates_random_exploration} 
select a subset of new inputs by optimizing an acquisition criterion, but also include some 
points sampled from the prior. Alternatively, \citet{FerEmulation} subsample new points from a
mixture of the prior with the current posterior approximation. We note that 
these recommendations are in line with the theoretical results of \citet{StuartTeck2}, who 
suggest designs based on an over-dispersed version of $\postDensNorm(\Par)$.
The notion of oversampling the tails has also been proposed to construct designs
for numerical integration \citep{briol2017sampling}.

\begin{rec} \label{rec:pred-check-tails}
Check the tails of the induced posterior density surrogate.
\end{rec}
As discussed in \Cref{sec:existence}, one must take care in designing emulators that
induce well-defined posterior approximations. In MCMC-based inference, this is crucial for
avoiding divergent MCMC chains. \Cref{rec:multiscale, rec:oversample-tails}
offer modeling suggestions to help avoid such issues, but they do not provide certified 
guarantees. We suggest an explicit prior predictive check to diagnose problematic 
tail behavior in the induced posterior surrogate $\postEm[\Ndesign]$. A simple graphical
check consists of plotting an upper quantile of $\mathrm{law}(\lpostEm[\Nesign](\Par))$ 
as $\Par$ varies over the parameter space. One should ensure that $\Par$ is allowed to 
vary well past the extent of the design points in order to assess the extrapolation behavior 
of the surrogate. A high quantile (say, 95\% or 99\%) is chosen to assess whether 
the distribution of $\lpostEm[\Ndesign](\Par)$ appears to be converging almost surely 
to $-\infty$ in the limit. For multidimensional input spaces we suggest varying one 
parameter at a time, projecting onto different fixed values of the other parameters.
Performing this simple check does not require any additional 
evaluations of the expensive simulator, and can prevent wasting resources by having
to wait to uncover issues with the surrogate during inference (for example, 
\citet{emPostDens} use divergent MCMC chains as a sort of surrogate diagnostic check).
 See \Cref{fig:vsem-prior-check-lpostem} in the below case study for an example
 of this predictive check.

\begin{rec} \label{rec:bound-constraints}
Enforce bound constraints.
\end{rec}

Recommend rectified adjustment.

\begin{rec} \label{rec:multimodal-post-approx}
Embrace multi-modal posterior approximations.
\end{rec}

In utilizing a flexible statistical model to approximate a complex response surface, one expects
the approximation to feature local modes. Given that the surface corresponds to a
log-posterior density in this setting, it is natural to anticipate that even a deterministic emulator
may induce multimodal posterior approximations, even when the true posterior is unimodal. 
Multimodality is even more common when propagating surrogate uncertainty, as the uncertainty 
level will vary over the input space (e.g., consider the ``sausage-shaped'' confidence 
bands in \Cref{fig:em_dist_1d}). For example, modes in the approximate posterior may reflect
that the surrogate expects high posterior mass in a certain region, or that the surrogate is simply 
uncertain in the region. This uncertainty quantification is essential for acknowledging surrogate 
inadequacies and for guiding future improvements (\Cref{sec:seq-design}). However, it does
present computational challenges in performing inference. While specialized algorithms 
can be employed \citep{adaptiveMultimodal}, we typically prefer practical heuristic approaches
in the spirit of \citep{multimodalYao}. In particular, we run multiple MCMC chains in parallel with 
the aim of characterizing the dominant modes of the distribution. We perform within-chain 
convergence diagnostics to validate that the chains are well-mixed locally, and then 
utilize heuristics to assign weights to each chain.

\vspace

\todo: Add active learning recommendation

\section{Numerical Case Study} \label{sec:case-study}
We present a numerical illustration of the surrogate-assisted Bayesian inference workflow, highlighting
the practical recommendations given in the previous section.
Our illustrative example is motivated by the problem of producing near-term forecasts of the 
terrestrial carbon cycle \citep{nearTermForecasts,FerEmulation}. In this setting, both model parameters 
and initial conditions are typically unknown and must be learned from observational data. Performing 
such parameter estimation runs into computational challenges for large-scale land surface models, 
underscoring the potential for surrogates in this domain \citep{paramLSM}.  

\subsection{Model Setup}
We start by introducing the mechanistic simulator model, and then define a Bayesian inverse 
problem with respect to this model.

\subsubsection{Mechanistic Model}
As a simple concrete example, we utilize the \textit{Very Simple Ecosystem Model} (VSEM), a toy model capturing the basic 
structure of more complex land surface models, thus ideally suited for algorithm evaluation \citep{vsem}. The model 
describes the evolution of the state vector
\begin{align*}
\state(\Time) \Def [\stateV(\Time), \stateR(\Time), \stateS(\Time)]^\top \in \R_{\geq 0}^{3}, 
\end{align*}
with the state variables representing the quantity of carbon (\textrm{kg C/$m^2$}) in above-ground vegetation, below-ground 
vegetation (roots), and soil reservoirs, respectively. The VSEM model is given by the system of coupled 
ordinary differential equations
\begin{align}
\dstateV(\Time) &= \alphaV \NPP(\stateV(\Time), \forcing(\Time)) - \frac{\stateV(\Time)}{\tauV} \\
\dstateR(\Time) &= (1.0 - \alphaV) \NPP(\stateV(\Time), \forcing(\Time)) - \frac{\stateR(\Time)}{\tauR} \nonumber \\ 
\dstateS(\Time) &= \frac{\stateR(\Time)}{\tauR} + \frac{\stateV(\Time)}{\tauV} - \frac{\stateS(\Time)}{\tauS}, \nonumber
\end{align}
where the model forcing $\forcing(\Time)$ is provided by photosynthetically active radiation 
(\textrm{MJ/$m^2$/day}), and the dynamics rely on the following parameterized model of 
Net Primary Productivity (NPP; \textrm{kg C/$m^2$/day}),
\begin{align}
\NPP(\stateV, \forcing) &= (1 - \fracRespiration) \GPP(\stateV, \forcing) \\
\GPP(\stateV, \forcing) &= \forcing \cdot \LUE \cdot \left[1 - \exp\left\{-\KEXT \cdot \LAI(\stateV) \right\} \right] \nonumber \\
\LAI(\stateV) &= \LAR \cdot \stateV, \nonumber
\end{align} 
where $\GPP(\stateV, \forcing)$ and $\LAI(\stateV)$ model Gross Primary Productivity (GPP; \textrm{kg C/$m^2$/day})
and Leaf Area Index (LAI; \textrm{$m^2/m^2$}), respectively.
Note that the ODE is of the form \ref{ode_ivp}, with the caveat that the dynamics $\odeRHS$ additionally depend on a 
time-dependent forcing $\forcing(\Time)$. Given a value of $\Par$, we numerically solve the ODE at a daily time step
via the basic Euler scheme implemented in the R \verb+BayesianTools+ package \citep{vsem}. We recall the discretized 
parameter-to-state map 
$\solutionOp: \Par \mapsto \left[\indexTime[0]{\state}(\Par), \dots, \indexTime[\NTimeStep]{\state}(\Par) \right]^\top$, 
as defined in \Cref{eq:ode-solution-op}.

Potential calibration parameters in this model include 
$\{\alphaV, \tauV, \tauR, \tauS, \LUE, \KEXT, \fracRespiration, \LAR\}$, as well as the initial conditions for the three state
variables $\{\stateV(0), \stateR(0), \stateS(0)\}$.
Noting that the model is over-parameterized, we will focus on estimating 
$\Par \Def \{\KEXT, \fracRespiration, \tauV, \stateV(0)\}$ with the remaining parameters held fixed.

\subsubsection{Statistical Model}
We consider estimating the parameters $\Par$ given noisy monthly averages of LAI over a two year period.
We assume an additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{eq:vsem-inv-prob} \\
\noise &\sim \Gaussian(0, \sigma^2 I) \nonumber 
\end{align}
where $\fwd: \parSpace \to \R^{24}$ maps to the twenty-four LAI averages; i.e., the first entry of $\fwd(\Par)$
is given by 
\begin{equation}
\fwd_{1}(\Par) \Def \frac{\LAR}{30} \sum_{\timeIndex=1}^{30} \state_{\textrm{v},\timeIndex}(\Par),
\end{equation}
and the remaining entries simply change the indices of the summation. For simplicity, we fix $\sigma^2$
and assume a priori independence over the entries of $\Par$. Synthetic data $\obs$ is simulated
using \Cref{eq:vsem-inv-prob} with fixed ground truth values $\{\Par_{\star}, \sigma^2_{\star}\}$. The same 
model is used when solving the inverse problem, but the values of the fixed parameters (those not
being estimated) and the noise variance are changed, implying the presence of parametric misspecification.
Table \todo compares these misspecified values relative to the ground truth, while
table \todo summarizes the prior distribution $\Par \sim \priorDens$. As a baseline for comparison, we 
draw samples from the posterior using exact MCMC. \Cref{fig:vsem_prior_post} compares the prior 
and posterior marginal distributions, while \Cref{fig:vsem_pred_dists} compares the prior and 
posterior predictive distributions over LAI trajectories.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.24\textwidth} % Cv 
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_post_par_Cv.png}}
         \caption{$\stateV(0)$}
         \label{fig:vsem_prior_post_Cv}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth} % GAMMA
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_post_par_GAMMA.png}}
         \caption{$\gamma$}
         \label{fig:vsem_prior_post_GAMMA}
     \end{subfigure}
      \begin{subfigure}[b]{0.24\textwidth} % tauV
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_post_par_tauV.png}}
         \caption{$\tauV$}
         \label{fig:vsem_prior_post_tauV}
     \end{subfigure}
          \begin{subfigure}[b]{0.24\textwidth} % KEXT
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_post_par_KEXT.png}}
         \caption{$\KEXT$}
         \label{fig:vsem_prior_post_KEXT}
     \end{subfigure}
        \caption{Marginal prior (black dashed) and exact posterior (red) distributions for the four
        calibration parameters.}
        \label{fig:vsem_prior_post}
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth} % Prior predictive distribution
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_pred_dist.png}}
         \caption{Prior Predictive}
         \label{fig:vsem_prior_pred}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % Posterior predictive distribution
         \centering
         \includegraphics[width=\textwidth]{{vsem/post_pred_dist.png}}
         \caption{Posterior Predictive}
         \label{fig:vsem_post_pred}
     \end{subfigure}
        \caption{(Left) The distribution over LAI trajectories induced by $\Par \sim \priorDens$. 
        The red points are the observed noisy observations of monthly LAI averages, with the 
        vertical bars indicating $\pm \sigma$ observation noise. The blue shaded region captures 
        90\% prior predictive probability. The gray lines are prior predictive samples, and the black 
        line is the ground truth LAI trajectory used to generate the data. (Right) The analogous plot
        for the posterior predictive distribution; i.e., the distribution over LAI trajectories induced 
        by $\Par \sim \postDens$.}
        \label{fig:vsem_pred_dists}
\end{figure}

\subsection{Initial Surrogate Fits}
For this problem, we compare GP emulators for both the forward model and log-posterior.
Let $\funcEm[\Ndesign]$ denote the underlying GP prediction of $\func$.
To evaluate these emulators we consider both prior and posterior averaged continuous
ranked probability score (CRPS; \citep{scoringRules})
\begin{align}
\mathrm{crps}(\funcEm[\Ndesign]) &\Def \int_{\parSpace} \mathrm{crps}(\funcEm[\Ndesign](\Par), \func(\Par)) \weightDens[](\Par) d\Par
\end{align}
and multivariate log-score (i.e., predictive deviance; \citep{scoringRules})
\begin{align}
\mathrm{logS}(\funcEm[\Ndesign]) 
&\Def \log \Gaussian(\func(\parMat) \given \emMean[\Ndesign]{}(\parMat), \emKer[\Ndesign]{}(\parMat)),
&&\parMat \overset{\mathrm{iid}}{\sim} \weightDens[].
\end{align}

The choice $\weightDens[] \in \{\priorDens, \postDensNorm\}$ determines whether error is assessed with
respect to the prior or true posterior; both scores are approximated by sampling $1000$ points 
from the relevant distribution. We similarly estimate 
\begin{align}
\mathrm{mae}(\postEm[\Ndesign]) &\Def 
\int_{\parSpace} \abs{\postApproxMean[\Ndesign](\Par) - \postDens(\Par)} \weightDens[](\Par) d\Par,
\end{align}
to evaluate the quality of the plug-in mean posterior approximation.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth} % lpost em, prior evaluation
         \centering
         \includegraphics[width=\textwidth]{{vsem/pred_scatter_prior_lpostem.png}}
         \caption{$\lpostEm[\Ndesign], \weightDens[] = \priorDens$}
         \label{fig:vsem_pred_scatter_prior_lpostem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth} % lpost em, posterior evaluation
         \centering
         \includegraphics[width=\textwidth]{{vsem/pred_scatter_post_lpostem.png}}
         \caption{$\lpostEm[\Ndesign], \weightDens[] = \postDensNorm$}
         \label{fig:vsem_pred_scatter_post_lpostem}
     \end{subfigure}
          \begin{subfigure}[b]{0.49\textwidth} % fwd em, prior evaluation
         \centering
         \includegraphics[width=\textwidth]{{vsem/pred_scatter_prior_lpostem.png}}
         \caption{$\fwdEm[\Ndesign], \weightDens[] = \priorDens$}
         \label{fig:vsem_pred_scatter_prior_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth} % fwd em, posterior evaluation
         \centering
         \includegraphics[width=\textwidth]{{vsem/pred_scatter_post_lpostem.png}}
         \caption{$\fwdEm[\Ndesign], \weightDens[] = \postDensNorm$}
         \label{fig:vsem_pred_scatter_post_fwdem}
     \end{subfigure}
        \caption{Emulator predictions based on the initial design. The top and bottom rows correspond
        to the forward model and log-posterior emulator, respectively. The left and right columns correspond
        to evaluation inputs sampled from the prior and exact posterior, respectively. The points summarize
        emulator mean predictions, while vertical bars are 90\% emulator predictive intervals. The orange 
        bars indicate that the 90\% interval does not contain the truth.}
        \label{fig:vsem_pred_scatter}
\end{figure}

\subsubsection{Forward Model Emulator}
\subsubsection{Log-Posterior Emulator}
This inverse problem presents several challenges for log-density emulators: the dynamic 
range of the log-likelihood over the prior support is quite large, the parameter space is 
unbounded, and the likelihood remains relatively flat along certain directions in parameter
space. One must therefore take care to control the tails of the emulator in order to produce well-defined
posterior approximations. This situation is not uncommon in applications, as it may simply reflect the 
fact that the simulator predictions asymptote in certain directions of parameter space. 
In general, we find a stationary GP performs quite poorly in such contexts. One could argue that 
the placement of design points in the tails of the prior distribution could alleviate this issue, 
as it would force the GP predictions downward. Although the GP would revert to its prior
as the distance from the design points increases, the hope is  
that an MCMC algorithm would never reach such regions, as the design points in the tail
would effectively create a ``moat'' of low probability.
Moreover, if emulating the log-likelihood then one could also rely on the prior to drive the posterior 
density emulator to zero.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth} % tauV
         \centering
         \includegraphics[width=\textwidth]{{vsem/extrap_q95_lpostem_tauV.png}}
         \caption{$\tauV$}
         \label{fig:vsem-prior-check-lpostem-tauv}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth} % Cv
         \centering
         \includegraphics[width=\textwidth]{{vsem/extrap_q95_lpostem_Cv.png}}
         \caption{$\stateV(0)$}
         \label{fig:vsem-prior-check-lpostem-cv}
     \end{subfigure}
        \caption{The prior predictive check recommended in \Cref{rec:pred-check-tails} 
        for the log-posterior emulator, focusing on the two parameters with unbounded support. 
        The lines correspond to the $95^{\mathrm{th}}$ percentile of the log-posterior surrogate 
        $\mathrm{Quantile}_{0.95}(\lpostEm[\Ndesign](\Par))$ as only one parameter in $\Par$ 
        is varied. Different lines correspond to different fixed values of the remaining parameters,
        which have been sampled from $\priorDens$. Dashed vertical lines indicate the extent of 
        the design points in that dimension, with inputs outside of these bounds representing
        pure extrapolation.}
        \label{fig:vsem-prior-check-lpostem}
\end{figure}

\section{Related Work} \label{sec:related-work}

\subsection{Computer Model Calibration} \label{sec:computer-model-calibration}
The challenge of emulating a black-box computer model has received widespread attention 
well beyond the scope of Bayesian inference. The design of surrogate models for Bayesian 
inverse problems may be informed by the vast literature from the computer experiments, 
engineering, applied math, machine learning, and statistics communities. As a starting point, 
we refer readers to \citet{gramacy2020surrogates,design_analysis_computer_experiments,SanterCompExp,UQpredCompSci} 
and the references therein. It is worth taking a moment to clarify the scope of our review 
with respect to related work in the computer experiments literature in particular.
Indeed, early work in this field addressed the challenge of learning 
model parameters  $\Par$ from observational data via the use of a surrogate for $\fwd$, a problem 
commonly referred to as \textit{computer model calibration} (see \citet{computerModelCalibrationReview}
for a review). When cast as a problem of Bayesian inference, the calibration problem falls within the 
framework considered in this article. However, much of this early calibration work focused on 
the added challenge of learning a discrepancy term between the computer model $\fwd$ and the 
true underlying system \citep{ModelDiscrepancy,emPostDens,OakleyllikEm}. 
For example, the pioneering work \citet{KOH} considers jointly learning 
a forward model emulator, calibration parameters, and a discrepancy function within a single 
Bayesian model. By contrast, we do not consider discrepancy modeling in this review, and moreover
focus on the alternative modular workflow, where the surrogate is fit offline 
without seeing the calibration data $\obs$ \citep{modularization}. 
\footnote{This point is specific to the forward model emulation setting. Log-density emulators 
do depend on $\obs$, since the log-likelihood is a function of the data.}
This modular two-stage approach naturally leads to 
the question of how to propagate the surrogate uncertainty in the calibration stage, which is one 
of the central questions motivating this review.

\subsection{Probabilistic Numerics} \label{sec:prob-numerics}
See Sullivan, Hennig papers; and Teckentrup random forward models paper section 5

\subsection{Approximate Bayesian Computation and Amortized Inference} \label{sec:abc}
\citet{Lueckmann2018LikelihoodfreeIW,VehtariParallelGP,ABCApproxLik,
BurknerAmortized,llikEmABC,ABCGP}

\subsection{Other}
Multifidelity methods? Active subspaces? Data subsampling/randomized misfits?

\section{Conclusion} \label{sec:conclusion}


% Appendix 
\section{Appendix}

\subsection{Marginal Approximation with Gaussian Likelihood: Forward Model Emulation}
The closed-form computations related to the marginal approximation with a Gaussian 
likelihood follow from standard results regarding the convolution of Gaussian densities.  

\begin{prop} \label{Gaussian_convolution}
Let $\Gaussian(A \mu, \likPar)$ and $\Gaussian(m, C)$ be Gaussian distributions on $\R^{\dimObs}$ and $\R^{\dimPar}$, 
respectively, with $A \in \R^{\dimObs \times \dimPar}$ and $\likPar, C$ symmetric, positive definite matrices. Then 
\begin{align*}
\int_{\R^{\dimPar}} \Gaussian(\obs | A \mu, \likPar) \Gaussian(\mu | m, C) d\mu
&= \Gaussian(\obs | Am, \likPar + ACA^\top). 
\end{align*}
\end{prop}

\begin{proof} 
\todo
\end{proof}

We next prove a lemma that will be used in the proofs of \Cref{prop:Gaussian_marginal_moments} 
and \Cref{lemma:fwd-Gaussian-density-dist} below. 
\begin{lemma} \label{lemma:squared_Gaussian_density}
Let $\Gaussian(m, C)$ be a Gaussian distribution on $\R^{\dimObs}$ with $C$ a symmetric, positive-definite 
matrix. Then, for $\obs \in \R^{\dimObs}$, 
\begin{align*}
\Gaussian(\obs | m, C)^2 
&= 2^{-\dimObs/2} \det(2\pi C)^{-1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right). 
\end{align*}
\end{lemma}

\begin{proof}
\begin{align*}
\Gaussian(\obs | m, C)^2 
&= \det(2\pi C)^{-1} \Exp{-\frac{1}{2} (\obs - m)^\top \left[\frac{1}{2}C \right]^{-1}(\obs - m)} \\
&= \det(2\pi C)^{-1} \det(2\pi (1/2)C)^{1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right) \\
&= 2^{-\dimObs/2} \det(2\pi C)^{-1/2} \Gaussian\left(\obs | m, \frac{1}{2}C\right). 
\end{align*}
\end{proof}

\begin{prop} \label{prop:Gaussian_marginal_moments}
Assume $\obs | \mu \sim \Gaussian(A \mu, \likPar)$ and $\mu \sim \Gaussian(m, C)$, where $\mu \in \R^{\dimPar}$, 
$A \in \R^{\dimObs \times \dimPar}$, and $\likPar$, $C$ are both symmetric, positive definite. Then 
\begin{align}
\E\left[\Gaussian(\obs | A \mu, \likPar) \right] &= \Gaussian(\obs | Am, \likPar + ACA^\top) \\
\Var\left[\Gaussian(\obs | A \mu, \likPar) \right] 
&= \frac{\Gaussian\left(\obs | Am, \frac{1}{2} \likPar + ACA^\top \right)}{2^{\dimObs/2} \det(2\pi \likPar)^{1/2}} - 
\frac{\Gaussian\left(\obs | Am, \frac{1}{2}[\likPar + ACA^\top] \right)}{2^{\dimObs/2} \det(2\pi[\likPar + ACA^\top])^{1/2}}
\end{align}
\end{prop}

\begin{proof} 
The first result follows immediately from \Cref{Gaussian_convolution}. For the variance, we have 
\begin{align}
\Var\left[\Gaussian(\obs | A \mu, \likPar) \right] 
&= \E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right] - \E\left[\Gaussian(\obs | A \mu, \likPar) \right]^2 \nonumber \\
&= \E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right] - \Gaussian(\obs | Am, \likPar + ACA^\top)^2. \label{two_terms_variance}
\end{align}
Starting with the first term, we apply \Cref{lemma:squared_Gaussian_density} and 
\Cref{Gaussian_convolution}, respectively, to obtain 
\begin{align*}
\E\left[\Gaussian(\obs | A \mu, \likPar)^2 \right]
&= 2^{-\dimObs/2} \det(2\pi \likPar)^{-1/2} \E\left[\Gaussian\left(\obs | A\mu, \frac{1}{2}\likPar \right)\right] \\
&= 2^{-\dimObs/2} \det(2\pi \likPar)^{-1/2} \Gaussian\left(\obs | Am, \frac{1}{2}\likPar + ACA^\top \right).
\end{align*}
For the second term in \ref{two_terms_variance}, another application of \Cref{lemma:squared_Gaussian_density} gives
\begin{align*}
\Gaussian(\obs | Am, \likPar + ACA^\top)^2
&= 2^{-\dimObs/2} \det(2\pi[\likPar + ACA^\top])^{-1/2} \Gaussian\left(\obs | Am, \frac{1}{2}[\likPar + ACA^\top]\right).
\end{align*}
Plugging these expressions back into \ref{two_terms_variance} completes the proof. 
\end{proof}


\subsection{Marginal Acceptance Probability}
In this section we derive an expression for 
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right], 
\end{align}
as considered in \ref{acc_prob_joint_marg}. We start by noting 
\begin{align}
\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)
&\sim \LN\left(\log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par), \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right] \right),
\label{acc_ratio_LN}
\end{align}
where 
\begin{align*}
C \Def \frac{\priorDens(\propPar) \propDens(\propPar, \Par)}{\priorDens(\Par) \propDens(\Par, \propPar)}.
\end{align*}
Thus, the required computation reduces to computing the expectation of $\min\left\{1, Y \right\}$, where $Y$ is a log-normally distributed 
random variable. 

\begin{lemma} \label{lemma:exp_max_one_LN}
Let $Y \sim \LN(m, s^2)$. Then, 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right] &= \GaussianCDF\left(\frac{m}{s} \right) + \GaussianCDF\left(-\frac{m + s^2}{s} \right) e^{m + \frac{1}{2}s^2}
\end{align}
\end{lemma}

\begin{proof}
We have 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right]
&= \int_{0}^{\infty} \min\{1, y\} \LN(y | m, s^2) dy \nonumber \\
&= \int_{-\infty}^{\infty} \min\{1, e^x\} \Gaussian(x | m, s^2) dx \nonumber \\
&= \int_{0}^{\infty} \Gaussian(x | m, s^2) dx +  \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx. \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx.  
\label{second_integral} 
\end{align}
For the integral in \ref{second_integral} we combine the exponential terms and complete the square. 
This yields 
\begin{align*}
&\int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx \\
&= \Exp{-\frac{m^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2} \left[\frac{x^2}{2} - 2\left(\frac{m}{s^2} + 1\right)x  \right]} dx \\
&= \Exp{-\frac{m^2}{2s^2} + \frac{[m + s^2]^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2s^2}(x - [m+s^2])^2} dx \\
&= \Exp{m + \frac{1}{2}s^2} \GaussianCDF\left(-\frac{m+s^2}{s}\right).
\end{align*}
Plugging this back into \ref{second_integral} completes the proof. 
\end{proof}

We now apply \Cref{lemma:exp_max_one_LN} to obtain the expression for the marginal acceptance probability. 
\begin{prop} \label{prop:joint-marg-accept-prob}
\begin{align*}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right] 
= w_1 + w_2 \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar), 
\end{align*}
where 
\begin{align*}
w_1 &= \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \Prob\left(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq \Exp{-\Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]}\right).
\end{align*}
\end{prop}

\begin{proof}
We recall from \ref{acc_ratio_LN} that $\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \sim \LN(m, s^2)$, with 
\begin{align*}
m &= \log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par) \\
s^2 &= \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]. 
\end{align*}
We thus have 
\begin{align*}
e^{m + \frac{1}{2}s^2} = \E_{\llikEm}\left[\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \right] 
=  \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar). 
\end{align*}
It remains to verify the expressions for the weights $w_1$ and $w_2$. Letting $Z \sim \Gaussian(0,1)$, 
we apply \Cref{lemma:exp_max_one_LN} to obtain
\begin{align*}
w_1 &= \GaussianCDF(m/s) = \Prob(Z \leq m/s) = \Prob(\Exp{m+sZ} \geq 1) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \GaussianCDF(-(m+s^2)/s) = \Prob(\Exp{m + sZ} \leq e^{-s}) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq e^{-s}).
\end{align*}
\end{proof}

\subsection{Transition Kernels of Approximate MCMC Algorithms} \label{transition_kernel_derivations}
We derive the expression for the \textit{MCWMH-joint} transition 
kernel $\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}$ given in \ref{MCWMH-joint-kernel}. 
Let $\Par \in \parSpace$ and $A \subset \parSpace$ a Borel set. 
We recall that the kernel for a standard MH algorithm is given by 
\begin{align}
\MarkovKernel(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar
+ [1 - \avgAccProbMH(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
where $\avgAccProbMH(\Par)$ is the overall acceptance probability, averaged over all proposals,
\begin{align}
\avgAccProbMH(\Par)
&= \int_{\parSpace} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar. 
\end{align}
Denote $\parMat \Def \{\Par, \propPar\}$. 
The \textit{MCWMH-joint} algorithm replaces the exact log-likelihood evaluations 
$\llik(\parMat) \Def [\llik(\Par), \llik(\propPar)]^\top$ used to define $\propDens(\Par, \propPar)$ by 
sampled approximate values 
\begin{align}
\llikSamp_{\parMat} \Def [\llikSamp, \llikSampProp]^\top \sim \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)). 
\end{align}
We let $\Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})$ denote the approximate acceptance probability defined using 
the sampled values $\llikSamp_{\parMat}$. The probability of accepting a state in the set $A$, conditional on the sample $\llikSamp_{\parMat}$, 
is thus 
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat})
&=  \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat}) d\propPar. 
\end{align}
The unconditional probability follows from the law of total probability:
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par)
&= \int_{\R^2} \Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat}) \Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \\
&= \int_{\R^2} \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\propPar \ d\llikSamp_{\parMat} \\
&= \int_{A} \propDens(\Par, \propPar) \int_{\R^2}  \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \ d\propPar \label{flip_integral_order} \\
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar
\end{align}
where \ref{flip_integral_order} follows from Tonelli's theorem, and using the definition of $\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)$
in \ref{acc_prob_joint_marg}. Setting $A \Def \parSpace$ in the above integral yields the overall acceptance probability 
\begin{align}
\llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par) \Def \int_{\parSpace} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar. 
\end{align}
These quantities thus give 
\begin{align}
\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar
+ [1 - \llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
which follows from same derivations for the standard MH kernel \ref{MH-kernel} with the marginal acceptance probabilities substituted for
the original ones. The transition kernel for the \textit{MCWMH-ind} algorithm follows immediately by marginalizing with respect 
to $\Gaussian(\emMean[\Ndesign]{\llik}\left(\parMat), \diag\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar) \right\}\right)$
in place of the full joint distribution. 

\subsection{Sequential Design Calculations}
We start by stating an identity for the inversion of partitioned matrices, which is very useful in updating GPs by 
conditioning on new design points. The generic result can be found in the lecture notes \cite{MinkaMatrixLectures}, 
but we specialize the statement to the GP setting.  

\subsubsection{Useful Lemmas}

\begin{prop} \label{partitioned-matrix-inverse}
Let $\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$ be a GP prior, with 
$\funcEm[\Ndesign] \Def \funcPrior | [\funcPrior(\designIn) = \func(\designIn)] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$  
the GP predictive distribution after conditioning on the design $(\designIn, \func(\designIn))$. Let 
$\designBatchIn$ be a set of $\Nbatch$ new design points. Define 
$\kerMat[\Ndesign] \Def \gpKerPrior(\designIn)$, $\kerMat[\Nbatch] \Def \gpKerPrior(\designBatchIn)$,
and $\kerMat[\Ndesign,\Nbatch] \Def \gpKerPrior(\designIn[\Ndesign], \designBatchIn)$.  
Then, letting 
$\designIn[\Naugment] \Def \designIn \cup \designBatchIn$, the inverse of the kernel matrix 
evaluated on the augmented design satisfies 
\begin{align}
\gpKerPrior(\designIn[\Naugment])^{-1}
&= \begin{pmatrix} \kerMat[\Ndesign] & \kerMat[\Ndesign,\Nbatch] \\
\kerMat[\Ndesign,\Nbatch]^\top & \kerMat[\Nbatch] \end{pmatrix}^{-1}
=  \begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix},
\end{align}
where 
\begin{align}
\tilde{K} = \kerMat[\Ndesign]^{-1} + \kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Nbatch,\Ndesign] \kerMat[\Ndesign]^{-1}.
\end{align}
Thus, assuming $\kerMat[\Ndesign]^{-1}$ has already been computed, $\gpKerPrior(\designIn[\Naugment])^{-1}$ can be constructed in an additional 
$\BigO(\Nbatch^3 + \Nbatch \Ndesign^2 + \Nbatch^2 \Ndesign)$ operations. 
\end{prop}

We repeatedly use the fact that $\funcPrior|[\funcPrior(\designIn[\Naugment]) = \func(\designIn[\Naugment])]$ and 
$\funcEm[\Ndesign] | [\funcEm[\Ndesign](\designBatchIn) = \func(\designBatchIn)]$ are equal in distribution ; 
i.e., conditioning the GP prior on the entire design 
is equivalent to sequentially conditioning on subsets of the design. For the sake of completeness, we provide the rigorous justification for this below.

\begin{lemma} \label{lemma:gp-condition-order}
Let $\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$ be a GP prior. Consider a set of $\Naugment$ design points $\{\designIn[\Naugment], \funcVal[\Naugment]\}$
partitioned as $\designIn[\Naugment] = \designIn[\Ndesign] \cup \designBatchIn$ and $\funcVal[\Naugment] = \funcVal[\Ndesign] \cup \funcVal[\Nbatch]$.
Let $\funcEm[\Ndesign] \Def \func | [\func(\designIn[\Ndesign]) = \funcVal[\Ndesign]] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$. 
Then the random process $\funcPrior | [\funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]]$ is equal in distribution to the random process
$\funcEm[\Ndesign] | [\funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]]$. 
\end{lemma} 

\begin{proof} 
Since both processes in question are Gaussian it suffices to check that 
\begin{align*}
\E[\funcPrior(\parMat) | \funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]] 
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]] \\
\Cov[\funcPrior(\parMat) | \funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]] 
&= \Cov[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]],
\end{align*}
for an arbitrary finite set of inputs $\parMat \subset \parSpace$. The quantities on the lefthand side are 
$\gpMean[\Naugment](\parMat)$ and $\gpKer[\Naugment](\parMat)$, by definition. We begin by expanding 
the expression $\gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}$, noting that we 
are borrowing the notation from \Cref{partitioned-matrix-inverse}. Applying the partitioned matrix 
inversion identity from \Cref{partitioned-matrix-inverse} yields 
\begin{align*}
\gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}
&= \begin{pmatrix} \gpKerPrior(\parMat, \designIn[\Ndesign]) &  \gpKerPrior(\parMat, \designIn[\Nbatch]) \end{pmatrix}
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix}.
\end{align*}
Denoting $\funcVal[\Ndesign]^\prime \Def \funcVal[\Ndesign] - \gpMeanPrior(\designIn[\Ndesign])$ and 
$\funcVal[\Nbatch]^\prime \Def \funcVal[\Nbatch] - \gpMeanPrior(\designIn[\Nbatch])$, the predictive mean $\gpMean[\Naugment](\parMat)$ is thus given by 
\begin{align*}
\gpMean[\Naugment](\parMat)
&= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}[\funcVal[\Naugment] - \gpMeanPrior(\designIn[\Naugment])] \\
&= \gpMeanPrior(\parMat) + \begin{pmatrix} \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix}^\top
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix} 
\begin{pmatrix}  \funcVal[\Ndesign]^\prime \\  \funcVal[\Nbatch]^\prime \end{pmatrix} \\
&= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn[\Ndesign])\kerMat[\Ndesign]^{-1}\funcVal[\Ndesign]^\prime + 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \funcVal[\Nbatch]^\prime - 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1} \funcVal[\Ndesign]^\prime \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch]^\prime - \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1} \funcVal[\Ndesign]^\prime] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch]^\prime - \gpMean[\Ndesign](\designIn[\Nbatch]) + \gpMeanPrior(\designIn[\Nbatch])] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch] - \gpMean[\Ndesign](\designIn[\Nbatch])] \\
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]] 
\end{align*}
where we have used the fact that the predictive covariance of the GP $\funcEm[\Ndesign]$ gives 
\[
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) = \gpKerPrior(\parMat, \designIn[\Nbatch]) - 
\gpKerPrior(\parMat, \designIn[\Ndesign])\kerMat[\Ndesign]^{-1} \gpKerPrior(\designIn[\Ndesign], \parMat).
\]
The covariance calculation proceeds similarly by replacing $\funcVal[\Ndesign]^\prime$ and $\funcVal[\Nbatch]^\prime$ with 
$\gpKerPrior(\designIn[\Ndesign], \parMat)$ and $\gpKerPrior(\designIn[\Nbatch], \parMat)$, respectively. We obtain 
\begin{align*}
\gpKer[\Naugment](\parMat)
&= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}[\funcVal[\Naugment] - \gpMeanPrior(\designIn[\Naugment])] \\
&= \gpKerPrior(\parMat) - \begin{pmatrix} \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix}^\top
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix} 
\begin{pmatrix}  \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix} \\
&= \gpKer[\Ndesign](\parMat) - 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} [\gpKerPrior(\designIn[\Nbatch], \parMat) - \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1}\gpKerPrior(\designIn[\Ndesign], \parMat)] \\
&= \gpKer[\Ndesign](\parMat) -  \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \gpKer[\Ndesign](\designIn[\Nbatch], \parMat) \\
&= \Cov[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]].
\end{align*}

\end{proof}

\subsubsection{Uncertainty in GP Predictive Mean Due to Unobserved Response}

\begin{proof} [Proof of \Cref{lemma:pred-mean-dist}]
Though the result is only required for a single input $\Par$, it is no more difficult to establish for a set of inputs $\parMat$. 
We recall that 
\begin{align*}
\gpMean[\Ndesign](\parMat | \designBatchFunc) 
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm(\designBatchIn) = \designBatchFunc] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} [\designBatchFunc - \gpMean[\Ndesign](\designBatchIn)],
\end{align*}
following from the GP predictive equations \ref{kriging_eqns}. Since, $\designBatchFunc|\parMat \sim \Gaussian(\gpMean[\Ndesign](\designBatchIn), \gpKer[\Ndesign](\designBatchIn))$, 
we see that $\gpMean[\Ndesign](\parMat | \designBatchFunc)$ is a linear function of a Gaussian random variable. It is thus Gaussian distributed, with mean and covariance
\begin{align*}
\E_{\designBatchFunc}[\gpMean[\Ndesign](\parMat | \designBatchFunc)]
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} [\gpMean[\Ndesign](\designBatchIn) - \gpMean[\Ndesign](\designBatchIn)] = \gpMean[\Ndesign](\parMat) \\
\Cov_{\designBatchFunc}[\gpMean[\Ndesign](\parMat | \designBatchFunc)]
&= \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1}  \gpKer[\Ndesign](\designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} \gpKer[\Ndesign](\designBatchIn, \parMat) \\
&=  \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} \gpKer[\Ndesign](\designBatchIn, \parMat) \\
&= \gpKer[\Ndesign](\parMat) - \gpKer[\Naugment](\parMat).
\end{align*}
\end{proof}

\subsubsection{Integrated Conditional Variance Calculations: Log-Likelihood Emulation}

\begin{proof} [Proof of \Cref{lemma:evar}]
We start by noting that 
\begin{align*}
\Var[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \Var[\priorDens(\Par) \Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \\
&= \priorDens(\Par)^2 \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik],
\end{align*}
so we will focus on the likelihood emulator, ignoring the prior for now. Since 
\begin{align*}
\Exp{\llikEm(\Par)} | [\llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \sim 
\LN(\emMean[\Ndesign]{\llik}(\Par| \designBatchLlik), \emKer[\Naugment]{\llik}(\Par)),
\end{align*}
we apply the formula for a log-normal variance to obtain 
\begin{align}
\Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \cst \Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)}, \label{formula_plug_in}
\end{align}
where 
\begin{align*}
\cst \Def \left[\Exp{\emKer[\Naugment]{\llik}(\Par)} -1 \right] \Exp{\emKer[\Naugment]{\llik}(\Par)}
\end{align*}
is not a function of the random variable $\designBatchLlik$. \Cref{lemma:pred-mean-dist} gives 
\begin{align*}
\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)
&\sim \LN(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)), 
\end{align*}
which implies 
\begin{align*}
\Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)}
&\sim \Gaussian(2\emMean[\Ndesign]{\llik}(\Par), 4[\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)]).
\end{align*}
Applying the formula for a log-normal mean thus yields 
\begin{align*}
\E_{\designBatchLlik} \left[\Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)} \right]
&= \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \Exp{2[\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)]} \\
&=  \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \varInflation(\Par; \designBatchIn), 
\end{align*}
where $\varInflation(\Par; \designBatchIn)$ is defined in \ref{var_inflation_factor}. Plugging this expression back 
into \ref{formula_plug_in} gives 
\begin{align*}
\E_{\designBatchLlik} \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \cst \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \varInflation(\Par; \designBatchIn) \\
&= \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \emMean[\Ndesign]{\llik}(\Par)] \varInflation(\Par; \designBatchIn).
\end{align*}
Multiplying both sides by $\priorDens(\Par)$ completes the proof, with the closed-form expression for the first term following 
immediately from the formula for a log-normal variance. 
\end{proof}

\subsubsection{Integrated Conditional Variance Calculations: Forward Model Emulation}

\begin{proof} [Proof of \Cref{prop:evar-fwd-emulation}]
We begin by noting that the squared prior density can be pulled out of the expectation as
\begin{align*}
\E_{\designBatchFwd} \Var[\postDens(\Par; \fwdEmCond{\designBatchFwd}) | \designBatchFwd]
&= \priorDens^2(\Par) \ \E_{\designBatchFwd}  \Var[\Gaussian\left(\obs |  \fwdEmCond{\designBatchFwd}(\Par), \likPar \right) | \designBatchFwd]. 
\end{align*}
The variance on the righthand side can be expanded as 
\begin{align}
\Var[\Gaussian\left(\obs |  \fwdEmCond{\designBatchFwd}(\Par), \likPar \right) | \designBatchFwd]
&= \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par) \right)}{\det(2\pi \likPar)^{1/2}}
- \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right]  \right)}{\det(2\pi [\likPar + \emKer[\Naugment]{\fwd}(\Par)])^{1/2}}
\end{align}
following from an application of \Cref{prop:fwd-em-Gaussian}. The denominators in the above expression can be 
pulled out of the outer expectation and are seen to equal the denominators in the desired expression. We thus complete the proof by 
computing the expectation of the numerators with respect to 
$\designBatchFwd | \designBatchIn \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\designBatchIn), \emKer[\Ndesign]{\fwd}(\designBatchIn))$. 
These expectations can be computed by noting \Cref{lemma:pred-mean-dist} and then applying \Cref{prop:Gaussian_marginal_moments}.
This yields,
\begin{align*}
\E_{\designBatchFwd}\left[\Gaussian\left(\obs \bigg| \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par) \right)\right]
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), 
\left[\frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par)\right] + \left[\emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)\right] \right) \\
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par) \right)
\end{align*}
and
\begin{align*}
\E_{\designBatchFwd}\left[\Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right]\right)\right] 
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2} \left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right] + \left[\emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)\right] \right) \\
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \left[\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right] - \frac{1}{2} \emKer[\Naugment]{\fwd}(\Par)\right),
\end{align*}
which completes the derivation of \Cref{fwd_evar1}. To obtain \Cref{fwd_evar2} we rearrange the covariances of the above expressions to obtain 
\begin{align*}
\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par) &= 
\left[\likPar +  \emKer[\Ndesign]{\fwd}(\Par) \right] - \frac{1}{2}\likPar 
= \CovComb(\Par) - \frac{1}{2}\likPar \\
\left[\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right] - \frac{1}{2} \emKer[\Naugment]{\fwd}(\Par)
&= \left[\likPar + \emKer[\Ndesign]{\fwd}(\Par) \right] - \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par) \right] 
= \CovComb(\Par) - \frac{1}{2} \CovComb[\Naugment](\Par). 
\end{align*}

\end{proof}


% Questions and TODOs
\section{Questions and TODOs}
\subsection{Questions}
\begin{enumerate}
\item How to reliably use a log-normal emulator for Bayesian inference? 
	\begin{enumerate}
	\item Improve GP calibration (e.g., quadratic mean) 
	\item Use more robust statistics (e.g., interquartile range)
	\item Truncate proposal or prior. 
	\end{enumerate}
\item How to deal with highly concentrated/correlated posteriors? 
	\begin{enumerate}
	\item MALA or other samplers. 
	\end{enumerate}
\end{enumerate}

\subsection{Need to add}
\begin{enumerate}
\item Summary of results from noisy MCMC literature.
\item Try working out results that compare the ratio of the approx density at two points across different approximations; 
alternatively could consider deriving these results for the normalized densities. 
\item Include result that sample and marginal approx agree at the design points.
\item Include existence results (check existence result from that new paper)
\item Viewing noisy MCMC approaches as approximations to the sample-based posterior.
\item Different views of noisy MCMC approaches, including extending the state space. How does this alg compare to the 
marginal and mcwmh-ind algs?
\item Add some sort of theoretical result that demonstrates that the marginal approximation is extremely sensitive to the GP 
variance. Based on numerical experiments, seems like this result should be given with respect to the dynamic range of the 
log-likelihood. Also of course depends on how fast the GP variance grows away from the design points, so perhaps should 
consider fill distance or something like this as well.
\item Numerical experiment that considers the different ways to weight the integrated uncertainty criteria (i.e., targeting 
the unnormalized posterior density vs. using the approx posterior samples as weights).
\item Posterior consistency results for the noisy MCMC emulators; combine the noisy MCMC results with GP approximation 
results.
\item Evaluating calibration of GP-approximated posteriors relative to calibration of the underlying GP emulator.
\item Constrained GPs
\item Pathwise sampling approach to approximate the sample-based approximation.
\item Compare noisy MCMC vs. deterministic version that considers integrating over the acceptance prob. 
\item Analyze effect of incorporating GP covariance structure; does it result in posteriors closer to the sample-based posterior? 
\item Compare marginal and sample-based approx.
\item Compare marginal approx in log-likelihood vs. forward model setting. 
\item Analyze distribution of likelihood under forward model emulation [I think this is the exponential of a folded Gaussian random variable]. 
How does its tail compare to the lognormal tail? 
\end{enumerate}

\subsection{Emulator Ideas}
\begin{enumerate}
\item Sum of quadratic kernel, Gaussian kernel, and some sort of flat/linear kernel (i.e. something with very long lengthscales) to capture the 
part of the response surface that "flattens" out. 
\end{enumerate}

\subsection{Notation}
\begin{enumerate}
\item How to clean up notation for all of the different approximations being considered here? 
\end{enumerate}

\subsection{Numerical experiments:}
My plan is to have emulation in dynamical settings (ODEs) as the unifying theme here. VSEM can provide the core example but could also consider 
others, such as Lorenz-63 (see Hwanwoo Kim, Daniel Sanz-Alonso paper for the ODEs they consider). When introducing the dynamical setting,
cite Stuart/Schneider Earth System modeling 2.0 paper. Provide various examples of observation operators: time-averages (moments) of state 
variables, identity operator, many shorter time-averages (e.g., weekly/monthly averages), multi-objective settings of calibrating to multiple state 
variables. I should probably include Lorenz-63 to have a more familiar example to many audiences. 

\begin{enumerate}
\item 1D example with 1D output for basic illustration. 
	\begin{enumerate}
	\item VSEM with single varied parameter. 
	\item For 1D output, consider long time average of of a single state variable. This will allow us to compare forward model and log-likelihood emulation directly.
	\item Gaussian likelihood. 
	\item Compare emulator distributions, log-likelihood emulator distributions, and likelihood emulator distributions, and various posterior approximations. 
	\item Validation metrics: RMSE, MAE, CRPS, Log-Score, Coverage. 
	\end{enumerate}
\item 2D examples for basic illustration and consideration of different posterior characteristics: Gaussian, banana, unidentifiable, concentration level, bimodal. 
\item Extension of 2D densities to 6-10 dims (see Vehtari paper numerical experiments) 
\item Sequential design performance at different levels of posterior concentration. 
\item Integrated uncertainty criterion: incorporating the current posterior via the integrand or the measure?
\item Large batch, few iteration sequential design experiment. 
(4 different combinations we could consider) 
\end{enumerate}

\begin{enumerate}
\item 1D example for basic illustration. 
\item 2D examples for basic illustration and consideration of different posterior characteristics: Gaussian, banana, unidentifiable, concentration level, bimodal. 
\item Extension of 2D densities to 6-10 dims (see Vehtari paper numerical experiments) 
\item Sequential design performance at different levels of posterior concentration. 
\item Integrated uncertainty criterion: incorporating the current posterior via the integrand or the measure?
\item Large batch, few iteration sequential design experiment. 
(4 different combinations we could consider) 
\end{enumerate}

\subsection{Potential examples:}
\begin{enumerate}
\item Banana, unimodal, bimodal, unidentifiable
\item Heat equation (see Sinsbeck and Nowak) 
\item VSEM
\end{enumerate}

\subsection{Things to consider trying}
\begin{enumerate}
\item Perhaps give some context by discussing connections to Bayesian optimization and to log-likelihood approximation used in 
simulation-based inference. 
\item Hyperparameter marginalization: need to look into opportunities for closed-form hyperparameter marginalization during 
sequential design phase. 
\item Developing a design criterion that better aligns with the MHWMC procedure; e.g., something that targets the likelihood ratio. 
\item Implementing Higdon basis function approach for comparison. 
\item Nonnegative constrained GPs (may be able to do this by modifying the kergp optimization code and using nloptr's option to add constraints) 
\item GP-accelerated MALA 
\end{enumerate}

\subsection{Limitations of existing literature}
\begin{enumerate}
\item Very little discussion of case where likelihood parameters are unknown. 
\item Lack of emphasis on batch design (with some exceptions).
\item Little guidance on which approximation/design criterion to choose.
\item Vehtari fixes the marginal approx, and focuses instead on varying the design criterion, but notes that sampling from the marginal approx is problematic. 
\end{enumerate}

\subsection{Conjectures}
\begin{enumerate}
\item The MCWMH algorithms will perform better than sampling from the marginal approx in the log-likelihood emulation setting, especially when the GP is very uncertain. 
\end{enumerate}

\subsection{Consideration}
\begin{enumerate}
\item Literature typically focuses on convergence of the approx posterior. But in cases with very expensive computer models, one might have to stop pre-convergence. 
In these settings the comparison between the approximate posteriors becomes even more important. 
\end{enumerate}



\bibliography{prob_surrogates_bayes} 
% \bibliographystyle{ieeetr}

\end{document}







