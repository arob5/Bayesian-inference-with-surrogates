\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{caption, subcaption} % Captions and sub-figures. 
\usepackage{fancyvrb} % For writing using verbatim font inline. 
\usepackage{float}
% \usepackage[demo]{graphicx}

% Bibliography
\bibliographystyle{plainnat}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Tables. 
\usepackage{multirow}

% Color boxes.
\usepackage[most]{tcolorbox}

\newtcolorbox{examplebox}[1][Gaussian Setting]{
  colback=blue!5!white,     % background color
  colframe=blue!75!black,   % frame color
  fonttitle=\bfseries,      % bold title
  title={#1}, % default title
  boxrule=0.8pt,            % line thickness
  arc=4pt,                  % rounded corners
  left=6pt, right=6pt, top=6pt, bottom=6pt % padding
}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{{./../output/plots/}}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{rec}{Recommendation}

\crefname{prop}{Proposition}{Propositions}
\Crefname{prop}{Proposition}{Propositions}
\crefname{definition}{Definition}{Definitions}
\Crefname{definition}{Definition}{Definitions}
\crefname{lemma}{Lemma}{Lemmas}
\Crefname{lemma}{Lemma}{Lemmas}
\crefname{thm}{Theorem}{Theorems}
\Crefname{thm}{Theorem}{Theorems}
\crefname{corollary}{Corollary}{Corollaries}
\Crefname{corollary}{Corollary}{Corollaries}
\crefname{rec}{Recommendation}{Recommendations}
\Crefname{rec}{Recommendation}{Recommendations}

% Title and author
\title{Probabilistic Surrogates for Bayesian Inverse Problems: Posterior Approximation and Sequential Design}
\author{Andrew Roberts}

\begin{document}

\maketitle

% Introduction 
\section{Introduction}
Within the Bayesian paradigm, there is growing need for posterior approximation algorithms tailored to settings 
where evaluation of the (unnormalized) posterior density incurs significant computational expense. 
This situation commonly arises in the physical, biological, and engineering sciences, 
where likelihood evaluations require running a complex computer simulation; 
e.g., \citep{ESM_modeling_2pt0,yeastMatingSurrogate,FerEmulation,FATES_CES,CLMBayesianCalibration}.
Such computer models often derive from systems of differential equations, with the goal being to estimate (i.e., calibrate) 
unknown parameters characterizing the physical model.
To address this computational bottleneck, a wide body of research has emerged that seeks to characterize the Bayesian 
posterior distribution over model parameters while minimizing the number of required posterior density evaluations. 
A popular class of methods involves replacing either the computer model, log-likelihood function, or
log-posterior density with a statistical surrogate (also called an emulator or meta-model), 
which induces a computationally cheap approximation of the posterior density.
The surrogate can be trained offline using a small set of expensive model runs (often performed in 
parallel), and plugged in as an approximation to facilitate the application of 
standard inference schemes such as Markov chain Monte Carlo (MCMC) 
\citep{modularization,BurknerSurrogate,BurknerTwoStep}.

This generic surrogate-based Bayesian workflow can be instantiated in a variety of ways.
A key modeling decision is the choice of input-output map that will generate training data for
fitting the emulator; i.e., the quantity actually being emulated. A wide body of work seeks to
emulate the underlying computer model, thus targeting the source of the computational 
bottleneck. Surrogates have been devised for a wide variety of computer models, including
those with high-dimensional outputs \citep{HigdonBasis,RougierHighDim}, 
high-dimensional inputs \citep{RemoteSensingEmulator,ZhouHighDimInput}, 
dynamical structure \citep{GP_dynamic_emulation,Bayesian_emulation_dynamic}, and
inherent stochasticity \citep{stochasticComputerModels,VehtariParallelGP,FadikarAgentBased}. 
Alternatively, in the context of Bayesian inference, 
it has been noted that ultimately only an approximation of the posterior density is required to 
accelerate posterior estimation \citep{StuartTeck1,GP_PDE_priors}. Therefore, an alternative surrogate 
strategy consists of directly emulating the log-likelihood 
(e.g., \citet{VehtariParallelGP,FerEmulation,trainDynamics}) or 
(unnormalized) log-posterior density (e.g., \citet{emPostDens,gp_surrogates_random_exploration,Kandasamy_2017}). 
In this work, we adopt a generic viewpoint that encompasses both of 
these strategies, which we respectively refer to as \textit{forward model emulation}
and \textit{log-density emulation}. The two approaches each have various strengths
and drawbacks, and one may be favored over the other depending on the context.

In addition to choosing the target map for emulation, another key decision is the 
choice of surrogate model. Gaussian processes \citep{KOH,HigdonBasis,StuartTeck1,VehtariParallelGP}, 
polynomial chaos expansions
\citep{dimRedPolyChaos,PCEBIP,BurknerSurrogate}, and 
neural networks \citep{Lueckmann2018LikelihoodfreeIW,DagonCLM} have 
been extensively applied for this purpose. While many previous studies
on surrogate-based Bayesian inference tend to focus on a specific model 
(e.g., \citet{StuartTeck1,VehtariParallelGP}), recent work has emphasized generic 
probabilistic workflows \citep{BurknerSurrogate,garegnani2021NoisyMCMC,BurknerTwoStep}.
We adopt the latter perspective in this review, remaining agnostic to the particular
surrogate model while highlighting Gaussian processes as a special case in order to 
aid intuition.

While surrogates offer the potential for significant computational savings, 
they introduce a new source of uncertainty within the Bayesian model. 
To avoid incorrect or miscalibrated inference, it is necessary
to both propagate and--within computational constraints--reduce this uncertainty.
A growing body of work has addressed the latter by means of sequential design (i.e., active learning)
algorithms, which refine the posterior approximation by iteratively constructing the set of simulator runs used for
surrogate training \citep{SinsbeckNowak,Li_2014,VehtariParallelGP,Lueckmann2018LikelihoodfreeIW,Surer2023sequential,
KandasamyActiveLearning2015,VillaniAdaptiveGP,weightedIVAR,Semler_2023,Semler2024GradientEnhanced,
surrogateNoisyMCMC,hydrologicalModel,AlawiehIterativeGP}.
In many practical applications it is computationally infeasible to reduce the surrogate approximation error to 
an arbitrarily small level. Hence, recent work has stressed the importance of inferential workflows that 
propagate surrogate uncertainty so that downstream uncertainty estimates acknowledge the 
surrogate approximation
\citep{BurknerSurrogate,surrogateNoisyMCMC,CES,FerEmulation,hydrologicalModel}.
In order to propagate surrogate uncertainty, the emulator must include uncertainty
estimates in addition to point predictions. We therefore focus on \textit{probabilistic emulators}, 
which provide predictions in the form of a probability distribution. Gaussian processes naturally 
fall within this category, while other models can be converted into probabilistic emulators
via Bayesian treatments \citep{BayesianPCE1,BayesianPCE2} or ensemble methods \citep{deepEnsembles}. 

% Goals
\subsection{Goals}
This review is motivated by a rapidly increasing body of literature using probabilistic surrogate models
to accelerate Bayesian inference. Similar methodological work has been pursued across various disciplines, 
including Bayesian statistics, computational science and engineering, and applied mathematics, in 
addition to applied work spanning a range of scientific applications. Much of this research appears to 
be siloed within its respective discipline; one of our aims is to synthesize this related work in an effort to break down
these barriers. Our review centers on two common problems problems faced in most applications: (i.) how to
use a stochastic surrogate to approximate the Bayesian posterior, and (ii.) how to efficiently refine 
an existing surrogate.

Our second goal is to highlight different approaches to incorporating surrogate models within a Bayesian
workflow. We draw a distinction between two general strategies common in the literature:
\textit{forward model emulation}, whereby (some function of) the expensive computer model is approximated 
by an emulator, and \textit{log-density emulation}, 
where a surrogate approximates either the log-likelihood or unnormalized log-posterior density directly. 
The latter approach is a special case of the former, and both strategies ultimately induce an 
approximation of the posterior density. However, there are significant practical differences to consider
when deciding which method to use for a particular application.  
We discuss the relative merits and downsides of each approach, as well as typical use cases.

A third goal is both to synthesize the large subset of literature focusing on the use of GP emulators, 
while also highlighting generic workflows that do not require particular distributional assumptions.
Our general viewpoint allows surrogates to have generic predictive distributions, which may only
be accessible through samples. We treat the GP setting as a special case, and provide streamlined 
derivations and interpretations of quantities that are available in closed-form in this setting.

% Paper Organization
\subsection{Paper Organization}
\Cref{sec:background} begins with background on Bayesian inverse problems and Gaussian processes.
\Cref{sec:surrogate-models} provides an overview of probabilistic surrogates, highlighting distinctions
between parametric and nonparametric emulators and describing the techniques of forward model 
emulation and log-density emulation. \Cref{sec:post-approx} addresses the question of how to 
use a probabilistic surrogate to approximate the posterior distribution, with an emphasis on correctly
propagating surrogate uncertainty. \Cref{sec:seq-design} reviews active learning strategies for 
goal-oriented surrogate refinement. We offer a list of practical recommendations in \Cref{sec:recs},
which are emphasized in a numerical case study presented in \Cref{sec:case-study}.
In \Cref{sec:related-work} we highlight related work in computer model calibration, probabilistic numerics,
and multifidelity methods (\todo: this third one may change). We conclude in \Cref{sec:conclusion}. 
All derivations and proofs are provided in the appendix.

% Background
\section{Background} \label{sec:background}
In this section we introduce the Bayesian inference setting of interest, with an emphasis on Bayesian inverse problems
stemming from mechanistic dynamical models. We provide a concrete example of parameter estimation for a
system of ordinary differential equations, and conclude the section with a brief overview of Gaussian processes.
 
 \subsection{Bayesian Inverse Problems}
We focus on the Bayesian inference setting in which pointwise evaluations of the unnormalized posterior
density 
\begin{equation}
\postDens(\Par) \Def \priorDens(\Par) p(\obs \given \Par) \label{post_dens_generic}
\end{equation}
are available, but expensive owing to the cost of computing the likelihood $p(\obs \given \Par)$. This setting 
commonly arises in the Bayesian approach to inverse problems \citep{Stuart_BIP}. 
Consider a \textit{forward model} $\fwd: \parSpace \subseteq \R^{\dimPar} \to \R^{\dimObs}$ describing 
some system of interest, parameterized by input parameters $\Par \in \parSpace$. We assume throughout
that $\fwd$ is a deterministic function, but extensions to the stochastic case are discussed in \Cref{sec:sbi}.
In addition, suppose that
we have noisy observations $\obs \in \obsSpace \subseteq \R^{\dimObs}$ of the output signal that $\fwd(\Par)$ 
seeks to approximate. The \textit{inverse problem} concerns learning the parameter values $\Par$ such
 that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating} the model so that it agrees with the observations. 
 The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on $\obs \given \Par$. We assume that this distribution 
admits a density with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs \given \Par) = \Exp{\llik(\Par)}$. The notation $\llik(\Par)$ suppresses the dependence 
on $\obs$, as the observed data will be fixed throughout. We start by focusing inference only on the 
calibration parameter $\Par$, assuming that other likelihood parameters (e.g., noise covariance) 
are fixed.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution
\begin{align}
\postDensNorm(\Par) \Def p(\Par \given \obs) = \frac{1}{\normCst}\postDens(\Par) = \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. We will also find it useful to introduce the notation
\begin{equation}
\lpost(\Par) \Def \log \postDens(\Par) = \log \priorDens(\Par) + \llik(\Par) \label{eq:lpost}
\end{equation}
for the logarithm of the unnormalized posterior density.
When relevant, we will make explicit the dependence on the forward 
model by writing $\llik(\Par; \fwd)$, $\lpost(\Par; \fwd)$, and $\postDens(\Par; \fwd)$. 
In addition to considering
generic likelihoods, we give special attention to the additive Gaussian noise model
\begin{align}
&\obs = \fwd(\Par) + \noise,
&&\noise \sim \Gaussian(0, \likPar) \label{inv_prob_Gaussian} 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par; \fwd) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) \\
&= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)), \label{llik_Gaussian}
\end{align}
as this model features prominently in the inverse problems literature.
In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are serial in nature, often requiring $\sim 10^5 - 10^7$ iterations, with each 
iteration involving an evaluation of $\lpost(\Par; \fwd)$. 
In the inverse problem context, this requirement is prohibitive when the forward model 
evaluations $\fwd(\Par)$ incur significant computational cost. Motivated by this problem, a large body 
of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\lpost(\Par)$ (note that approximating the former induces 
an approximation of the latter). We focus on surrogates that take the form of a statistical 
regression model approximating the map $\Par \mapsto \fwd(\Par)$ or $\Par \mapsto \lpost(\Par)$.
\footnote{This is in contrast to other surrogate modeling strategies (e.g., reduced-order modeling)
that exploit the specific structure of the forward model; see \citet{multifidelityReview} for a detailed review.}
We refer to methods that explicitly model the former map as \textit{forward model emulation}, and those that 
model the latter as \textit{log-density emulation}. We also include methods that emulate the log-likelihood
map $\Par \mapsto \llik(\Par)$ in this latter category (see \Cref{sec:llik_vs_lpost}).
Throughout this review, we will typically view the functions
$\fwd(\Par)$, $\llik(\Par)$, and $\lpost(\Par)$ as computationally expensive black-boxes. In applications,
the forward model often takes the form of a costly computer simulation, so we will occasionally refer to 
queries to these functions as simulator evaluations.
The following section provides a concrete example of such a simulation
model stemming from the numerical solution of a system of differential equations. 
 
\subsection{Motivating Example: Parameter Estimation for ODEs} \label{dynamical_models}
The problem of estimating the parameters of differential equation models 
 can be cast within the Bayesian inverse problem framework. Our primary motivating applications 
 stem from the use of dynamical models in Earth system modeling  \citep{ESM_modeling_2pt0,paramLSM}.
Consider a parameter-dependent initial value problem 
\begin{align}
\frac{d}{d\Time} \state(\Time, \Par) &= \odeRHS(\state(\Time, \Par), \Par), &&x(\timeStart) = \stateIC, \label{ode_ivp}
\end{align}
describing the time evolution of $\dimState$ state variables 
$\state(\Time, \Par) \Def \left[\indexState{\state}(\Time, \Par)\right]_{\stateIndex=1}^{\dimState}$
with dynamics depending on $\Par \in \parSpace$. As our focus will be on estimating these parameters 
from observations, we consider the parameter-to-state map
\begin{align}
\Par &\mapsto \left\{\state(\Time, \Par) :  \Time \in [\timeStart, \timeEnd] \right\}.
\end{align}
In practice, the solution is typically approximated via a numerical discretization of the form 
\begin{align}
\solutionOp: \Par &\mapsto \left[\indexTime[0]{\state}(\Par), \dots, \indexTime[\NTimeStep]{\state}(\Par) \right]^\top. \label{eq:ode-solution-op}
\end{align}
Here, $\solutionOp: \parSpace \to \R^{\NTimeStep \times \dimState}$ represents the map induced by a numerical solver, 
and $\indexTime[0]{\state}(\Par), \dots, \indexTime[\NTimeStep]{\state}(\Par)$ are approximations of the state 
values $\state(\Time, \Par)$ at a finite set of time points in $[\timeStart, \timeEnd]$. 
Going forward, we focus on the discrete-time operator $\solutionOp$, neglecting discretization error
for simplicity. Finally, suppose we have observed data $\obs \in \obsSpace \subseteq \R^{\dimObs}$ that we model as a
noise-corrupted function of the state trajectory. This is formalized by the definition of an observation operator 
$\obsOp: \R^{\NTimeStep \times \dimState} \to \obsSpace$ mapping from the state trajectory to a 
$\dimObs$-dimensional observable quantity. We assume the observations are generated as 
\begin{align}
&\obs = (\obsOp \circ \solutionOp)(\ParTrue) + \noise, 
&&\noise \sim \Gaussian(0, \likPar) \label{ode_inv_prob} 
\end{align}
for some ``true'' parameter value $\ParTrue \in \parSpace$. Observe that this falls within the generic 
inverse problem framework with forward model $\fwd \Def \obsOp \circ \solutionOp$. 
Generalizations can be considered to settings with non-additive or non-Gaussian noise, or the 
presence of model discrepancy. \Cref{sec:case-study} presents a numerical case study using 
a problem of this form, motivated by applications to near-term forecasting of the 
terrestrial carbon cycle.

 \subsection{Gaussian Processes} \label{gp_review}
 Since Gaussian processes will serve as our primary example of a probabilistic surrogate,
 we provide a brief overview here; for in-depth treatments, we refer to 
 \cite{gramacy2020surrogates, StuartTeck2, gpML}. We also discuss basic extensions
 to Gaussian process models for multi-output functions, which will be relevant in various examples
we discuss later.
 
 \subsubsection{Single Output Gaussian Processes}
 A Gaussian process (GP) can be thought of as a
random function $\funcPrior: \parSpace \to \R$, defined by the property that 
the random vector $[\funcPrior(\Par_1), \dots, \funcPrior(\Par_\Ndesign)]^\top$ has
a joint Gaussian distribution for any finite set of inputs $\parMat \Def \{\Par_1, \dots, \Par_\Ndesign\} \subset \parSpace$. 
A GP is defined by its mean function $\gpMeanPrior: \parSpace \to \R$ and positive-definite kernel
(i.e., covariance function) $\gpKerPrior: \parSpace \times \parSpace \to \R$ and is thus denoted by 
$\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$. Throughout this paper, we will vectorize notation as follows; letting 
$\tilde{\parMat} \Def [\tilde{\Par}_1, \dots, \tilde{\Par}_M]  \subset \parSpace$ be a second set of inputs, we define 
$\funcPrior(\parMat) \Def [\funcPrior(\Par_1), \dots, \funcPrior(\Par_\Ndesign)]^\top \in \R^{\Ndesign}$, 
$\gpKerPrior(\parMat, \tilde{\parMat}) \Def \{\gpKerPrior(\Par_{\idxDesign}, \tilde{\Par}_m)\}_{\substack{1 \leq \idxDesign \leq \Ndesign \\ 1 \leq m \leq M}} \in \R^{\Ndesign \times M}$, 
and $\gpKerPrior(\parMat) \Def \gpKerPrior(\parMat, \parMat) \in \R^{\Ndesign \times \Ndesign}$. 
We similarly use this vectorized notation for other functions; e.g., $\gpMeanPrior(\parMat)$, $\fwd(\parMat)$, and $\llik(\parMat)$.
Given this notation, 
the defining property of the GP can be written as 
\begin{align}
\funcPrior(\parMat) &\sim \Gaussian(\gpMeanPrior(\parMat), \gpKerPrior(\parMat)).
\end{align}
For our purposes, the GP $\funcPrior$ will represent a prior distribution for a \textit{deterministic} function 
$\func: \parSpace \to \R$, such that the randomness in $\funcPrior$ encodes our epistemic 
uncertainty about $\func$ \citep{epistemicAleatoric}.
The underlying ideas explored here can also be extended to the setting where evaluations of $\func$ 
are corrupted by noise \citep{stochasticComputerModels,VehtariParallelGP,OakleyllikEm}. 
If we observe noiseless function evaluations $\func(\designIn)$ at a set of inputs $\designIn$, then we can consider
the conditional distribution $\funcPrior|[\funcPrior(\designIn)=\func(\designIn)]$. It is well-known
\citep{gpML} that the conditional is also a GP
\begin{align}
\funcEm[\Ndesign] \Def \funcPrior|[\funcPrior(\designIn)=\func(\designIn)]\sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign]), \label{generic_gp_conditional}
\end{align}
with mean and kernel given by
\begin{align}
\gpMean[\Ndesign](\parMat) &= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} [\func(\designIn) - \gpMeanPrior(\designIn)] \label{kriging_eqns} \\
\gpKer[\Ndesign](\parMat) &= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn) \gpKerPrior(\designIn)^{-1} \gpKerPrior(\designIn, \parMat). \nonumber
\end{align}

We will refer to observed input locations $\designIn$ as \textit{design points}, and the conditional
distribution $\GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$ as the \textit{predictive distribution}.
The predictive mean can be seen to interpolate the design points (i.e., $\gpMean[\Ndesign](\designIn) = \func(\designIn[\Ndesign])$), with 
the predictive variance vanishing at these inputs (i.e., $\gpKer[\Ndesign](\designIn) = 0$). 
To break these interpolation properties,
a constant can be added to the diagonal of the kernel matrix so that $(\gpKerPrior(\designIn) + \nuggetSD^2 I)^{-1}$ replaces 
$\gpKerPrior(\designIn)^{-1}$ in \cref{kriging_eqns}. In the present context of deterministic forward models, 
this interpolation property is typically desirable. Nevertheless, a small, positive fixed constant $\nuggetSD^2$ is still 
typically included for numerical stability. 

Appropriate choices for the prior mean $\gpMeanPrior$ and covariance $\gpKerPrior$ depend on the 
particular application, with common choices for the mean including a constant or a linear model of the form 
$\gpMeanPrior(\Par) = \beta^\top \phi(\Par)$, for some feature map $\phi$. A typical default choice of 
covariance function is the exponentiated quadratic (also called squared exponential, Gaussian, or radial basis function)
kernel,
\begin{align}
\gpKerPrior(\Par, \tilde{\Par}) &= 
\margSD^2 \Exp{- \sum_{\idxParDim=1}^{\dimPar} \left(\frac{\Par_{\idxParDim} - \tilde{\Par}_{\idxParDim}}{\lengthscale_{\idxParDim}} \right)^2}, \label{Gaussian_kernel}
\end{align}
defined by the choice of positive hyperparameters $\margSD, \lengthscale_{1}, \dots, \lengthscale_{\dimPar}$. 
This is an example of a stationary (i.e., shift-invariant) kernel since $\gpKerPrior(\Par, \tilde{\Par})$ only depends 
on its arguments through their componentwise difference $\Par - \tilde{\Par}$. 
A common approach to set the mean and kernel hyperparameters is the empirical Bayes method of fixing
their values at their maximum (marginal) likelihood estimates, which conveniently retains the Gaussian
predictive distribution in \cref{generic_gp_conditional} at the expense of failing to acknowledge uncertainty
in the hyperparameters. The fully Bayesian alternative significantly increases computational cost, 
and results in a non-Gaussian predictive distribution after marginalizing the hyperparameters.
Unless otherwise indicated, we assume the mean and kernel hyperparameters are
fixed, so that the GP predictive distribution remains Gaussian.

\subsubsection{Multiple Output Gaussian Processes}
We now consider the extension to multi-output functions $\funcPrior: \parSpace \to \R^{\dimObs}$. 
There is a large literature on multi-output kernels \citep{multiOutputKernels}, but we focus here on the 
simplest case in which each output is modeled separately as an independent GP. 
In this setting, the notation $\gpKerPrior(\Par)$ is now interpreted as the $\dimObs \times \dimObs$ 
diagonal matrix collecting the variance of each independent GP. Similarly, for a set of inputs 
$\parMat \in \R^{M \times \dimPar}$, $\gpKerPrior(\parMat)$ is now the 
$M\dimObs \times M\dimObs$ block diagonal matrix constructed from the covariances of 
each independent GP.
 
This basic multi-output strategy becomes problematic when the output dimension $\dimObs$ 
is large or the outputs are highly interdependent, rendering the independent GP approach infeasible or ill-advised.
A popular alternative is to approximate the function output as a linear combination of a small number
of basis vectors, with the coefficients modeled as independent GPs; that is, 
\begin{align}
\funcPrior(\Par) &= \sum_{\idxBasis=1}^{\dimBasis} \basisWeight_{\idxBasis}(\Par) \basisVec_{\idxBasis} + \noise_{\basisVec} \label{basis_func_model} \\
\basisWeight_{\idxBasis} &\overset{\textrm{ind}}{\sim} \GP(\emMean[0]{(\idxBasis)}, \emKer[0]{(\idxBasis)}), \label{basis_func_GPs} \\
 \noise_{\basisVec} &\sim \Gaussian(0, \sigma^2_{\basisVec}), \label{basis_func_noise}
\end{align}
where the $\basisVec_{\idxBasis}$ are fixed basis vectors.
These basis vectors are commonly constructed via a singular value decomposition, though other 
bases are also possible \citep{HigdonBasis, emulate_functional_output, functionValuedModels, PODemulation}.
Under this model, the univariate GPs $\basisWeight_{\idxBasis}$ are fit and conditioned independently, then 
plugged back into \cref{basis_func_model} to construct the predictive distribution. The linear Gaussian 
structure of \cref{basis_func_model} implies that this approach retains a Gaussian predictive distribution
for $\funcEm[\Ndesign]$, conditional on the hyperparameters.

% Initial Design and Surrogate Models
\section{Initial Design and Surrogate Models} \label{sec:surrogate-models}
We begin our review by summarizing the surrogate modeling workflow in the context of solving
Bayesian inverse problems. We highlight commonly used surrogates, emphasizing
models that provide a probabilistic representation of emulator uncertainty in addition to 
standard point predictions. We conclude the section with discussions of the tradeoffs
between the forward and log-density emulation strategies. Our aim is not to investigate any specific 
model in depth; see \Cref{sec:recs,sec:case-study} for practical recommendations.

\subsection{Initial Design}
Before discussing specific emulators, we comment on the general process of surrogate fitting.
We focus exclusively on a modular workflow \citep{modularization}, such that the surrogate is
 trained only on data produced
by the simulator, as opposed to jointly learning the surrogate with the parameters in a 
unified Bayesian model (e.g., as in the seminal work \citet{KOH}). Fitting the surrogate requires 
generating training data by evaluating the simulator at a chosen set of input parameters
$\designIn \Def \{\Par_1, \dots, \Par_{\Ndesign}\}$. We refer to $\designIn$ as the \textit{initial design}.
Depending on which map is being emulated, the training data for the emulator will take the form 
$\{\designIn, \fwd(\designIn)\}$, $\{\designIn, \llik(\designIn)\}$, or $\{\designIn, \lpost(\designIn)\}$. 
In the computer experiments literature, 
the initial design is typically selected to satisfy some sort of ``space-filling'' criterion; canonical examples 
include latin hypercube designs and Sobol sequences \citep{initDesignReview}. In the Bayesian inference setting, 
a natural approach is to sample $\designIn$ from the prior $\priorDens$, either via simple Monte Carlo or 
deterministic alternatives \citep{supportPoints, SteinPoints}.
As we will demonstrate in experiments, fitting emulators on prior designs may simultaneously produce 
reasonable approximations in a prior-averaged sense but poor approximations in a posterior-averaged
sense. This is due to a common phenomenon in Bayesian inverse problems, whereby the posterior is highly 
concentrated relative to the prior \citep{StuartTeck2,Li_2014,PCEBIP}. Even in parameter spaces of moderate dimension, 
it is common that the initial design may contain few or no points in the region of high posterior mass. 
This is one of the central challenges of surrogate modeling in this setting, and motivates the use of sequential 
design algorithms to iteratively construct the design in order to target high-posterior regions (see \Cref{sec:seq-design}).

Another approach to this problem is to first run an alternative algorithm (e.g., an approximate 
sampler or optimizer) to produce design points in regions of high posterior mass, which can then 
be used as part of an initial design to fit an emulator. This general strategy is exemplified by the 
\textit{calibrate, emulate, sample} workflow \citep{CES,idealizedGCM,CESSoftware,FATES_CES,adaptiveMultimodal}, 
which uses Ensemble Kalman methods to produce the initial design. In similar spirit, the earlier work 
\citet{emPostDens} utilizes a sequence of importance sampling steps to construct a Gaussian 
approximation of the posterior, which is then sampled from to produce an initial surrogate design.
\citet{JosephMEDSampling} proposes the use of deterministic sampling methods to generate 
the initial design. While we do not focus on these methods further in this review, continued 
methodological development investigating such workflows may prove a fruitful avenue for future research. 

\subsection{Probabilistic Surrogates}
While any regression model may be used as an emulator, those that produce only point 
predictions do not provide a direct means of propagating surrogate uncertainty. We thus
focus on probabilistic (i.e., random/stochastic) surrogates, which are emulators that 
provide a probabilistic description of approximation uncertainty. 

\subsubsection{Common Models}
Though the notion of a probabilistic surrogate is quite broad, we highlight a few common
model choices.

\paragraph{Gaussian Processes.}
Gaussian processes (GPs) have emerged as a canonical surrogate model, and provide the 
basis for examples given throughout this review (refer to \Cref{gp_review} for GP
background and notation). GPs have been extensively applied in a wide variety of 
surrogate modeling tasks, including as emulators for solving Bayesian inverse problems.
The Gaussian predictive distribution of a 
GP facilitates many convenient closed-form computations, which we will derive and present 
throughout this review. Extensions of the GP methodology can produce more flexible 
emulators with non-Gaussian predictive distributions. For example, fully Bayesian 
GPs \citep{fullyBayesianGPs} and deep GPs \citep{deepGPVecchia,deepGPAL}
typically yield predictions in the form of (infinite) mixtures of Gaussians.

\paragraph{Polynomials.}
Surrogates that consist of linear combinations of polynomial basis functions are 
commonly used in the engineering and applied math communities. Polynomial 
chaos expansions (PCE; \citet[Chapter 9]{UQpredCompSci}) are a particular example whereby a polynomial 
basis is used to approximate a random variable $\func(\Par)$ as a function of
a random input $\Par \sim \priorDens$. In particular, the expansion is of the form
\begin{equation}
\funcEm[\dimBasis](\Par) = \sum_{\idxBasis=1}^{\dimBasis} c_{\idxBasis} \basisVec_{\idxBasis}(\Par),
\end{equation}
where $\basisVec_{1}, \dots, \basisVec_{\dimBasis}$ are orthogonal polynomials with respect 
to the distribution $\priorDens$ on the inputs. Once constructed, a PCE is often used to 
approximate statistical moments of $\func(\Par)$. More relevant to our context,
the map $\funcEm[\dimBasis](\Par)$ can also be used as a surrogate for $\func(\Par)$.
With fixed coefficients $c_{\idxBasis}$, this map is deterministic and thus PCEs do not fall
within our definition of probabilistic surrogates. We nonetheless highlight them here
due to their popularity, the fact that they can be converted into random surrogates by 
considering Bayesian treatments of the coefficients 
\citep{BayesianPCE1,BayesianPCE2,BurknerSurrogate},
and their use in conjunction with probabilistic surrogates (e.g., as the mean function
of a GP; \citet{PCEGPWind,PCEGP2,SinsbeckNowak}). PCE surrogates have been
employed to accelerate Bayesian inversion in various applications 
\citep{dimRedPolyChaos,BurknerSurrogate,PCEBIP}.

\paragraph{Neural Networks.}
There has been growing interest in the use of neural networks as surrogate models, 
which are well-suited to the regime where both the input dimension $\dimPar$ 
and computational budget $\Ndesign$ are large. Bayesian treatment of neural 
network parameters provides probabilistic predictions, but in practice significant 
approximations are required for inference \citep{BayesOptNN}. One popular simplification
treats only the final neural network layer in a Bayesian fashion 
\citep{BayesLastLayer,BayesOptBayesLastLayer}. Motivated by these difficulties,
there has also been growing interest in surrogate models with predictive distributions 
not necessarily rooted in the Bayesian philosophy. This includes deep ensembles, 
which summarize uncertainty via an ensemble of neural network 
models \citep{deepEnsembles,Lueckmann2018LikelihoodfreeIW},
and epistemic neural networks \citep{epistemicNN,BayesOptEpistemicNN}. 

\subsubsection{Parametric vs. Nonparametric Models}
Much of the existing literature on surrogate-based Bayesian inference tends to 
be geared towards a particular emulator model (e.g., GPs or PCEs). Posterior 
approximation or active learning algorithms proposed in the context of one 
model may not easily generalize to another. A key distinction affecting
the generality of an algorithm is whether it can be implemented for 
a nonparametric emulator (e.g., a GP), or if it confined to the parametric 
setting (e.g., PCEs and neural networks). For example, the papers 
\citep{BurknerSurrogate,garegnani2021NoisyMCMC} suggest algorithms
that were deemed impractical in \citep{VehtariParallelGP,StuartTeck1}, which can
be explained by the fact that the latter authors are working with GPs, while the former
implicitly assume parametric emulators (see \Cref{sec:post-approx} for details).
We define our use of the terms
\textit{parametric} and \textit{nonparametric} for the purpose of this article 
below. Let $\funcEm[\Ndesign]$ denote a generic probabilistic surrogate 
model for some function $\func$.

\paragraph{Parametric Model.}
We refer to $\funcEm[\Ndesign]$ as \textit{parametric}
or \textit{finite-dimensional} if the randomness in $\funcEm[\Ndesign]$ stems from 
a finite-dimensional parameter $\theta_{\Ndesign}$; i.e., 
$\funcEm[\Ndesign](\cdot) = g(\cdot; \theta_{\Ndesign})$ for some 
random vector $\theta_{\Ndesign}$ and non-random function $g$.
In this case, it is feasible to sample trajectories (sample paths) of 
$\funcEm[\Ndesign]$ via 
\begin{align}
&\func_{\mathrm{samp}}(\cdot) \Def g(\cdot, \theta_{\mathrm{samp}}),
&&\theta_{\mathrm{samp}} \sim \mathrm{law}(\theta_{\Ndesign}).
\end{align}
The sampled function $\func_{\mathrm{samp}}(\cdot)$ can now 
be evaluated at any input value. A standard example is a linear 
regression model of the form
\begin{align}
\funcEm[\Ndesign](\cdot) &= \sum_{\idxBasis=1}^{\dimBasis} (\theta_{\Ndesign})_{\idxBasis} \basisVec_{\idxBasis}(\cdot)
\label{eq:finite-basis}
\end{align}
for some fixed basis vectors $\basisVec_{1}, \dots, \basisVec_{\dimBasis}$.
 
\paragraph{Nonparametric Model.} For a nonparametric model, no such finite-dimensional 
parameter $\theta_{\Ndesign}$ exists. In this setting, a generic view of a 
surrogate model is as a map from finite sets of inputs $\parMat$ to a predictive distribution 
over the associated responses $\func(\parMat)$. This viewpoint has formed the basis 
for the generic implementation of surrogate models in software packages such as 
\verb+BoTorch+ \citep{botorch}. It is therefore feasible to sample the finite-dimensional distributions
$\funcEm[\Ndesign](\parMat)$ for finite input sets $\parMat$, but sampling trajectories
$\func_{\mathrm{samp}}(\cdot) \sim \mathrm{law}(\funcEm[\Ndesign])$ would require 
infinite computing resources. Practical techniques to sample approximate trajectories have been
extensively studied when $\funcEm[\Ndesign]$ is a GP, including work in 
Bayesian inverse problems \citep{dimRedPolyChaos,functionSpaceMCMC} 
and Bayesian optimization \citep{pathwiseConditioning,samplingGPPosts}.
Common approaches construct finite-rank approximations of the form  
in \Cref{eq:finite-basis}.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth} % GP dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/fwd_dist_fwdem.png}}
         \caption{$\fwdEm[\Ndesign]$}
         \label{fig:fwd_dist_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % llik dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/llik_dist_fwdem.png}}
         \caption{$\llik(\cdot; \fwdEm[\Ndesign])$}
         \label{fig:llik_dist_fwd}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % lik dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lik_dist_fwdem.png}}
         \caption{$\Exp{\llik(\cdot; \fwdEm[\Ndesign])}$}
         \label{fig:lik_dist_fwdem}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth} % GP dist [llik] (equal to llik dist so excluding)
     \includegraphics[width=\textwidth]{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % llik dist [llik] 
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/llik_dist_llikem.png}}
         \caption{$\llikEm[\Ndesign]$}
         \label{fig:llik_dist_llikem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth} % lik dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lik_dist_llikem.png}}
         \caption{$\Exp{\llikEm[\Ndesign]}$}
         \label{fig:lik_dist_llikem}
     \end{subfigure}
        \caption{Comparison of forward model and log-likelihood emulation in the simple one-dimensional problem 
        ($\dimPar = \dimObs = 1$) described in $\todo$. The top and bottom rows correspond to the forward model 
        emulation and log-likelihood emulation settings, respectively. The columns correspond to (1) the forward model emulator;
        (2) the log-likelihood emulator induced by the underlying GP; 
        (3) the likelihood emulator induced by the underlying GP. When directly emulating the 
        log-likelihood, the underlying GP is itself the log-likelihood emulator, and there is no forward model emulator.
        The black line is the ground truth and the red points are the design points used for emulator training. 
        The blue line is the mean of the respective emulators, and the shaded area corresponds to 90\% credible 
        intervals for the predictive distributions.}
        \label{fig:em_dist_1d}
\end{figure}

\subsection{Surrogate Targets for Bayesian Inference}
A wide variety of strategies have been proposed to incorporate emulators within a Bayesian workflow.
These approaches tend to differ based upon the target quantity that is emulated.
We group these strategies into the broad categories of \textit{forward model emulation}
and \textit{log-density emulation}. This dichotomy is also explored in 
\citet{StuartTeck1,GP_PDE_priors,random_fwd_models}, and briefly noted in 
\citet{Surer2023sequential,trainDynamics,ActiveLearningMCMC,emPostDens}.

\subsubsection{Forward Model Emulation}
One natural approach is to fit an emulator approximating the forward model map $\Par \mapsto \fwd(\Par)$,
which entails fitting a predictive model to a design $\{\designIn, \fwd(\designIn)\}$. 
We let $\fwdEm[\Ndesign]$ denote a stochastic surrogate that has been fit to this design,
thus representing a distribution over functions that summarizes the uncertainty about the 
numerically-inaccessible function $\fwd$. Let $\emMean[\Ndesign]{\fwd}(\Par)$ and 
$\emKer[\Ndesign]{\fwd}(\Par)$ denote the mean and variance of the random 
variable $\fwdEm[\Ndesign](\Par)$, even when $\fwdEm[\Ndesign]$ is not a GP.
Plugging the random function $\fwdEm[\Ndesign]$ in place of $\fwd$ in the log-likelihood 
induces random approximations of the likelihood and posterior. In general, we use 
$\llikEm[\Ndesign]$ to denote a stochastic approximation to the log-likelihood induced
by an underlying emulator which has been fit to $\Ndesign$ design points. We write 
$\llikEm[\Ndesign](\Par) = \llik(\Par; \fwdEm[\Ndesign])$ when it is necessary to specify that
the log-likelihood approximation is induced by a forward model emulator. Similar notation 
$\postEm(\Par) = \postDens(\Par; \fwdEm[\Ndesign])$ and 
$\lpostEm(\Par) = \lpost(\Par; \fwdEm[\Ndesign])$ is used for the induced unnormalized 
posterior density and log posterior density approximations. The induced distributions for 
these quantities depend on the predictive distribution of the forward model emulator, as
well as the structure of the likelihood function and prior density. The first row of \Cref{fig:em_dist_1d} 
demonstrates how a GP forward model emulator induces predictive distributions 
for the log-likelihood and likelihood. 

The particular form of the forward model surrogate will be dictated by the properties of $\fwd$.  
A common challenge in emulating forward models stems from the fact that the observation 
space (i.e., the output space of $\fwd(\Par)$) can be very high-dimensional. 
Such complications commonly arise in settings with spatial or temporal structure, 
such as epidemic modeling \citep{FadikarAgentBased},
engineering design \citep{PODemulation}, ecological forecasting 
\citep{emPostDens,DagonCLM}, and climate modeling \citep{ESM_modeling_2pt0,idealizedGCM}.
One popular approach to deal with this challenge is to approximate the forward model output
as a linear combination of a small number of basis vectors, and then emulate the scalar coefficients 
of these vectors \citep{HigdonBasis,FadikarAgentBased,PODemulation}. In \Cref{basis_func_GPs}, 
we have already defined such a model in the GP setting.
Alternatively, surrogates 
can be fit to low-dimensional summaries of the high-dimensional output, such as spatial or 
temporal averages \citep{ESM_modeling_2pt0,idealizedGCM,CLMBayesianCalibration,CLMSurrogates}.
Many other approaches have been proposed, including emulators designed 
specifically for dynamical models \citep{GP_dynamic_emulation, Bayesian_emulation_dynamic, 
Liu_West_dynamic_emulation, dynamic_nonlinear_simulators_GP}.
Log-density emulation, described in \Cref{log_density_emulation}, offers another avenue for 
dimension reduction when the observation space is high-dimensional.

\begin{examplebox}[Gaussian Forward Model Emulation Setting] 
Consider the special case where both the likelihood and forward model emulator are Gaussian.
In particular, this implies an induced unnormalized posterior density surrogate
\begin{align}
\postDens(\Par; \fwdEm[\Ndesign]) &= \priorDens(\Par)\Gaussian(\obs| \fwdEm[\Ndesign](\Par), \likPar) \label{eq:fwd-em-Gaussian-post} \\
\fwdEm[\Ndesign](\Par) &\sim \Gaussian(\emMean[\Ndesign]{\fwd}(\Par), \emKer[\Ndesign]{\fwd}(\Par))
\label{eq:fwd-em-Gaussian-em}
\end{align}
for each $\Par \in \parSpace$. We refer to \Cref{eq:fwd-em-Gaussian-post,eq:fwd-em-Gaussian-em} as the 
\textit{Gaussian forward model emulation setting}.
These Gaussian assumptions facilitate closed-form computation of the posterior surrogate mean and variance
\begin{align*}
\E_{\fwdEm}\left[\postDens(\Par; \fwdEm[\Ndesign]) \right] 
&= \priorDens(\Par) \Gaussian(\obs | \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par)) \\
\Var_{\fwdEm}\left[\postDens(\Par; \fwdEm[\Ndesign]) \right]
&= \priorDens^2(\Par) \bigg[\frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\likPar + 
\emKer[\Ndesign]{\fwd}(\Par)  \right)}{\det(2\pi \likPar)^{1/2}} - \\
&\qquad \qquad\qquad \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\left[\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right]  \right)}{\det(2\pi [\likPar + \emKer[\Ndesign]{\fwd}(\Par)])^{1/2}}\bigg]
\end{align*}
for each $\Par \in \parSpace$.
The canonical example of a Gaussian surrogate is a GP, which is defined by specifying a prior distribution  
$\fwdPrior \sim \GP(\emMeanPrior{\fwd}, \emKerPrior{\fwd})$ for $\fwd$. Conditioning on the design yields 
the predictive distribution 
$\fwdEm[\Ndesign]  \Def \fwdPrior \given [\fwdPrior(\designIn) = \fwd(\designIn)] \sim \GP(\emMean[\Ndesign]{\fwd}, \emKer[\Ndesign]{\fwd})$, 
which constitutes the forward model emulator.
In the typical case that the observation space has dimension $\dimObs > 1$, then we interpret 
$\emMeanPrior{\fwd}, \emKerPrior{\fwd}$ as multi-output mean and covariance functions, respectively
(see \Cref{basis_func_model,basis_func_GPs,basis_func_noise}).
This Gaussian setting has been considered frequently in the literature
\citep{StuartTeck1,GP_PDE_priors,hydrologicalModel,Surer2023sequential,
VillaniAdaptiveGP,weightedIVAR,idealizedGCM,CES}.
\end{examplebox}

\subsubsection{Log-Density Emulation} \label{log_density_emulation}
Observe in \Cref{post_dens} that the expensive forward model evaluations appear in the posterior density only through the 
likelihood. Thus, for the purposes of posterior inference, it is sufficient to directly approximate either
$\Par \mapsto \llik(\Par)$ or $\Par \mapsto \lpost(\Par)$ without obtaining an approximation of the forward model.
This implies fitting a surrogate model to the design $\{\designIn, \llik(\designIn)\}$ or $\{\designIn, \lpost(\designIn)\}$,
which results in emulators $\llikEm[\Ndesign]$ and $\lpostEm[\Ndesign]$, respectively.
As with forward model emulators, we generically use $\emMean[\Ndesign]{\llik}(\Par)$ and $\emKer[\Ndesign]{\llik}(\Par)$ 
to denote the mean and variance of $\llikEm[\Ndesign](\Par)$. We likewise write $\emMean[\Ndesign]{\lpost}(\Par)$ and 
$\emKer[\Ndesign]{\lpost}(\Par)$ for the mean and variance of $\lpostEm[\Ndesign](\Par)$.
We refer to these two approaches collectively as \textit{log-density emulation}.
As with forward model surrogates, log-density surrogates induce a random approximation $\postEm[\Ndesign]$ of the 
unnormalized posterior density; we write $\postDens(\Par; \llikEm[\Ndesign])$ and $\postDens(\Par; \lpostEm[\Ndesign])$
when it is necessary to emphasize the quantity that is directly emulated.
The second row of \Cref{fig:em_dist_1d} demonstrates how a GP log-likelihood emulator induces a predictive 
distribution for the likelihood.
The log-likelihood emulation approach is considered in 
\citet{VehtariParallelGP,FATES_CES,trainDynamics,quantileApprox,ActiveLearningMCMC,FerEmulation,
StuartTeck1,random_fwd_models,GP_PDE_priors,OakleyllikEm,JosephMinEnergy,AlawiehIterativeGP}.
Log-likelihood emulation has also been utilized for Bayesian quadrature for numerical 
integration \citep{BayesQuadrature,BayesQuadRatios} and approximate Bayesian 
computation \citep{llikEmABC}.
The closely-related unnormalized log-posterior density (i.e., joint parameter-data density) emulation
strategy is used in \citet{emPostDens,Kandasamy_2017,llikRBF,gp_surrogates_random_exploration,landslideCalibration}.
We colloquially refer to this latter approach as \textit{log-posterior emulation} for brevity.
A related method is used in \citep{wang2018adaptive,adaptiveMultimodal}, in which surrogates are constructed 
to approximate a sequence of functions designed to be more regular than the log-likelihood.   
In general, emulating densities on the log scale is  preferred as a way to improve numerical stability,
enforce non-negativity in the density approximation, and yield a smoother target function for emulation.

Perhaps the most significant benefit of using log-density emulation is the reduction to predicting a scalar-valued output quantity,
as opposed to the potentially high-dimensional output space of the forward model. This notion is referred 
to as \textit{scalarization} in \citet{ranjan2016inverse, trainDynamics}.
On the other hand, log-density emulation has several downsides. Even on the log scale, log-densities can be fast-varying and 
exhibit large dynamic range, proving troublesome for surrogate models that assume stationarity \cite{wang2018adaptive}.
A simple toy example is used in \citep{Surer2023sequential} to demonstrate such modeling challenges, and the 
authors conclude that forward model emulation is preferred in their application of interest. A second potential weakness 
stems from the fact that log-densities often have known bound constraints, which may be difficult to enforce 
for certain surrogate models. \citet{quantileApprox} investigates the enforcement of 
bound constraints in GP-based log-likelihood emulation. In our numerical experiments, we demonstrate that ignoring
such constraints can yield very poor posterior approximations [\todo: maybe include a 1d example in the
recommendations section?]. A third challenge follows from the fact that 
the likelihood parameters (e.g., $\likPar$ in \Cref{inv_prob_Gaussian}) are typically not known and must also be 
learned from data. An obvious solution is to extend the input space of the emulator to include the likelihood parameters
\citep{llikRBF,emPostDens}, but the resulting response surface may prove more challenging to emulate. 
This approach also increases both the input dimensionality of the emulator, as well as the number of design points required to achieve 
a reasonable fit. Various approaches are explored in \citet{llikRBF} for dealing with nuisance likelihood parameters
without requiring additional runs of the expensive simulator. Alternatively, specific likelihood 
choices may admit a sufficient statistic which can be emulated independently of the likelihood parameters \citep{FerEmulation}. 

\begin{examplebox}[Gaussian Log-Density Emulation Setting]
Consider the special case where either the log-likelihood or log-posterior emulator is Gaussian.
Both cases imply
\begin{align}
\postEm[\Ndesign](\Par) &= \Exp{\lpostEm[\Ndesign](\Par)} \sim \LN\left(\emMean[\Ndesign]{\lpost}(\Par), \emKer[\Ndesign]{\lpost}(\Par)\right)
\label{eq:llik-em-Gaussian}
\end{align}
for each $\Par \in \parSpace$, where $\LN$ denotes a log-normal distribution parameterized by the mean and variance of the
underlying Gaussian random variable. The standard log-normal moment equations yield
\begin{align*}
\E\left[\postEm[\Ndesign](\Par)\right] 
&= \Exp{\emMean[\Ndesign]{\lpost}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\lpost}(\Par)}, \\
\Var\left[\postEm[\Ndesign](\Par)\right] 
&= \left[\Exp{\emKer[\Ndesign]{\lpost}(\Par)} - 1\right] \Exp{2\emMean[\Ndesign]{\lpost}(\Par) + \emKer[\Ndesign]{\lpost}(\Par)}.
\end{align*}
This \textit{Gaussian log-density emulation setting} arises when either the log-likelihood or log-posterior is emulated
by a GP. In either case, the induced unnormalized posterior density surrogate is a 
\textit{log-normal process}, the exponential of a GP. Note that the Gaussian log-density emulation setting is defined by 
an assumption only on the surrogate model,
unlike the forward model analog in \Cref{eq:fwd-em-Gaussian-post,eq:fwd-em-Gaussian-em}, which additionally makes an assumption on the 
likelihood. Gaussian log-density emulators are studied in 
\citet{StuartTeck1,StuartTeck2,FerEmulation,gp_surrogates_random_exploration,
trainDynamics,emPostDens,OakleyllikEm,ActiveLearningMCMC}.
\end{examplebox}

\paragraph{Log-Likelihood vs. Log-Posterior Emulation.} \label{sec:llik_vs_lpost}
The literature summary above indicates that both log-likelihood and log-posterior emulators are commonly used;
we are not aware of any cases in which the performance of the two methods is compared. 
Indeed, the choice appears inconsequential, as one emulator can easily be converted into the other 
by adding or subtracting the log-prior density, which represents a deterministic shift to the predictive 
distribution. For example, given a fit log-likelihood emulator $\llikEm[\Ndesign]$, a log-posterior emulator 
can be constructed via
\begin{equation}
\lpostEm[\Ndesign](\Par) \Def \llikEm[\Ndesign](\Par) + \log \priorDens(\Par),
\end{equation}
which simply adjusts the predictive mean. Despite the similarity, there are potential tradeoffs from 
a modeling standpoint. One benefit of emulating the log-posterior is the qualitative knowledge 
that the tails must decay, which can (and should) be leveraged in designing an appropriate 
surrogate model. The tail behavior of $\llik$ is not always well-known a priori, which can 
present more of a modeling challenge. On the other hand, the downside of emulating $\lpost$ is
that the prior is being approximated, and therefore cannot be relied on to decay in the tails.
These tradeoffs are related to the challenge of ensuring the emulator induces a well-defined
posterior approximation, a topic we discuss in \Cref{sec:existence}. 
 
\section{Posterior Approximation} \label{sec:post-approx}
The primary goal of Bayesian inference is to characterize the posterior distribution $\postDensNorm$,
which is then used for downstream analysis and decision making. Therefore, a central question in surrogate-based
inference is how to utilize the emulator in approximating $\postDensNorm$ \citep{StuartTeck1,SinsbeckNowak,VehtariParallelGP}.
Error in the emulator approximation will induce error in the posterior approximation, yielding 
biased posterior estimates. In the ideal setting, this bias can be eliminated by iteratively refining the 
surrogate (\Cref{sec:seq-design}), but computational constraints often place practical limits on such procedures.
It is thus crucial to acknowledge the surrogate surrogate uncertainty within the posterior approximation
in order to properly calibrate uncertainty estimates
\citep{BurknerSurrogate,StuartTeck1,FerEmulation,BurknerTwoStep}. However, there is no single 
objectively correct method for propagating the uncertainty captured by the surrogate predictive 
distribution \citep{BurknerSurrogate}. Rather, a variety of methods have been proposed, each with their 
own assumptions and practical limitations. It is important for practitioners to understand these 
considerations in order to determine the best approach for a particular application. In this section, we survey 
the various posterior approximations that have been proposed. See \Cref{sec:recs} for recommendations
in choosing a particular method.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth} % lpost dist [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lpost_dist_fwdem_trim.png}}
         \caption{$\lpost(\cdot; \fwdEm[\Ndesign])$}
         \label{fig:lpost_dist_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % post norm approx [fwd]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/post_norm_approx_fwdem.png}}
         \caption{$\postDensNorm$ approx}
         \label{fig:post_norm_approx_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % lpost dist [llik]
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/lpost_dist_llikem_trim.png}}
         \caption{$\lpost(\cdot; \llikEm[\Ndesign])$}
         \label{fig:lpost_dist_llikem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % post norm approx [llik] 
         \centering
         \includegraphics[width=\textwidth]{{toy_1d/post_norm_approx_llikem.png}}
         \caption{$\postDensNorm$ approx}
         \label{fig:post_norm_approx_llikem}
     \end{subfigure}
        \caption{A continuation of the example in \Cref{fig:em_dist_1d}. The top and bottom rows correspond
        to the forward model and log-likelihood emulation settings, respectively. The first column summarizes
        the distributions of the log-posterior approximations induced by the underlying emulators, with 
        the predictive mean in blue and the shaded region containing 90\% of the mass. The second column 
        plots compares various (normalized) posterior approximations derived from the log-posterior 
        emulator. Note that even though  the emulators interpolate the design points, the posterior
        approximations do not owing to the normalization.}
        \label{fig:post_norm_approx_1d}
\end{figure}

\subsection{Plug-In Mean}
Before discussing methods to propagate surrogate uncertainty, we establish the baseline method of simply 
ignoring this uncertainty. A deterministic emulator can be defined by utilizing only the predictive mean function
of the random surrogate model. Plugging the predictive mean in place of the quantity it is emulating induces 
a deterministic approximation $\postApproxNormMean[\Ndesign]$ of the posterior distribution 
$\postDensNorm$, which we refer to as the \textit{plug-in mean}, or simply \textit{mean}, approximation. 
In the forward model and log-likelihood emulation settings the mean approximations take the form
\begin{align}
\postApproxNormMean[\Ndesign](\Par; \fwdEm[\Ndesign])
&\Def \frac{1}{\normCstEm(\emMean[\Ndesign]{\fwd})} \postDens(\Par; \emMean[\Ndesign]{\fwd}) \label{eq:mean-approx-fwd} \\
\postApproxNormMean[\Ndesign](\Par; \llikEm[\Ndesign]) &\Def \frac{1}{\normCstEm(\emMean[\Ndesign]{\llik})} \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par)}, \label{eq:mean-approx-llik}
\end{align}
with normalizing constants
\begin{align}
&\normCstEm(\emMean[\Ndesign]{\fwd}) \Def \int_{\parSpace} \postDens(\Par; \emMean[\Ndesign]{\fwd}) d\Par,
&&\normCstEm(\emMean[\Ndesign]{\llik}) \Def \int_{\parSpace} \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par)} d\Par.
\end{align}
Existence of these integrals is discussed in \Cref{sec:existence}.
The mean approximation has been applied in various contexts
\citep{VehtariParallelGP,trainDynamics,emPostDens,BurknerSurrogate,CLMBayesianCalibration,Lueckmann2018LikelihoodfreeIW} 
and analyzed theoretically 
\citep{StuartTeck1,StuartTeck2,random_fwd_models,TeckHyperpar,gp_surrogates_random_exploration}.
If the emulator predictive mean is known to be highly accurate then this approximation may be reasonable,
but in general ignoring the surrogate uncertainty can lead to posterior approximations that are 
both inaccurate and overly confident \citep{BurknerSurrogate}. 

\subsection{Expected Posterior}
We now consider posterior approximations that incorporate the surrogate uncertainty.
Consider a random surrogate $\postEm[\Ndesign]$ for the unnormalized posterior density, 
induced by some underlying emulator (e.g., a forward model or log-density emulator).
We will use the notation $\E_{\Ndesign}$ to indicate expectations with respect to the 
distribution of the underlying surrogate model; e.g., the law of $\fwdEm[\Ndesign]$ or $\lpostEm[\Ndesign]$.
By normalizing $\postEm[\Ndesign]$, we obtain a random density
\footnote{More generally, this is a random measure. We work with densities to avoid measure-theoretic technicalities.}
\begin{align}
&\postNormEm[\Ndesign](\Par) 
\Def \frac{1}{\llikEmRdm[\Ndesign]{\normCst}} \postEm[\Ndesign](\Par),
&&\normCstEm[\Ndesign]\Def \int_{\parSpace} \postEm[\Ndesign](\Par) d\Par,
\label{eq:sample-approx}
\end{align}
which summarizes the uncertainty about the true posterior. 
While $\postEm[\Ndesign](\Par)$ depends 
only on the univariate surrogate prediction at $\Par$, $\llikEmRdm[\Ndesign]{\postDensNorm}(\Par)$
depends on the entire random function $\llikEmRdmDens[\Ndesign]$ due to its dependence on 
the (random) normalizing constant $\llikEmRdm[\Ndesign]{\normCst}$. The quantity
$\llikEmRdm[\Ndesign]{\postDensNorm}$ is referred to as the \textit{sample} approximation in
\citet{StuartTeck1, StuartTeck2,random_fwd_models,TeckHyperpar}.
To construct a deterministic approximation to $\postDens$, we can consider the expectation
\begin{equation}
\postApproxEP[\Ndesign](\Par) \Def \E_{\Ndesign}\left[\postNormEm[\Ndesign](\Par) \right], \label{eq:ep-approx}
\end{equation}
which we refer to as the \textit{expected posterior (EP)}, following the terminology in \citet{BurknerSurrogate}.
Under a nonparametric surrogate (e.g., a GP), the expectation in \Cref{eq:ep-approx} is with 
respect to an infinite-dimensional random element, and thus its existence is a non-trivial matter \citep{StuartTeck1}.
Assuming the expectation exists, \Cref{alg:ep} could in principle 
be used to draw samples from $\llikEmSampDensNorm$. 

\begin{algorithm}
    \caption{Direct sampling from $\llikEmSampDensNorm$}
    \label{alg:ep}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{sampleEP}{$\llikEmRdm[\Ndesign]{\postDensNorm}, \NSample, M$}     
        \For{$\sampleIndex \gets 1, \dots, \NSample$} 
        		\State $\llikEmRdm[\Ndesign]{\postDensNorm}^{(\sampleIndex)} \sim \law(\llikEmRdm[\Ndesign]{\postDensNorm})$ \Comment{Sample posterior trajectory}
		\State $\Par^{(\sampleIndex, 1)}, \dots, \Par^{(\sampleIndex, M)} \overset{iid}{\sim} \llikEmRdm[\Ndesign]{\postDensNorm}^{(\sampleIndex)}$ \Comment{Sample parameters}
	\EndFor
	\State \Return $\{\Par^{(\sampleIndex, m)}\}_{1 \leq \sampleIndex \leq \NSample, \ 1 \leq m \leq M}$
	\EndFunction
    \end{algorithmic}
\end{algorithm}

A practical implementation 
would entail first sampling a trajectory of the surrogate model, which induces a sample 
trajectory of the unnormalized random density $\llikEmRdmDens[\Ndesign]$. Parameter samples
could then be drawn from the distribution implied by this unnormalized density using standard 
methods (e.g., MCMC).
The output of the algorithm thus represents a mixture over an 
ensemble of posterior distributions. Choosing $M = 1$  (drawing one sample per
trajectory of $\postNormEm[\Ndesign]$) implies that \Cref{alg:ep} returns independent 
samples from $\postApproxEP[\Ndesign]$. For $M > 1$, the samples will be dependent.
For example, in the extreme case $\NSample = 1$ they will 
still have the correct marginal distribution $\postApproxEP[\Ndesign]$, but will 
only provide a summary of a single trajectory from $\postNormEm[\Ndesign]$ and thus 
in general provide a poor representation of this random density.

\Cref{alg:ep} is readily implementable in practice provided the ability to sample trajectories 
of $\postEm[\Ndesign]$, which will generally be possible for
parametric surrogate models. \citet{garegnani2021NoisyMCMC,BurknerSurrogate}, and \citet{BurknerTwoStep}
investigate \Cref{alg:ep} in such settings. Under certain assumptions, \citet{garegnani2021NoisyMCMC} 
bounds the Monte Carlo error in the sample-based approximation of $\postApproxEP[\Ndesign]$.
\citet{BurknerTwoStep} proposes an importance sampling approximation to reduce the 
required number of sampled trajectories.

With a nonparametric emulator, the practical difficulty in 
implementing \Cref{alg:ep} is the requirement to sample an infinite-dimensional 
trajectory from the surrogate model.
These difficulties are noted in \citet{VehtariParallelGP} as justification for pursuing alternative 
approaches. As far as we are aware, \citet{trainDynamics} is the only work to attempt an 
implementation of \Cref{alg:ep} in the nonparametric context. Their method consists of approximating GP 
trajectories by sampling the surrogate only at a finite grid of points, and then approximating 
the trajectory as the GP mean, conditional on the sampled values at these points.
We note that an alternative approach could involve constructing a finite-dimensional approximation 
of the surrogate in order to provide the practical means to simulate (approximate) trajectories 
of $\postEm[\Ndesign]$. In the GP setting, approaches such as the Karhunen-Loeve expansion 
and random Fourier features could be leveraged for this purpose \citep{dimRedPolyChaos,samplingGPPosts}. 
But as with the \citet{trainDynamics} method, this approach is only an approximate implementation of \Cref{alg:ep}.

\begin{table}[h]
\centering
\begin{tabular}{>{\centering\arraybackslash}p{4cm} >{\centering\arraybackslash}p{5cm} >{\centering\arraybackslash}p{5cm}}
\toprule
\textbf{Plug-In Mean} & \textbf{Expected Likelihood} & \textbf{Expected Posterior} \\
\midrule
$\displaystyle \frac{\postDens(\Par; \E_{\Ndesign}[\fwdEm[\Ndesign]])}{\normCst(\E_{\Ndesign}[\fwdEm[\Ndesign]])}$ & 
$\displaystyle \frac{\E_{\Ndesign}\left[\postDens(\Par; \fwdEm[\Ndesign])\right]}{\E_{\Ndesign}\left[\normCstEm[\Ndesign]\right]}$ & 
$\displaystyle \E_{\Ndesign}\left[\frac{\postDens(\Par; \fwdEm[\Ndesign])}{\normCstEm[\Ndesign]}\right]$ \\
\bottomrule
\end{tabular}
\caption{Comparison of the normalized density approximations implied by the plug-in mean, expected likelihood,
and expected posterior approximations in the forward model emulation setting.}
\label{tab:post-approx-comparison}
\end{table}

\subsection{Expected Likelihood}
Given the challenge of implementing \Cref{alg:ep} with a nonparametric emulator, the majority of 
previous literature instead proposes deterministic approximations of the unnormalized
density. As opposed to the expected posterior method, these approximations are computed 
pointwise in $\Par$. This is a potential downside of this approach, as it does not leverage 
the full structure of the surrogate predictive distribution (e.g., the predictive covariance of a GP emulator).
However, the focus on unnormalized density approximation has several practical benefits, 
including that it does not require approximating surrogate trajectories and is amenable to the application of 
standard Bayesian inference procedures using a single MCMC run.

We start by considering the approximation derived by computing the pointwise expectation 
of $\postEm(\Par)$ and then normalizing after-the-fact.
This yields the unnormalized density estimate
\begin{align}
\postApproxMarg[\Ndesign](\Par) \Def \E_{\Ndesign}\left[\postEm[\Ndesign](\Par) \right], \label{eq:post-approx-EL}
\end{align}
which we refer to as the \textit{expected likelihood (EL)}
\footnote{Note that we are technically considering expectations of the unnormalized posterior density approximation 
$\postEm[\Ndesign](\Par)$, but this is equivalent to taking expectations of the likelihood approximation and
then multiplying by the prior density.}
approximation, again borrowing terminology from
 \citet{BurknerSurrogate}. \citet{StuartTeck1} instead refer to this as the \textit{marginal} approximation, 
 and show the normalizing constant implied by \Cref{eq:post-approx-EL} is given by
 \begin{equation}
 \int_{\parSpace} \E_{\Ndesign}\left[\postEm[\Ndesign](\Par) \right] d\Par 
 = \E_{\Ndesign} \int_{\parSpace} \postEm[\Ndesign](\Par) d\Par
 = \E_{\Ndesign}\left[\normCstEm \right],
 \end{equation} 
meaning the expected likelihood approximation is a ratio estimator of the form 
 $\postApproxNormMarg[\Ndesign](\Par) = \E_{\Ndesign}[\postEm[\Ndesign](\Par)] / \E_{\Ndesign}[\normCstEm]$.
 Note the difference with respect to 
 $\postApproxEP[\Ndesign](\Par) = \E_{\Ndesign}[\postEm[\Ndesign](\Par) / \normCstEm]$, 
 which we highlight in \Cref{tab:post-approx-comparison} in the forward model emulation setting.  
 The unnormalized expected likelihood approximation $\postApproxMarg[\Ndesign]$ may be justified from a Bayesian
 decision-theoretic viewpoint as the minimizer of an $L^2$ risk. However, this does not 
 confer the same optimality on the normalized approximation $\postApproxNormMarg[\Ndesign]$;
 see \citep{SinsbeckNowak,StuartTeck2,VehtariParallelGP} for details. To our knowledge,
 \citep{SinsbeckNowak,StuartTeck1} are the first to consider the expected likelihood approximation
 to propagate surrogate uncertainty.
 
 If the pointwise expectation 
 $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$ is computable in closed-form, then 
the expected likelihood approximation can be sampled using standard MCMC software.
In cases where this expectation is intractable, pseudo-marginal MCMC 
\citep{pseudoMarginalMCMC} may be employed, so long as 
samples can be drawn from the surrogate predictive distribution $\postEm[\Ndesign](\Par)$
at any input $\Par$. The pseudo-marginal approach is discussed in \Cref{sec:MH-approx}.
\citet{BurknerSurrogate} alternatively propose to replace  $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$
with a fixed Monte Carlo estimate derived from surrogate samples (i.e., a sample average approximation). 
Their method differs from the 
pseudo-marginal algorithm in that the Monte Carlo samples are simulated once 
and then fixed throughout the MCMC run. This allows for the application of standard 
MCMC methods, but targets an approximation to $\postApproxNormMarg[\Ndesign]$
and is only directly implementable in the parametric setting.
In the nonparametric case, most previous literature 
has focused on the Gaussian settings from \Cref{eq:fwd-em-Gaussian-post,eq:fwd-em-Gaussian-em} 
and \Cref{eq:llik-em-Gaussian} due to their analytic tractability. We summarize these special cases below.

\begin{examplebox}[Gaussian Forward Model Emulation Setting]
In the Gaussian forward model emulation setting from \Cref{eq:fwd-em-Gaussian-post,eq:fwd-em-Gaussian-em}, the
expected likelihood approximation is given by 
\begin{align}
\postApproxMarg[\Ndesign](\Par) 
&= \priorDens(\Par) \Gaussian\left(\obs \given \emMean[\Ndesign]{\fwd}(\Par), \likPar + \emKer[\Ndesign]{\fwd}(\Par) \right). 
\label{eq:post-approx-fwd-EL-Gaussian}
\end{align}
Moreover, \Cref{eq:post-approx-fwd-EL-Gaussian} is the unnormalized posterior corresponding to the modified
Bayesian inverse problem
\begin{align}
&\obs = \emMean[\Ndesign]{\fwd}(\Par) + \eta_{\Ndesign}(\Par) + \noise
&&\noise \sim \Gaussian(0, \likPar)  \label{inv_prob_Gaussian_modified} \\
&\eta_{\Ndesign}(\Par) \sim \Gaussian(0, \emKer[\Ndesign]{\fwd}(\Par))
&&\Par \sim \priorDens  \nonumber
\end{align}
where $\noise$, $\Par$, and $\eta_{\Ndesign}(\Par)$ are pairwise a priori independent for all $\Par$.
The approximate likelihood retains a Gaussian form, where the surrogate predictive mean
$\emMean[\Ndesign]{\fwd}$ has replaced the true forward model $\fwdEm[\Ndesign]$. The surrogate uncertainty
is incorporated via the addition of $\emKer[\Ndesign]{\fwd}(\Par)$ to the noise covariance $\likPar$.
We emphasize that even if the prior $\priorDens$ is Gaussian, the posterior approximation will typically be non-Gaussian
due to the nonlinearity of $\emMean[\Ndesign]{\fwd}(\Par)$ and the fact that the likelihood covariance depends on 
$\Par$ through $\emKer[\Ndesign]{\fwd}(\Par)$. The modified inverse problem viewpoint 
in \Cref{inv_prob_Gaussian_modified} is noted in \citet{SinsbeckNowak} and \citet{StuartTeck1}; the former shows that
this viewpoint extends beyond the Gaussian setting, even when analytical expressions for $\postApproxMarg[\Ndesign](\Par) $ 
may not exist.
The closed-form expression in \Cref{eq:post-approx-fwd-EL-Gaussian} is also noted in both
\citet{SinsbeckNowak, StuartTeck1} and has been used in many subsequent studies 
\citep{VehtariParallelGP,weightedIVAR,StuartTeck2,GP_PDE_priors,CES,idealizedGCM,
villani2024posteriorsamplingadaptivegaussian,hydrologicalModel}. 
\end{examplebox}

\begin{examplebox}[Gaussian Log-Density Emulation Setting]
Consider the Gaussian log-density emulation setting from \Cref{eq:llik-em-Gaussian}
with a log-likelihood emulator. With this setup, the expected likelihood approximation is given by 
\begin{align}
\postApproxMarg[\Ndesign](\Par) 
&= \priorDens(\Par) \Exp{\emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)}
= \postApproxMean(\Par) \Exp{\frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)}. 
\label{eq:post-approx-llik-EL-Gaussian}
\end{align}
The log-posterior emulation case is very similar. From \Cref{eq:post-approx-llik-EL-Gaussian},
we see that $\postApproxMarg[\Ndesign]$ inflates the mean approximation at points
where the emulator is uncertain. It is notable that the uncertainty inflation factor 
$\Exp{\frac{1}{2}\emKer[\Ndesign]{\lpost}(\Par)}$ scales very quickly as the surrogate variance 
increases. \citet{VehtariParallelGP} note this fact, emphasizing that using the expectation can
be a misleading summary of the log-normal random variable $\postEm[\Ndesign](\Par)$.
In practice, we also find this to be the case; the heavy-tailed nature of log-normal distributions
can lead $\postApproxNormMarg[\Ndesign]$ to be heavily concentrated in small regions with high 
predictive variance. In \Cref{rec:bound-constraints} and \Cref{sec:case-study-lpost-em} we 
describe how enforcing bound constraints can avoid these pathologies.
The EL approximation in the Gaussian log-density emulation setting is analyzed theoretically
in \citet{StuartTeck1,StuartTeck2,GP_PDE_priors,random_fwd_models,TeckHyperpar}.
\end{examplebox}

\subsection{Other Unnormalized Density Approximations}
Various alternative approximations have been proposed for the unnormalized posterior density. 
Instead of computing pointwise expectations, one can consider summarizing $\postEm[\Ndesign](\Par)$
by computing pointwise quantiles \citep{VehtariParallelGP,quantileApprox}. In the log-likelihood 
emulation setting with a GP emulator, \cite{VehtariParallelGP} considers the $\quantileProb$-quantile of 
the likelihood surrogate
\begin{equation}
\mathrm{Quantile}_{\quantileProb}(\Exp{\llikEm[\Ndesign](\Par)})
= \Exp{\emMean[\Ndesign]{\llik}(\Par) + \GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}},
\label{eq:post-approx-quantile}
\end{equation}
where $\GaussianCDF$ denotes the standard Gaussian distribution function. This expression is of the same form 
as the expected likelihood in \Cref{eq:post-approx-llik-EL-Gaussian}, but the uncertainty inflation term 
$\Exp{\GaussianCDF^{-1}(\quantileProb) \sqrt{\emKer[\Ndesign]{\llik}(\Par)}}$ scales more slowly than 
$\Exp{\frac{1}{2} \emKer[\Ndesign]{\llik}(\Par)}$. The special case $\quantileProb = 1/2$ (i.e., the median) 
reduces to the mean approximation in \Cref{eq:mean-approx-llik}, while values $\quantileProb > 1/2$ imply 
the density will be inflated in regions of higher surrogate uncertainty. \citet{FATES_CES} also utilize the 
approximation in \Cref{eq:post-approx-quantile}, though they do not explicitly draw the connection to 
the quantile estimator.

In the forward model emulation setting, \citet{BurknerSurrogate} also consider an \textit{expected log-likelihood}
approximation of the form $\priorDens(\Par) \Exp{\E_{\Ndesign}[\llik(\Par; \fwdEm[\Ndesign])]}$ and draw 
a connection with power-scaled likelihoods. However, the authors ultimately recommend the expected 
posterior and expected likelihood as preferred alternatives to this method.

\subsection{Noisy MCMC Approximations} \label{sec:MH-approx}
In this section, we describe an alternative approach to surrogate-based inference
that focuses on constructing approximations to MCMC algorithms as opposed to approximations
of the unnormalized posterior density. Some of these methods provide algorithms for sampling 
from approximate posteriors already defined above, while others represent new approaches to inference.
The benefits of these approaches include ease of implementation and generality--they only require 
the ability to draw samples from the emulator predictive distribution at finite sets of inputs.
Therefore, these algorithms do not rely on distributional 
assumptions on the surrogate or likelihood, and can be used in the nonparametric surrogate setting.

To start, we recall the standard Metropolis-Hastings (MH) algorithm, which is defined by a proposal 
kernel with density $\propDens(\Par, \cdot)$. If the Markov chain is in the current state 
$\Par \in \parSpace$ then the next state is defined by sampling the proposal
 $\propPar \sim \propDens(\Par, \cdot)$, which is accepted with probability
\begin{align}
&\accProbMH(\Par, \propPar) \Def 
\min\left\{1, \frac{\postDens(\propPar)\propDens(\propPar, \Par)}{\postDens(\Par) \propDens(\Par, \propPar)} \right\}.
\label{MH_acc_prob_exact}
\end{align}
If accepted, the next state is defined to be $\propPar$, else it is set to the current state $\Par$. 
This procedure is summarized in \Cref{alg:MH}. 

\begin{figure}[ht]
\centering
\begin{minipage}[t]{0.495\textwidth}
    \floatname{algorithm}{Alg.}
    \captionsetup{type=algorithm}
    \caption{Metropolis-Hastings}
    \label{alg:MH}
    \begin{algorithmic}[1]
    \Function{MH}{$\Par_0, \NMCMC$}     
        \For{$k \gets 0, \dots, \NMCMC$} 
            \State $\tilde{\Par} \sim q(\Par_{k}, \cdot)$
            \State $\alpha \gets \min\left\{1, \frac{\pi(\tilde{\Par}) q(\tilde{\Par}, \Par_k)}{\pi(\Par_k) q(\Par_k, \tilde{\Par})} \right\}$
            \State $b \sim \text{Bernoulli}(\alpha)$
            \If{$b = 1$}
                \State $\Par_{k+1} \gets \tilde{\Par}$ 
            \Else
                \State $\Par_{k+1} \gets \Par_k$
            \EndIf
        \EndFor
    \EndFunction
    \end{algorithmic}
\end{minipage}
\hfill
\begin{minipage}[t]{0.495\textwidth}
    \floatname{algorithm}{Alg.}
    \captionsetup{type=algorithm}
    \caption{Noisy Metropolis-Hastings}
    \label{alg:MH-noisy}
    \begin{algorithmic}[1]
    \Function{MH-Noisy}{$\Par_0, \NMCMC, \postSampleKernel$}
        \State $\hat{\pi}_{\text{curr}} \sim \mathrm{law}(\postEm[\Ndesign](\indexMCMC[0]{\Par}))$
        \For{$k \gets 0, \dots, \NMCMC$} 
            \State $\tilde{\Par} \sim q(\Par_k, \cdot)$
            \State $[\hat{\pi}_{\Par}, \hat{\pi}_{\tilde{\Par}}] \sim \postSampleKernel([\Par_k, \tilde{\Par}, \hat{\pi}_{\text{curr}}], \cdot)$
            \State $\hat{\alpha} \gets \min\left\{1, \frac{\hat{\pi}_{\tilde{\Par}} \cdot q(\tilde{\Par}, \Par_k)}{\hat{\pi}_{\Par} \cdot q(\Par_k, \tilde{\Par})} \right\}$
            \State $b \sim \text{Bernoulli}(\hat{\alpha})$
            \If{$b = 1$}
                \State $\Par_{k+1} \gets \tilde{\Par}$
                \State $\hat{\pi}_{\text{curr}} \gets \hat{\pi}_{\tilde{\Par}}$
            \Else
                \State $\Par_{k+1} \gets \Par_k$
            \EndIf
        \EndFor
    \EndFunction
    \end{algorithmic}
\end{minipage}
\caption{\textbf{(Left)} A standard Metropolis-Hastings MCMC algorithm with proposal density $\propDens(\Par, \cdot)$.
\textbf{(Right)} A generic noisy Metropolis-Hastings algorithm. The choice of Markov kernel $\postSampleKernel$ 
defines particular algorithms.}
\end{figure}

We consider a family of algorithms that change $\accProbMH(\Par, \propPar)$ by
substituting the exact densities $\postDens(\Par)$  and $\postDens(\propPar)$ 
with approximations sampled from a specified distribution. The particular choice of 
this distribution yields different algorithms, all of which are ``noisy'' in the sense that 
an additional Monte Carlo step has been injected within the standard MH scheme
\citep{noisyMCSurvey,noisyMCMC,stabilityNoisyMH}. In particular, we assume 
that the approximation of $[\postDens(\Par), \postDens(\propPar)]$ is sampled as
\begin{equation}
[\hat{\postDens}_{\Par}, \hat{\postDens}_{\propPar}] \sim \postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot),
\end{equation}
where $\postSampleKernel$ is a Markov kernel mapping from source $\parSpace^2 \times \R_+$
to target $\R^2_{+}$. The generic noisy MH algorithm is stated in \Cref{alg:MH-noisy}. 
We consider three special cases below.

\paragraph{Pseudo-Marginal.}
We first consider the choice 
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \delta_{\postDens^\prime} \otimes \mathrm{law}(\postEm[\Ndesign](\propPar)),
\end{equation}
which implies that at each iteration only the density value at the proposed point 
$\postEm[\Ndesign](\propPar)$  is sampled, while the value at the current point 
is recycled from the previous iteration. This is a pseudo-marginal algorithm 
targeting the stationary distribution $\postApproxNormMarg[\Ndesign]$ \citep{pseudoMarginalMCMC}.
In principle, it provides an exact MCMC scheme to sample from the expected likelihood 
approximation even when the expectation $\E_{\Ndesign}[\postEm[\Ndesign](\Par)]$ is intractable, 
a fact noted in \citet{StuartTeck1}. The key property ensuring invariance with respect to 
$\postApproxNormMarg[\Ndesign]$ is that the value $\hat{\postDens}_{\propPar}$ sampled from 
$\postSampleKernel$ is an unbiased estimate of $\E_{\Ndesign}[\postEm[\Ndesign](\propPar)]$.
Therefore, the method remains valid for estimators of the form 
\begin{align}
&\hat{\postDens}_{\tilde{\Par}}^{J} \Def \frac{1}{J} \sum_{j=1}^{J} \hat{\postDens}_{\tilde{\Par}}^{(j)},
&&\hat{\postDens}_{\tilde{\Par}}^{(j)} \overset{\mathrm{iid}}{\sim} \mathrm{law}(\postEm[\Ndesign](\propPar)). \label{eq:pm-unbiased-est}
\end{align}
It is well-known that pseudo-marginal methods
can suffer from slow mixing, but that efficiency can be improved by reducing the variance 
in the estimate of $\E_{\Ndesign}[\postEm[\Ndesign](\propPar)]$; e.g., by increasing $J$ in
\Cref{eq:pm-unbiased-est} \citep{pseudoMarginalMCMC,pseudoMarginalEfficiency}.
The pseudo-marginal approach to sampling $\postApproxNormMarg[\Ndesign]$ is studied 
in \citet{garegnani2021NoisyMCMC}.

\paragraph{Monte Carlo within Metropolis Hastings.}
We next consider the Markov kernel 
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \mathrm{law}(\postEm[\Ndesign](\Par)) \otimes \mathrm{law}(\postEm[\Ndesign](\propPar)),
\end{equation}
which independently re-samples the density values at both the current and proposed locations
at each iteration (the kernel does not depend on $\postDens^\prime$). Algorithms of this form are 
typically referred to as Monte Carlo within Metropolis Hastings (MCwMH), and have been studied as an
efficient alternative to the pseudo-marginal algorithm \citep{noisyMCMC,stabilityNoisyMH}.
The sampled density values can also be replaced with sample means as in \Cref{eq:pm-unbiased-est},
though the efficiency of MCwMH is generally less sensitive to the variance in the estimate
\citep{garegnani2021NoisyMCMC,stabilityNoisyMH}.
The MCwMH is typically referred to as inexact, in the sense that it does not admit 
$\postApproxNormMarg[\Ndesign]$ as an invariant distribution. However, in the present setting
$\postApproxNormMarg[\Ndesign]$ is itself an approximation of $\postDensNorm$ so we might
view MCwMH as an alternative method for propagating surrogate uncertainty
on equal footing with $\postApproxNormMarg[\Ndesign]$. This perspective is taken in 
\citet{surrogateNoisyMCMC}, while \citet{garegnani2021NoisyMCMC} studies MCwMH
as an approximation to $\postApproxNormMarg[\Ndesign]$. A variant of MCwMH
is employed in \citet{FerEmulation}.

\paragraph{Expected Acceptance Probability.}
Finally, we consider
\begin{equation}
\postSampleKernel([\Par, \propPar, \postDens^\prime], \cdot)
\Def \mathrm{law}(\postEm[\Ndesign](\Par), \postEm[\Ndesign](\propPar)),
\end{equation}
implying that both density values are re-sampled each iteration, but now from
the joint distribution implied by the surrogate $\postEm[\Ndesign]$. 
We refer to this as the \textit{expected acceptance probability (E-Acc)} approximation,
as it can be viewed as marginalizing the MH acceptance probability with respect
to the surrogate. Indeed, by inserting $\postEm[\Ndesign]$
in place of $\postDens$ in $\accProbMH(\Par, \propPar)$, the surrogate induces a random 
approximation
\begin{equation}
\accProbMHEm[\Ndesign](\Par, \propPar) 
\Def \min\left\{1, \frac{\postEm[\Ndesign](\propPar)\propDens(\propPar, \Par)}{\postEm[\Ndesign](\Par) \propDens(\Par, \propPar)} \right\}
\label{eq:MH-prob-surrogate}
\end{equation}
of the MH acceptance probability. The E-Acc algorithm can thus be viewed as a modification of the
MH scheme in \Cref{alg:MH} with $\E_{\Ndesign}[\accProbMHEm[\Ndesign](\Par, \propPar)]$
replacing $\accProbMH(\Par, \propPar)$. Many of the other posteriors introduced above can also
be viewed as invoking particular approximations of $\accProbMH(\Par, \propPar)$, as summarized
in \Cref{tab:acc-prob-comparison}. \citet{surrogateNoisyMCMC} propose the E-Acc algorithm, 
and explore connections to the expected posterior approximation.

\begin{table}[h]
\centering
\small % (optional) shrink slightly if needed
\renewcommand{\arraystretch}{1.6} % row height
\setlength{\tabcolsep}{10pt} % column separation
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}} % centered column of width #1

\begin{tabular}{lC{10cm}}
\toprule
\textbf{Posterior Approximation} & \textbf{MH Approximation} \\
\midrule
Plug-In Mean & 
\(\displaystyle \min\left\{1, \frac{\Exp{\E[\lpostEm[\Ndesign](\propPar)]}}{\Exp{\E[\lpostEm[\Ndesign](\Par)]}} \right\}\) \\ 
Expected Likelihood & 
\(\displaystyle \min\left\{1, \frac{\E \left[\Exp{\lpostEm[\Ndesign](\propPar)}\right]}{\E\left[\Exp{\lpostEm[\Ndesign](\Par)}\right]} \right\}\) \\
Expected Acc. Prob. & 
\(\displaystyle \E\left[\min\left\{1, \frac{\Exp{\lpostEm[\Ndesign](\propPar)}}{\Exp{\lpostEm[\Ndesign](\Par)}} \right\}\right]\) \\
\bottomrule
\end{tabular}
\caption{The approximations to the Metropolis-Hastings (MH) acceptance probability implied by different posterior approximations using a log-posterior surrogate $\lpostEm[\Ndesign]$. All expectations are with respect to 
the underlying surrogate predictive distribution. For brevity, the acceptance probabilities 
are presented for the case of a symmetric proposal distribution.}
\label{tab:acc-prob-comparison}
\end{table}

\subsection{Existence and Integrability} \label{sec:existence}
We have so far implicitly assumed that the surrogate-induced posterior approximations 
introduced above are well-defined. Consideration of this question points to important 
practical issues in constructing emulators for use in posterior approximation (see
\Cref{sec:recs}). The primary concern
is that the tail behavior of the surrogate may lead to approximations of $\postDens$ that 
are not integrable. Such pathologies can easily arise if utilizing a stationary surrogate, where 
the predictive mean and variance stabilize at constant values as distance from the design points 
increases. This issue is especially a concern if emulating the log-posterior density, as the prior 
density is being modeled and hence cannot be relied upon to ensure integrability \citep{emPostDens}.
Figure \todo illustrates simple examples where such pathologies can occur.
In order to avoid issues related to
tail behavior, previous work tends to restrict to the setting where $\parSpace$
is a compact subset of $\R^{\dimPar}$ \citep{StuartTeck1, VehtariParallelGP}.
Other applications have not directly addressed this issue, but in numerical experiments focus on 
prior distributions with compact support \citet{trainDynamics,FATES_CES} or truncate an unbounded
prior to achieve compact support \citep{gp_surrogates_random_exploration,FerEmulation}. 
\citet{emPostDens} address this issue in depth, describing how the use of stationary log-posterior
surrogates can lead to divergent MCMC chains when performing approximate posterior inference.
Theoretical treatments have also explored conditions to ensure existence over generic unbounded
domains \citep{random_fwd_models,garegnani2021NoisyMCMC}. We refer to 
\citep{StuartTeck1,StuartTeck2} for a comprehensive theoretical treatment in the GP setting.
Ideally, the surrogate ought to be constructed such that every realization of $\postEm[\Ndesign]$
is integrable (i.e., $\normCstEm$ exists almost surely). This assumption is implicit in \Cref{alg:ep}, 
which assumes that each sampled trajectory of $\postEm[\Ndesign]$ induces a valid probability 
distribution from which samples can be drawn. This condition can be weakened for other 
approximations; for example, in \Cref{eq:llik-em-Gaussian} we observe the requirement is that
$\log\left[\priorDens(\Par)\right] + \emMean[\Ndesign]{\llik}(\Par) + \frac{1}{2}\emKer[\Ndesign]{\llik}(\Par)$
decays sufficiently quickly as $\norm{\Par} \to \infty$. 

\section{Sequential Design} \label{sec:seq-design}
Achieving a desired level of surrogate accuracy using a single round of simulator runs 
typically requires an impractically large number of design points. This stems from the 
fact that the posterior distribution can be highly concentrated relative to the prior support.
Initial designs sampled from the prior may completely miss the subset of $\parSpace$ where 
the posterior is large. In such settings, it is necessary to adaptively refine the surrogate using 
additional rounds of simulator runs. Given a limited computational budget, the new inputs at which
the simulator is run should be carefully chosen with respect to the goal of improving the posterior approximation.
The cycle of selecting design points, querying the simulator, and updating the surrogate represents a 
particular \textit{sequential design}, or \textit{active learning}, problem. 

Passively sampling new simulator evaluation points from the prior can suffer from the same issues
as the initial design. By contrast, the current information in the surrogate can be leveraged to actively
target promising regions of $\parSpace$. However, such model-based design strategies must 
avoid placing too much trust in the (imperfect) surrogate. In other words, effective active 
learning algorithms must target promising regions (according to the current surrogate) but also explore
areas where the surrogate is uncertain.
This ``exploration vs. exploitation'' balancing act is familiar in fields such as 
Bayesian optimization \citep{reviewBayesOpt}, Bayesian quadrature 
\citep{BayesQuadrature,BayesQuadratureAL,BayesQuadRatios,quadratureLogGP}, 
reinforcement learning \citep{BadiaRL,LiuRL}, and bandit algorithms \citep{banditsEmpirical,LattimoreBandits}, 
among others. For GP surrogates, a variety of design criteria (i.e., acquisition functions) have been 
proposed that seek to strike this balance in the context of solving Bayesian inverse problems \citep{SinsbeckNowak,Surer2023sequential,KandasamyActiveLearning2015,weightedIVAR,
VehtariParallelGP,VillaniAdaptiveGP,AlawiehIterativeGP}.

The exploration-exploitation question is one of decision making under uncertainty.   
Even if one had perfect knowledge about the true posterior, there remains the question concerning the optimal
placement of design points for surrogate-based posterior approximation. Insights into this question can guide
the development of improved design algorithms.
Various papers informally remark that accurate approximation requires design points only in the region where the 
posterior is large \citep{AlawiehIterativeGP} [\todo add other citations].
This statement is somewhat misleading, and should be amended as follows:
achieving an accurate posterior approximation
requires an accurate emulator in the subset of $\parSpace$ where the posterior is large, while also
containing enough global information to know where the posterior is small. This informal statement is 
put on rigorous footing in \citet{StuartTeck2}, with theoretical results suggesting designs based on 
an over-dispersed version of $\postDensNorm$.
The notion of oversampling the tails has also been proposed to construct designs
for numerical integration \citep{briol2017sampling}. These results are consistent with the notion of 
constraining the global surrogate behavior, while also fine-tuning the local fit in high-probability regions.
Therefore, in addition to balancing exploration and 
exploitation when selecting design points, one must also consider this local-global tradeoff
\citep{StuartTeck2, gp_surrogates_random_exploration,emPostDens,SinsbeckNowak,Surer2023sequential,adaptiveMultimodal}.
The choice of design algorithm to navigate this tradeoff is problem-specific. For example, 
\citet{adaptiveMultimodal} propose an active learning strategy that specifically targets multimodal posteriors.
The choice will also be determined by the particular surrogate model being used.

The manner in which this tradeoff is navigated depends on the particular problem setup and 
the surrogate model itself. For example, \citet{adaptiveMultimodal} propose an active learning strategy 
targeting multimodal posteriors. Another promising line of research consists of adapting local approximations, 
avoiding the challenging task of fitting a single global surrogate \citep{Li_2014,ConradLocalExactMCMC}. 

In the following, we outline the general structure of an active learning algorithm and review design strategies 
that have been proposed for surrogate-based Bayesian inference. We place particular emphasis on 
the batch sequential design setting, where multiple design points are acquired simultaneously [\todo: use problem-solution here].
We conclude this section by briefly noting connections to so-called ``exact-approximate'' MCMC algorithms.

To remain agnostic to the quantity being emulated, we state algorithms with respect to
a generic underlying emulator $\funcEm[\Ndesign]$, which approximates some function
$\func$ (e.g., the forward model or log-likelihood).

\subsection{Sequential Design Loop}
Given a current surrogate fit to design inputs $\designIn[\Ndesign] \in \parSpace^{\Ndesign}$,
our goal is to select a new batch of inputs $\designBatchIn \in \parSpace^{\Nbatch}$
to form the augmented design $\designIn[\Naugment] \Def \designIn[\Ndesign] \cup \designBatchIn$.
After running the simulator at the new points $\designBatchIn$ and updating the emulator, we obtain
an updated posterior surrogate $\postEm[\Naugment]$. We therefore seek to select $\designBatchIn$
to yield the best possible improvement in the approximate posterior, which is naturally cast as 
an optimization problem with respect to some objective function $\acq[]: \parSpace^{\Nbatch} \to \R$, typically
called the \textit{acquisition function} or \textit{design criterion}:
\begin{equation}
\designBatchIn \in \argmin_{\parMat \in \parSpace^{\Nbatch}} \acq[](\parMat).
\label{eq:acq-opt}
\end{equation}
Depending on computational resources, this optimization can be repeated over a sequence of 
$\Nrounds$ rounds, yielding the sequential design loop summarized in \Cref{alg:seq-des-loop}.
\begin{algorithm}
    \caption{Sequential Design Loop}
    \label{alg:seq-des-loop}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Function{SeqDesign}{$\func, \funcPrior, \designIn[\Ndesign], \Nrounds$}     
    	\State $\hat{\func} \gets \mathrm{fit}(\funcPrior \given \designIn[\Ndesign], \func(\designIn[\Ndesign]))$
	\Comment{Fit to initial design}	
        \For{$\designIndex \gets 1, \dots, \Nrounds$} \Comment{Sequential design loop}
        		\State $\designBatchIn^{(\designIndex)} \gets \argmin_{\parMat \in \parSpace^{\Nbatch}} \acq[]^{(\designIndex)}(\parMat)$ 
		\State $\hat{f} \gets \mathrm{update}(\hat{f} \given \designBatchIn^{(\designIndex)}, \func(\designBatchIn^{(\designIndex)}))$
	\EndFor
	\EndFunction
    \end{algorithmic}
\end{algorithm}
We state the algorithm for a generic probabilistic model $\funcPrior$. 
This model is first fit to an initial design $\{\designIn[\Ndesign], \func(\designIn[\Ndesign])\}$ and then 
updated at each iteration $\designIndex$ by augmenting the existing design with newly 
acquired points $\{\designBatchIn^{(\designIndex)}, \func(\designBatchIn^{(\designIndex)})\}$.
\footnote{One could of course consider varying the batch size $\Nbatch$ across the $\Ndesign$ iterations but to simplify notation
we keep $\Nbatch$ constant.}
For GPs, these updates involve conditioning and potentially also re-fitting hyperparameters. 
The special case $\Nbatch = 1$ corresponds to the 
pure sequential design setting in which the function $\func$ is evaluated at each acquired point before
considering the subsequent acquisition. The more challenging batch sequential design setting
$\Nbatch > 1$ requires selection of inputs without observing the responses for the other points
in the batch. In practice, batch selection may be a necessity when simulations are costly but can 
be run in parallel. Though it accommodates batch acquisitions, \Cref{alg:seq-des-loop} is
still ``myopic'' in the sense that it disregards the potential for acquisitions in future rounds. 
Non-myopic strategies have been considered
through a dynamic programming lens, though they typically come at the cost of significant 
computational expense \citep{SURThesis, supermartingaleSUR}.

\subsection{Goal-Oriented Acquisition Functions}
We now make \Cref{alg:seq-des-loop} concrete by introducing several acquisition functions
that explicitly target the goal of posterior approximation. We draw a distinction between 
acquisition criteria restricted to the pure sequential ($\Nbatch = 1$) setting
and those naturally defined in the batch setting. It is still possible to perform 
batch acquisition with criteria of the former type, as discussed in \Cref{sec:greedy-opt}.

\subsubsection{Single Point Criteria} \label{sec:acq-single-point}
When optimizing for a single design point, a natural strategy is to simply select the input
where the uncertainty in $\postEm[\Ndesign](\Par)$ is largest. If predictive variance
is used as the measure of uncertainty, this yields the \textit{maximum variance} criterion
\begin{equation}
\acq[](\Par) \Def -\Var_{\Ndesign}[\postEm[\Ndesign](\Par)], \label{eq:acq-maxvar}
\end{equation}
which is negated to align with our convention of minimizing acquisition functions.
Alternative ``maximum uncertainty'' criteria can be defined by changing the measure
of uncertainty (e.g., variance, entropy, interquartile range) and the target quantity
(e.g., $\postEm[\Ndesign]$, $\funcEm[\Ndesign]$). For example, targeting the 
maximum variance of $\funcEm[\Ndesign](\Par)$ yields a classical criterion 
for exploring the parameter space \citep[Section 6.2.1]{gramacy2020surrogates}.
However, this criterion tends not to perform well in the Bayesian inference setting
since it does not take into account the magnitude of the posterior density. 
It may be that $\funcEm[\Ndesign](\Par)$ is highly uncertain, but 
$\postEm(\Par; \funcEm[\Ndesign])$ concentrates on a very small value.  

The predictive variance of $\postEm[\Ndesign](\Par)$
is given in closed-form in \Cref{eq:fwd-em-Gaussian,eq:llik-em-Gaussian} for 
forward model and log-density emulators in the Gaussian settings. For example, 
when $\lpostEm[\Ndesign](\Par) \sim \Gaussian(\emMean[\Ndesign]{\lpost}(\Par), \emKer[\Ndesign]{\lpost}(\Par))$
we have
\begin{equation}
\Var\left[\postEm[\Ndesign](\Par; \lpostEm[\Ndesign])\right] &= 
\left[\Exp{\emKer[\Ndesign]{\lpost}(\Par)} - 1\right] \Exp{2\emMean[\Ndesign]{\lpost}(\Par) + \emKer[\Ndesign]{\lpost}(\Par)},
\end{equation}
which demonstrates that the criterion favors inputs where both the expected log-posterior
$\emMean[\Ndesign]{\lpost}(\Par)$ and epistemic uncertainty $\emKer[\Ndesign]{\lpost}(\Par)$ are large, encoding an exploration-exploitation
tradeoff. The maximum variance criterion is utilized in \citet{Lueckmann2018LikelihoodfreeIW,Kandasamy_2017,AlawiehIterativeGP}. 
In the GP log-density emulator setting (\Cref{eq:llik-em-Gaussian})
$\postEm[\Ndesign](\Par)$ is log-normal. Given the heavy tails and asymmetry of a log-normal random variable, 
\citep{VehtariParallelGP,wang2018adaptive} argue that the variance provides a misleading measure
of uncertainty in this setting, instead favoring the entropy or interquartile range. 
As an alternative to these maximum uncertainty criteria, \citet{gp_surrogates_random_exploration}
suggest explicitly decoupling exploration and exploitation by selecting one point 
that maximizes $\emMean[\Ndesign]{\lpost}(\Par)$ and randomly sampling a second point from 
$\priorDens$. 

\subsubsection{Multipoint Criteria} \label{sec:acq-multipoint}
We next consider acquisition functions that can be evaluated at batches of multiple inputs. In particular,
we focus on a class of acquisition functions that we call \textit{expected conditional uncertainty (ECU)} 
criteria. Criteria falling within this general class have been explored in various contexts 
\citep{ALC,Roy2001,Mercer_kernels_IVAR,Binois_2018,deepGPAL,
ALExpErrReduction,parallelSURExcursionSet,SurerStochasticIVAR}
\footnote{Many names have been used for such criteria, including \textit{expected error reduction}, 
\textit{active learning Cohn}, \textit{integrated mean squared prediction error}, \textit{integrated variance}, 
and generically \textit{integral criteria}.},
but we focus on examples geared specifically towards surrogate-induced posterior approximation.

ECU acquisitions arise from the following logic: we would like to select an input batch 
$\parMat \in \parSpace^{\Nbatch}$ that induces an updated surrogate $\postEm[\Naugment]$ with the largest 
possible reduction in uncertainty on average over $\parSpace$. This uncertainty reduction is not, in general,
computable without observing the response $\func(\parMat)$. However, it can be approximated by modeling
$\func(\parMat)$ as a random vector $\gamma \sim \mathrm{law}(\funcEm[\Ndesign](\parMat))$. Let
$\funcEm[\Naugment]^{\parMat,\gamma} \Def \funcEm[\Ndesign] \given [\funcEm[\Ndesign](\parMat) = \gamma]$
denote $\funcEm[\Ndesign]$ conditioned on new data $\{\parMat, \gamma\}$. In computing ECU, we marginalize
over $\gamma$ in addition to averaging over $\parSpace$. If we again choose variance as our measure of 
uncertainty, we have
\begin{equation}
\acq[](\parMat) \Def 
\int_{\parSpace} \E_{\gamma} \Var_{\Naugment}[\postDens(\Par; \funcEm[\Naugment]^{\parMat,\gamma}) \given \gamma] \ \weightDens(\Par) d\Par,
 \label{eq:acq-intvar}
\end{equation}
where $\weightDens$ is a measure on $\parSpace$ that we are free to choose.
Notice that in contrast to the pointwise 
criteria, ECU acquisition functions are ``global'' in that they account for the effect of the 
new batch $\parMat$ on the surrogate uncertainty over the entire space $\parSpace$. Variations of 
ECU criteria can be created by changing the measure of uncertainty (e.g., variance, entropy, interquartile range),
the target quantity (e.g., $\postEm$, $\lpostEm$, $\funcEm$), and the weighting measure $\weightDens$.
For example, targeting the variance of $\funcEm$ with uniform weighting measure yields the
classical integrated mean squared prediction error \citep{Mercer_kernels_IVAR}.

Outside of special cases \citep{Binois_2018,MakTargetedVar,Koermer2024} the outer integral (over $\parSpace$) 
in ECU criteria is not tractable. This is especially the case in our present setting,
and thus we focus on the sample average approximation \citep{Mercer_kernels_IVAR,botorch},
\begin{align}
&\acq[](\parMat) \Def \frac{1}{J}
\sum_{j=1}^{J} \E_{\gamma} \Var_{\Naugment}[\postDens(\Par_j; \funcEm[\Naugment]^{\parMat,\gamma}) \given \gamma],
&&\Par_j \overset{\mathrm{iid}}{\sim} \weightDens[\Ndesign].
 \label{eq:acq-intvar-saa}
\end{align}
The $\Par_j$ are sampled at the beginning of the sequential design round and then fixed, so that
\Cref{eq:acq-intvar-saa} is viewed as a deterministic objective function. 
The expected variance terms in the summands admit closed forms in the Gaussian 
settings of \Cref{eq:fwd-em-Gaussian,eq:llik-em-Gaussian}, as shown below. As an alternative to 
\Cref{eq:acq-intvar-saa}, we can consider an ECU criterion that targets uncertainty in the underlying
emulator $\funcEm[\Ndesign]$, but acknowledges the goal of posterior approximation through the choice
of $\weightDens[\Ndesign]$; i.e., 
\begin{align}
&\acq[](\parMat) \Def \frac{1}{J}
\sum_{j=1}^{J} \Var_{\Naugment}[\funcEm[\Naugment](\Par_j)],
&&\Par_j \overset{\mathrm{iid}}{\sim} \postNormEm[\Ndesign]^{\mathrm{approx}},
 \label{eq:acq-intvar-lartaud-saa}
\end{align}
where $\postNormEm^{\mathrm{approx}}$ is any of the approximate posteriors from \Cref{sec:post-approx}.
An acquisition function of this form is proposed in \citet{weightedIVAR}, and compares 
favorably against \Cref{eq:acq-intvar-saa}.

\begin{examplebox}[Gaussian Forward Model Emulation Setting]
In the Gaussian forward model emulation setting from \Cref{eq:fwd-em-Gaussian-post,eq:fwd-em-Gaussian-em}, 
the the ECU-variance criterion in \Cref{eq:acq-intvar-saa} reduces to
\begin{align}
\acq[](\parMat) &= \frac{1}{J}
\sum_{j=1}^{J} \priorDens^2(\Par_j) \bigg[\frac{\Gaussian\left(\obs \given \emMean{\fwd}(\Par_j), \CovComb(\Par_j) - \frac{1}{2}\likPar \right)}{2^{\dimObs/2} \det(2\pi \likPar)^{1/2}} - \label{eq:acq-intvar-saa-fwd} \\
&\qquad\qquad \frac{\Gaussian\left(\obs \given \emMean{\fwd}(\Par_j), \CovComb(\Par_j) - \frac{1}{2}\CovComb[\Naugment](\Par_j) \right)}{2^{\dimObs/2} \det(2\pi \CovComb[\Naugment](\Par_j))^{1/2}} \bigg], \nonumber
\end{align}
where $\CovComb(\Par) \Def \likPar + \emKer{\fwd}(\Par)$ and $\CovComb[\Naugment](\Par) \Def \likPar + \emKer[\Naugment]{\fwd}(\Par)$.  
\citet{SinsbeckNowak} are the first to propose an ECU-variance criterion in the forward model emulation setting,
and briefly note the above closed-form expression in the Gaussian special case.
\citet{Surer2023sequential} also derive this expression, but in the particular case of the multi-output basis vector model
in \Cref{basis_func_model}.

\citet{weightedIVAR} consider an alternative ECU-variance criterion in the Gaussian forward model emulator setting:
\begin{align}
&\acq[](\parMat) \Def \frac{1}{J}
\sum_{j=1}^{J} \emKer[\Naugment]{\fwd}(\Par_j),
&&\Par_j \sim \postApproxNormMarg.
 \label{eq:acq-intvar-lartaud}
\end{align}
The summands target uncertainty in $\fwdEm[\Ndesign]$ rather than $\postEm[\Ndesign]$, while the weighting measure
$\weightDens = \postApproxNormMarg$ is chosen to target the goal of posterior approximation. This has the 
benefit of being much easier to compute, since the GP variance $\emKer[\Naugment]{\fwd}(\Par)$ is independent of the 
unseen response $\fwd(\parMat)$. \citet{weightedIVAR} establish asymptotic convergence results for this criterion
using the stepwise uncertainty reduction framework \citep{BectSUR}. 
\end{examplebox}

\begin{examplebox}[Gaussian Log-Density Emulation Setting]
Consider the Gaussian log-density emulation setting from \Cref{eq:llik-em-Gaussian}
with a log-likelihood emulator. With this setup, the ECU-variance criterion in 
\Cref{eq:acq-intvar-saa} reduces to
\begin{equation}
\acq[](\parMat) = \frac{1}{J}
\sum_{j=1}^{J} \Var\left[\postEm(\Par_j) \given \llikEm(\parMat) = \emMean[\Ndesign]{\llik}(\parMat) \right] \varInflation_{\Ndesign}(\Par_j; \parMat)   
\label{eq:acq-intvar-saa-ldens}
\end{equation}
where
\begin{align*}
\Var\left[\postEm(\Par) \given \llikEm(\parMat) = \emMean[\Ndesign]{\llik}(\parMat) \right]
&= \priorDens(\Par)^2 \Exp{2\emMean[\Ndesign]{\llik}\left(\Par\right) + \emKer[\Naugment]{\llik}(\Par)} \cdot \\
&\qquad\qquad\qquad \left[\Exp{\emKer[\Naugment]{\llik}(\Par)} - 1 \right] \\
\varInflation_{\Ndesign}(\Par; \parMat)
&= \Exp{2\left(\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)\right)}.
\end{align*}

Notice that the first term in \Cref{eq:acq-intvar-saa-ldens} is the variance of $\postEm[\Ndesign](\Par_j)$ conditioned on
$\{\parMat, \emMean[\Ndesign]{\llik}(\parMat)\}$; i.e., the current GP prediction is treated as if it were the true response.
The second term accounts for the uncertainty in the true response; in particular, the penalty
$\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)$ is large when the input batch $\parMat$ is highly 
``influential.''

\Cref{eq:acq-intvar-saa-ldens} is derived in \citet{VehtariParallelGP}, but the authors ultimately recommend
an alternative criterion that uses the interquartile range in place of the variance. The interquartile range
is more robust to the heavy tails of the log-normal predictive distribution, a benefit that translates to 
improved empirical performance in their experiments.
\end{examplebox}

\subsection{Batch Design Strategies}
When simulation cost is high but runs can be parallelized, batch acquisition is often essential. For example, in large-scale
geoscience applications the batch size may be in the hundreds ($\Nbatch \approx 100$), with only a few rounds 
of acquisition possible ($\Nrounds < 10$) \citep{FerEmulation}. This is in stark contrast to much of the sequential 
design literature, which focuses on the behavior of single-point acquisition algorithms as $\Nrounds$ grows large.
The challenge of batch acquisition has gained interest in the active learning and Bayesian 
optimization literature \citep{Ginsbourger2010,LOEPPKY20101452,Chevalier2013,batchBOThesis}. We begin 
by highlighting ideas from this broader literature, and then discuss some promising approaches
well-suited to the particular goal of posterior approximation.

\subsubsection{Joint Optimization}
The task of minimizing $\acq[](\parMat)$ requires an optimization over $\dimPar\Nbatch$ variables, and is thus
difficult to scale as the parameter dimension or batch size increases. \citet{Mercer_kernels_IVAR} investigate
the performance of gradient-based continuous optimization over $\parSpace^\Nbatch$ for integrated variance
criteria, demonstrating favorable performance for small values of $\dimPar$ and $\Nbatch$. 
\citep{botorch} describes the gradient-based optimization framework using sample average approximations
implemented in the \verb+BoTorch+ package. A large portion of the literature eschews continuous optimization in 
favor of discrete methods. These algorithms take the form of a subset selection problem; the optimal batch
is sought from within a finite set of candidate points $\parMatCand \subset \parSpace$. This presents a 
challenging combinatorial optimization problem, commonly solved by employing stochastic exchange 
algorithms (i.e., Federov exchange) to find a local minimum \citep{FederovExchange,WynnDiscreteExchange,LOEPPKY20101452}
\footnote{Exchange algorithms can also be employed to optimize over continuous space and leverage gradient information.}.

\subsubsection{Greedy Optimization} \label{sec:greedy-opt}
Motivated by the difficulty of joint optimization, greedy approximations are common in practice \citep{Ginsbourger2010}.
These methods seek to approximate the joint optimum using a sequence of $\Nbatch$ simpler optimization 
problems over $\parSpace$. Under this approach, at the beginning of round $\designIndex$ the first 
point in the batch $\Par^{(\designIndex,1)}$ is selected by optimizing $\acq[]^{(\designIndex)}(\Par)$ as 
in the pure sequential design setting. The remaining $\Nbatch - 1$ points are added sequentially via
\begin{align}
&\Par^{(\designIndex, b)} \Def \argmin_{\Par \in \parSpace} \acq[]^{(\designIndex,b)}(\parMat^{(\designIndex,1:b-1)} \cup \{\Par\}),
&&b = 2, \dots, \Nbatch. 
\label{eq:greedy-acq-opt}
\end{align}
In pure sequential design, the acquisition function changes every iteration owing to the addition of a new
observation $(\Par, \func(\Par))$. Greedy batch design differs in that the criterion is updated to reflect
the new input $\Par$ only, as we do not observe $\func(\Par)$ until the entire batch is acquired.
The superscript in $\acq[]^{(\designIndex,b)}$ indicates that the objective is approximated to account for
these unobserved responses. \citep{Ginsbourger2010} introduce several heuristics for this approximation.
At each iteration, the \textit{Kriging believer} strategy updates the surrogate with a pseudo-observation;
the current emulator predictive mean is used in place of the truth $\func(\Par^{(\designIndex, b)})$.
Believing the surrogate prediction over a sequence of iterations can be problematic, so the popular 
\textit{constant liar} alternative consists of fixing the pseudo-observation to a constant value throughout the 
entire round. \citep{Ginsbourger2010} refer to this constant value as a ``lie'' and investigate the empirical 
performance of different choices for the lie. A related alternative consists of using the predictive mean of 
$\funcEm[\Ndesign]$ to generate the pseudo-observation over the course of the round, rather than using the 
updated predictive mean function after each iteration as in the Kriging believer approach \citep{VehtariParallelGP}.
Both \citep{VehtariParallelGP,Surer2023sequential} consider such greedy heuristics in the context of
solving a Bayesian inverse problem.

An unappealing aspect of the greedy approach is that the potential for batch acquisitions to assess within-batch
interactions is not fully leveraged. In order to make better use of a batch acquisition, one idea consists of generating
multiple candidate batches of points and then using the batch criterion to select among the competitors. This is
exemplified by the \textit{constant liar mix} strategy, where the candidate batches are generated using the constant
liar heuristic with different choices of the lie \citep{Chevalier2013}.
Finally, note that since the optimization in \Cref{eq:greedy-acq-opt} proceeds one point at a time then the single point criteria
in \Cref{sec:acq-single-point} can be combined with greedy heuristics to enable batch optimization (e.g., \citet{VehtariParallelGP}).

\subsubsection{Sampling-Based Approaches}
The batch design methods discussed above are generic active learning techniques, not unique to our particular
context. However, the specific goal of posterior approximation suggests a natural alternative: select the batch by sampling from the 
current approximate posterior. \citep{hydrologicalModel,quantileApprox} both adopt this strategy.
\citet{FerEmulation} instead sample from a mixture of the current approximate posterior and the prior, with the mixture weights 
representing a user-defined tuning parameter. \citet{adaptiveMultimodal} samples from the current approximate posterior, and
then updates these samples with an iteration of an ensemble Kalman algorithm; this method is 
closely related to the \textit{calibrate, emulate, sample} workflow \citep{CES}.

These sampling-based approaches enjoy the practical benefits of being inherently parallel and not requiring the solution
of a difficult optimization problem. In this sense, they are similar in spirit to Thompson sampling for
batch Bayesian optimization \citep{parallelBOThompson}. We conjecture that such approaches may be superior to 
alternatives in the large $\Nbatch$, small $\Nrounds$ setting. We expect the performance of this approach to be closely 
tied to the choice of approximate posterior; e.g., samples from the plug-in mean posterior will likely under-explore the 
parameter space. The \citet{FerEmulation} approach of sampling from a mixture of the prior and approximate posterior might
be viewed as a more scalable analog of the method in \citet{gp_surrogates_random_exploration}, in the sense of 
explicitly decoupling exploration and exploitation. We believe further work is warranted to better understand the 
behavior of these methods, and their applicability to large-scale batch design.

\subsection{Exact-Approximate MCMC}
Throughout this section, we have framed the question of surrogate refinement through a standard active 
learning lens, in which the selection of design points proceeds over a sequence of rounds. We briefly
highlight related methods that stray from this framework; in particular, algorithms that 
perform sequential design within a single MCMC run, often with the goal of converging 
to the true target $\postDensNorm$. Such methods are sometimes referred to as \textit{exact-approximate}
MCMC. The papers \citet{Li_2014,ConradLocalExactMCMC} 
adaptively construct local polynomial surrogates within an MCMC algorithm, and are able to establish asymptotic 
exactness by showing the polynomial approximation error diminishes over time.
\citep{ActiveLearningMCMC}
also conduct sequential design within a MCMC run, using emulator predictions at points where the 
surrogate is confident, and running exact simulations at points that exceed a user-defined uncertainty tolerance.
We note that it is possible to ensure convergence to $\postDensNorm$ even with a fixed surrogate
by using the surrogate predictions to filter out poor proposals, avoiding the cost of unnecessary 
expensive model runs. This idea dates back to the delayed acceptance mechanism of \citep{DelayedAcceptance}
for Metropolis-Hastings algorithms.
In \citet{MCMC_GP_proposal}, a similar approach is used to accelerate Hamiltonian Monte Carlo.
These algorithms all leverage a surrogate in various ways with the goal of reducing the simulation load, while
maintaining asymptotic exactness. The cost of exactness is the requirement of more simulator runs, often 
performed serially. We therefore view such approaches as well-suited to inverse problems with moderately 
expensive forward models. Convergence to $\postDensNorm$ will typically be impractical as the computational
costs increase, especially in the large $\Nbatch$, small $\Nrounds$ setting.

\section{Practical Recommendations} \label{sec:recs}
We now provide a set of practical recommendations for building surrogate models, constructing
posterior approximations, and sequentially refining the surrogate. \Cref{sec:case-study}
highlights these recommendations through a numerical case study.

\begin{rec} \label{rec:prop-uncertainty}
The choice of surrogate model and uncertainty propagation method should be conducted jointly.
\end{rec}

In general, we emphasize the importance of propagating surrogate uncertainty in 
the posterior approximation, but are not convinced of a single ``correct'' uncertainty 
propagation method. Instead we find a few different approaches to be reasonable, but
emphasize that the empirical success of any particular method will be closely tied to the
surrogate model itself. The expected posterior approximation is conceptually appealing
as a direct summary of the random posterior $\postNormEm[\Ndesign]$, but is often 
challenging to implement and computationally expensive. 
Therefore, we tend to prefer the noisy MCMC methods defined in \Cref{sec:MH-approx},
which show empirical promise, require only a single MCMC run, and are applicable to 
generic probabilistic surrogates.
However, in choosing these methods one needs to appreciate how the behavior of the posterior
approximation will vary based on the properties of the surrogate predictive distribution.
For example, in log-density emulation the expected likelihood approximation can concentrate in very 
small regions of parameter space where the surrogate is uncertain, in which case a pseudo-marginal 
MCMC run will almost certainly fail. However, in this case we have found that the simple adjustment 
of incorporating a bound constraint in the emulator can solve this problem and produce reasonable 
results. Alternatively, the E-acc method tends to be more robust with respect to 
heavy-tailed surrogate predictions, but requires stronger requirements to ensure 
existence (\Cref{sec:existence}). 

\begin{rec} \label{rec:multiscale}
Treat density emulation as a multiscale problem.
\end{rec}

In many applications, it is common for log-likelihoods and log-posteriors to exhibit various
properties that render log-density emulation a challenging task. They tend to have very 
large dynamic ranges over the support of the prior distribution, even when their range 
may be relatively small over the posterior support. In addition, their values may drop sharply 
in certain directions while asymptoting in others. In general, we find that traditional stationary 
surrogates perform quite poorly in approximating such response surfaces.
\todo: finish this recommendation

\begin{rec} \label{rec:oversample-tails}
The design should oversample the tails.
\end{rec}
As discussed in \Cref{sec:seq-design}, it is natural to aim to place design points in regions
where $\postDensNorm(\Par)$ is large. However, it is often critical to also include sufficient
points in regions where $\postDensNorm(\Par)$ is small, and at least initially, where 
$\priorDens(\Par)$ is small as well. Intuitively, such points are required to 
learn the global trend of the response surface (see \Cref{rec:multiscale}) and to ``pin down''
regions of negligible posterior mass. In constructing the initial design, we often find it helpful
to sample design points from an over-dispersed version of $\priorDens$. In the 
case study in \Cref{sec:case-study} we sample the initial design from the prior, but also
explicitly include additional points in the tails of the prior. We also suggest that sequential
acquisition schemes are designed to ensure sufficient exploration. This can be encoded
in an acquisition function (e.g., via the choice of $\weightDens[\Ndesign]$ in \Cref{eq:acq-intvar})
or via the explicit selection of exploratory points \citep{gp_surrogates_random_exploration}.  
For example, at each active learning iteration \citet{gp_surrogates_random_exploration,FerEmulation}. 
We note that these recommendations are in line with the theoretical results of \citet{StuartTeck2}, who 
suggest designs based on an over-dispersed version of $\postDensNorm(\Par)$.
The notion of oversampling the tails has also been proposed to construct designs
for numerical integration \citep{briol2017sampling}.

\begin{rec} \label{rec:pred-check-tails}
Conduct goal-oriented predictive checks of the induced posterior density surrogate.
\end{rec}
As discussed in \Cref{sec:existence}, one must take care in designing emulators that
induce well-defined posterior approximations. In MCMC-based inference, this is crucial for
avoiding divergent MCMC chains. \Cref{rec:multiscale,rec:oversample-tails}
offer modeling suggestions to help avoid such issues, but they do not provide certified 
guarantees. We suggest an explicit prior predictive check to diagnose problematic 
tail behavior in the induced posterior surrogate $\postEm[\Ndesign]$. A simple graphical
check consists of plotting an upper quantile of $\mathrm{law}(\lpostEm[\Nesign](\Par))$ 
as $\Par$ varies over the parameter space. One should ensure that $\Par$ is allowed to 
vary well past the extent of the design points in order to assess the extrapolation behavior 
of the surrogate. A high quantile (say, 95\% or 99\%) is chosen to assess whether 
the distribution of $\lpostEm[\Ndesign](\Par)$ appears to be converging almost surely 
to $-\infty$ in the limit. For multidimensional input spaces we suggest varying one 
parameter at a time, projecting onto different fixed values of the other parameters.
Performing this simple check does not require any additional 
evaluations of the expensive simulator, and can prevent wasting resources by having
to wait to uncover issues with the surrogate during inference (for example, 
\citet{emPostDens} use divergent MCMC chains as a sort of surrogate diagnostic check).
 See \Cref{fig:vsem-prior-check-lpostem} in the below case study for an example
 of this predictive check.

\begin{rec} \label{rec:bound-constraints}
Enforce bound constraints.
\end{rec}

\todo: Recommend rectified adjustment.

\begin{rec} \label{rec:multimodal-post-approx}
Embrace multi-modal posterior approximations.
\end{rec}

In utilizing a flexible statistical model to approximate a complex response surface, one expects
the approximation to feature local modes. Given that the surface corresponds to a
log-posterior density in this setting, it is natural to anticipate that even a deterministic emulator
may induce multimodal posterior approximations, even when the true posterior is unimodal. 
Multimodality is even more common when propagating surrogate uncertainty, as the uncertainty 
level will vary over the input space (e.g., consider the ``sausage-shaped'' confidence 
bands in \Cref{fig:em_dist_1d}). For example, modes in the approximate posterior may reflect
that the surrogate expects high posterior mass in a certain region, or that the surrogate is simply 
uncertain in the region. This uncertainty quantification is essential for acknowledging surrogate 
inadequacies and for guiding future improvements (\Cref{sec:seq-design}). However, it does
present computational challenges in performing inference. While specialized algorithms 
can be employed \citep{adaptiveMultimodal}, we typically prefer practical heuristic approaches
in the spirit of \citep{multimodalYao}. In particular, we run multiple MCMC chains in parallel with 
the aim of characterizing the dominant modes of the distribution. We perform within-chain 
convergence diagnostics to validate that the chains are well-mixed locally, and then 
utilize heuristics to assign weights to each chain.

\vspace

\todo: Add active learning recommendation

\section{Numerical Case Study} \label{sec:case-study}
We present a numerical illustration of a surrogate-assisted Bayesian inference workflow, highlighting
the practical recommendations given in the previous section.
Our illustrative example is motivated by the problem of producing near-term forecasts of the 
terrestrial carbon cycle \citep{nearTermForecasts,FerEmulation}. In this setting, both model parameters 
and initial conditions are typically unknown and must be learned from observational data. Performing 
such parameter estimation runs into computational challenges for large-scale land surface models, 
underscoring the potential for surrogates in this domain \citep{paramLSM}. 

\subsection{Model Setup}
We start by introducing the mechanistic simulator model, and then define a Bayesian inverse 
problem with respect to this model.

\subsubsection{Mechanistic Model}
As a simple concrete example, we utilize the \textit{Very Simple Ecosystem Model} (VSEM), a toy model capturing the basic 
structure of more complex land surface models, thus ideally suited for algorithm evaluation \citep{vsem}. The model 
describes the evolution of the state vector
\begin{align*}
\state(\Time) \Def [\stateV(\Time), \stateR(\Time), \stateS(\Time)]^\top \in \R_{\geq 0}^{3}, 
\end{align*}
with the state variables representing the quantity of carbon (\textrm{kg C/$m^2$}) in above-ground vegetation, below-ground 
vegetation (roots), and soil reservoirs, respectively. The VSEM model is given by the system of coupled 
ordinary differential equations
\begin{align}
\dstateV(\Time) &= \alphaV \NPP(\stateV(\Time), \forcing(\Time)) - \frac{\stateV(\Time)}{\tauV} \\
\dstateR(\Time) &= (1.0 - \alphaV) \NPP(\stateV(\Time), \forcing(\Time)) - \frac{\stateR(\Time)}{\tauR} \nonumber \\ 
\dstateS(\Time) &= \frac{\stateR(\Time)}{\tauR} + \frac{\stateV(\Time)}{\tauV} - \frac{\stateS(\Time)}{\tauS}, \nonumber
\end{align}
where the model forcing $\forcing(\Time)$ is provided by photosynthetically active radiation 
(\textrm{MJ/$m^2$/day}), and the dynamics rely on the following parameterized model of 
Net Primary Productivity (NPP; \textrm{kg C/$m^2$/day}),
\begin{align}
\NPP(\stateV, \forcing) &= (1 - \fracRespiration) \GPP(\stateV, \forcing) \\
\GPP(\stateV, \forcing) &= \forcing \cdot \LUE \cdot \left[1 - \exp\left\{-\KEXT \cdot \LAI(\stateV) \right\} \right] \nonumber \\
\LAI(\stateV) &= \LAR \cdot \stateV, \nonumber
\end{align} 
where $\GPP(\stateV, \forcing)$ and $\LAI(\stateV)$ model Gross Primary Productivity (GPP; \textrm{kg C/$m^2$/day})
and Leaf Area Index (LAI; \textrm{$m^2/m^2$}), respectively.
Note that the ODE is of the form \ref{ode_ivp}, with the caveat that the dynamics $\odeRHS$ additionally depend on a 
time-dependent forcing $\forcing(\Time)$. Given a value of $\Par$, we numerically solve the ODE at a daily time step
via the basic Euler scheme implemented in the R \verb+BayesianTools+ package \citep{vsem}. We recall the discretized 
parameter-to-state map 
$\solutionOp: \Par \mapsto \left[\indexTime[0]{\state}(\Par), \dots, \indexTime[\NTimeStep]{\state}(\Par) \right]^\top$, 
as defined in \Cref{eq:ode-solution-op}.

Potential calibration parameters in this model include 
$\{\alphaV, \tauV, \tauR, \tauS, \LUE, \KEXT, \fracRespiration, \LAR\}$, as well as the initial conditions for the three state
variables $\{\stateV(0), \stateR(0), \stateS(0)\}$.
Noting that the model is over-parameterized, we will focus on estimating 
$\Par \Def \{\KEXT, \fracRespiration, \tauV, \stateV(0)\}$ with the remaining parameters held fixed.

\subsubsection{Statistical Model}
We consider estimating the parameters $\Par$ given noisy monthly averages of LAI over a two year period.
We assume an additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{eq:vsem-inv-prob} \\
\noise &\sim \Gaussian(0, \sigma^2 I) \nonumber 
\end{align}
where $\fwd: \parSpace \to \R^{24}$ maps to the twenty-four LAI averages; i.e., the first entry of $\fwd(\Par)$
is given by 
\begin{equation}
\fwd_{1}(\Par) \Def \frac{\LAR}{30} \sum_{\timeIndex=1}^{30} \state_{\textrm{v},\timeIndex}(\Par),
\end{equation}
and the remaining entries simply change the indices of the summation. For simplicity, we fix $\sigma^2$
and assume a priori independence over the entries of $\Par$. Synthetic data $\obs$ is simulated
using \Cref{eq:vsem-inv-prob} with fixed ground truth values $\{\Par_{\star}, \sigma^2_{\star}\}$. The same 
model is used when solving the inverse problem, but the values of the fixed parameters (those not
being estimated) and the noise variance are changed, implying the presence of parametric misspecification.
Table \todo compares these misspecified values relative to the ground truth, while
table \todo summarizes the prior distribution $\Par \sim \priorDens$. As a baseline for comparison, we 
draw samples from the posterior using exact MCMC. \Cref{fig:vsem_prior_post} compares the prior 
and posterior marginal distributions, while \Cref{fig:vsem_pred_dists} compares the prior and 
posterior predictive distributions over LAI trajectories.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.24\textwidth} % Cv 
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_post_par_Cv.png}}
         \caption{$\stateV(0)$}
         \label{fig:vsem_prior_post_Cv}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth} % GAMMA
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_post_par_GAMMA.png}}
         \caption{$\gamma$}
         \label{fig:vsem_prior_post_GAMMA}
     \end{subfigure}
      \begin{subfigure}[b]{0.24\textwidth} % tauV
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_post_par_tauV.png}}
         \caption{$\tauV$}
         \label{fig:vsem_prior_post_tauV}
     \end{subfigure}
          \begin{subfigure}[b]{0.24\textwidth} % KEXT
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_post_par_KEXT.png}}
         \caption{$\KEXT$}
         \label{fig:vsem_prior_post_KEXT}
     \end{subfigure}
        \caption{Marginal prior (black dashed) and exact posterior (red) distributions for the four
        calibration parameters.}
        \label{fig:vsem_prior_post}
\end{figure}


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.48\textwidth} % Prior predictive distribution
         \centering
         \includegraphics[width=\textwidth]{{vsem/prior_pred_dist.png}}
         \caption{Prior Predictive}
         \label{fig:vsem_prior_pred}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth} % Posterior predictive distribution
         \centering
         \includegraphics[width=\textwidth]{{vsem/post_pred_dist.png}}
         \caption{Posterior Predictive}
         \label{fig:vsem_post_pred}
     \end{subfigure}
        \caption{(Left) The distribution over LAI trajectories induced by $\Par \sim \priorDens$. 
        The red points are the observed noisy observations of monthly LAI averages, with the 
        vertical bars indicating $\pm \sigma$ observation noise. The blue shaded region captures 
        90\% prior predictive probability. The gray lines are prior predictive samples, and the black 
        line is the ground truth LAI trajectory used to generate the data. (Right) The analogous plot
        for the posterior predictive distribution; i.e., the distribution over LAI trajectories induced 
        by $\Par \sim \postDens$.}
        \label{fig:vsem_pred_dists}
\end{figure}

\subsection{Initial Surrogate Fits}
For this problem, we compare GP emulators for both the forward model and log-posterior.
Let $\funcEm[\Ndesign]$ denote the underlying GP prediction of $\func$.
To evaluate these emulators we consider both prior and posterior averaged continuous
ranked probability score (CRPS; \citep{scoringRules})
\begin{align}
\mathrm{crps}(\funcEm[\Ndesign]) &\Def \int_{\parSpace} \mathrm{crps}(\funcEm[\Ndesign](\Par), \func(\Par)) \weightDens[](\Par) d\Par
\end{align}
and multivariate log-score (i.e., predictive deviance; \citep{scoringRules})
\begin{align}
\mathrm{logS}(\funcEm[\Ndesign]) 
&\Def \log \Gaussian(\func(\parMat) \given \emMean[\Ndesign]{}(\parMat), \emKer[\Ndesign]{}(\parMat)),
&&\parMat \overset{\mathrm{iid}}{\sim} \weightDens[].
\end{align}

The choice $\weightDens[] \in \{\priorDens, \postDensNorm\}$ determines whether error is assessed with
respect to the prior or true posterior; both scores are approximated by sampling $1000$ points 
from the relevant distribution. We similarly estimate 
\begin{align}
\mathrm{mae}(\postEm[\Ndesign]) &\Def 
\int_{\parSpace} \abs{\postApproxMean[\Ndesign](\Par) - \postDens(\Par)} \weightDens[](\Par) d\Par,
\end{align}
to evaluate the quality of the plug-in mean posterior approximation.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth} % lpost em, prior evaluation
         \centering
         \includegraphics[width=\textwidth]{{vsem/pred_scatter_prior_lpostem.png}}
         \caption{$\lpostEm[\Ndesign], \weightDens[] = \priorDens$}
         \label{fig:vsem_pred_scatter_prior_lpostem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth} % lpost em, posterior evaluation
         \centering
         \includegraphics[width=\textwidth]{{vsem/pred_scatter_post_lpostem.png}}
         \caption{$\lpostEm[\Ndesign], \weightDens[] = \postDensNorm$}
         \label{fig:vsem_pred_scatter_post_lpostem}
     \end{subfigure}
          \begin{subfigure}[b]{0.49\textwidth} % fwd em, prior evaluation
         \centering
         \includegraphics[width=\textwidth]{{vsem/pred_scatter_prior_lpostem.png}}
         \caption{$\fwdEm[\Ndesign], \weightDens[] = \priorDens$}
         \label{fig:vsem_pred_scatter_prior_fwdem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth} % fwd em, posterior evaluation
         \centering
         \includegraphics[width=\textwidth]{{vsem/pred_scatter_post_lpostem.png}}
         \caption{$\fwdEm[\Ndesign], \weightDens[] = \postDensNorm$}
         \label{fig:vsem_pred_scatter_post_fwdem}
     \end{subfigure}
        \caption{Emulator predictions based on the initial design. The top and bottom rows correspond
        to the forward model and log-posterior emulator, respectively. The left and right columns correspond
        to evaluation inputs sampled from the prior and exact posterior, respectively. The points summarize
        emulator mean predictions, while vertical bars are 90\% emulator predictive intervals. The orange 
        bars indicate that the 90\% interval does not contain the truth.}
        \label{fig:vsem_pred_scatter}
\end{figure}

\subsubsection{Forward Model Emulator}
\subsubsection{Log-Posterior Emulator} \label{sec:case-study-lpost-em}
This inverse problem presents several challenges for log-density emulators: the dynamic 
range of the log-likelihood over the prior support is quite large, the parameter space is 
unbounded, and the likelihood remains relatively flat along certain directions in parameter
space. One must therefore take care to control the tails of the emulator in order to produce well-defined
posterior approximations. This situation is not uncommon in applications, as it may simply reflect the 
fact that the simulator predictions asymptote in certain directions of parameter space. 
In general, we find a stationary GP performs quite poorly in such contexts. One could argue that 
the placement of design points in the tails of the prior distribution could alleviate this issue, 
as it would force the GP predictions downward. Although the GP would revert to its prior
as the distance from the design points increases, the hope is  
that an MCMC algorithm would never reach such regions, as the design points in the tail
would effectively create a ``moat'' of low probability.
Moreover, if emulating the log-likelihood then one could also rely on the prior to drive the posterior 
density emulator to zero.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth} % tauV
         \centering
         \includegraphics[width=\textwidth]{{vsem/extrap_q95_lpostem_tauV.png}}
         \caption{$\tauV$}
         \label{fig:vsem-prior-check-lpostem-tauv}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth} % Cv
         \centering
         \includegraphics[width=\textwidth]{{vsem/extrap_q95_lpostem_Cv.png}}
         \caption{$\stateV(0)$}
         \label{fig:vsem-prior-check-lpostem-cv}
     \end{subfigure}
        \caption{The prior predictive check recommended in \Cref{rec:pred-check-tails} 
        for the log-posterior emulator, focusing on the two parameters with unbounded support. 
        The lines correspond to the $95^{\mathrm{th}}$ percentile of the log-posterior surrogate 
        $\mathrm{Quantile}_{0.95}(\lpostEm[\Ndesign](\Par))$ as only one parameter in $\Par$ 
        is varied. Different lines correspond to different fixed values of the remaining parameters,
        which have been sampled from $\priorDens$. Dashed vertical lines indicate the extent of 
        the design points in that dimension, with inputs outside of these bounds representing
        pure extrapolation.}
        \label{fig:vsem-prior-check-lpostem}
\end{figure}

\subsection{Active Learning}
\todo: note that different active learning algorithms can be combined with different acquisition 
functions to define many algorithms. Mention this is too many combs to test, so we instead
first run active learning algorithms, then focus on the best performing algorithm to compare
the posterior approximation.



\section{Related Work and Extensions} \label{sec:related-work}

\subsection{Computer Model Calibration} \label{sec:computer-model-calibration}
The challenge of emulating a black-box computer model has received widespread attention 
well beyond the scope of Bayesian inference. The design of surrogate models for Bayesian 
inverse problems may be informed by the vast literature from the computer experiments, 
engineering, applied math, machine learning, and statistics communities. As a starting point, 
we refer readers to \citet{gramacy2020surrogates,design_analysis_computer_experiments,SanterCompExp,UQpredCompSci} 
and the references therein. It is worth taking a moment to clarify the scope of our review 
with respect to related work in the computer experiments literature in particular.
Indeed, early work in this field addressed the challenge of learning 
model parameters  $\Par$ from observational data via the use of a surrogate for $\fwd$, a problem 
commonly referred to as \textit{computer model calibration} (see \citet{computerModelCalibrationReview}
for a review). When cast as a problem of Bayesian inference, the calibration problem falls within the 
framework considered in this article. However, much of this early calibration work focused on 
the added challenge of learning a discrepancy term between the computer model $\fwd$ and the 
true underlying system \citep{ModelDiscrepancy,emPostDens,OakleyllikEm}. 
For example, the pioneering work \citet{KOH} considers jointly learning 
a forward model emulator, calibration parameters, and a discrepancy function within a single 
Bayesian model. By contrast, we do not consider discrepancy modeling in this review, and moreover
focus on the alternative modular workflow, where the surrogate is fit offline 
without seeing the calibration data $\obs$ \citep{modularization}. 
\footnote{This point is specific to the forward model emulation setting. Log-density emulators 
do depend on $\obs$, since the log-likelihood is a function of the data.}
This modular two-stage approach naturally leads to 
the question of how to propagate the surrogate uncertainty in the calibration stage, which is one 
of the central questions motivating this review.

\subsection{Probabilistic Numerics} \label{sec:prob-numerics}
See Sullivan, Hennig papers; and Teckentrup random forward models paper section 5

\subsection{Stochastic Simulators and Simulation-Based Inference} \label{sec:sbi}
\citet{Lueckmann2018LikelihoodfreeIW,VehtariParallelGP,ABCApproxLik,
BurknerAmortized,llikEmABC,ABCGP}

\subsection{Other}
Multifidelity methods? Active subspaces? Data subsampling/randomized misfits?

\section{Conclusion} \label{sec:conclusion}


% Appendix 
\section{Appendix}

\subsection{Surrogate-Induced Posterior Distributions}
This section proves \Cref{eq:fwd-em-Gaussian,eq:llik-em-Gaussian}, which characterize the distribution 
of $\postEm[\Ndesign]$ in the Gaussian forward model and log-density emulation settings, respectively.
The latter result is immediate, but the former requires proving some preliminary Gaussian identities.

\subsubsection{Forward Model Emulator}
We start by deriving the convolution of two Gaussians.

\begin{prop} \label{Gaussian_convolution}
Let $\Gaussian(A \mu, \likPar)$ and $\Gaussian(m, C)$ be Gaussian distributions on $\R^{\dimObs}$ and $\R^{\dimPar}$, 
respectively, with $A \in \R^{\dimObs \times \dimPar}$ and $\likPar, C$ symmetric, positive definite matrices. Then 
\begin{align*}
\int_{\R^{\dimPar}} \Gaussian(\obs \given A \mu, \likPar) \Gaussian(\mu \given m, C) d\mu
&= \Gaussian(\obs \given Am, \likPar + ACA^\top). 
\end{align*}
\end{prop}

\begin{proof} 
\todo
\end{proof}

We next prove a lemma that will be used in calculating $\Var[\postDens(\Par; \fwdEm[\Ndesign])]$.
\begin{lemma} \label{lemma:squared_Gaussian_density}
Let $\Gaussian(m, C)$ be a Gaussian distribution on $\R^{\dimObs}$ with $C$ a symmetric, positive-definite 
matrix. Then, for $\obs \in \R^{\dimObs}$, 
\begin{align*}
\Gaussian(\obs \given m, C)^2 
&= 2^{-\dimObs/2} \det(2\pi C)^{-1/2} \Gaussian\left(\obs \given m, \frac{1}{2}C\right). 
\end{align*}
\end{lemma}

\begin{proof}
\begin{align*}
\Gaussian(\obs \given m, C)^2 
&= \det(2\pi C)^{-1} \Exp{-\frac{1}{2} (\obs - m)^\top \left[\frac{1}{2}C \right]^{-1}(\obs - m)} \\
&= \det(2\pi C)^{-1} \det(2\pi (1/2)C)^{1/2} \Gaussian\left(\obs \given m, \frac{1}{2}C\right) \\
&= 2^{-\dimObs/2} \det(2\pi C)^{-1/2} \Gaussian\left(\obs \given m, \frac{1}{2}C\right). 
\end{align*}
\end{proof}

\Cref{eq:fwd-em-Gaussian} follows directly from the following result.

\begin{prop} \label{prop:Gaussian_marginal_moments}
Assume $\obs \given \mu \sim \Gaussian(A \mu, \likPar)$ and $\mu \sim \Gaussian(m, C)$, where $\mu \in \R^{\dimPar}$, 
$A \in \R^{\dimObs \times \dimPar}$, and $\likPar$, $C$ are both symmetric, positive definite. Then 
\begin{align*}
\E\left[\Gaussian(\obs \given A \mu, \likPar) \right] &= \Gaussian(\obs \given Am, \likPar + ACA^\top) \\
\Var\left[\Gaussian(\obs \given A \mu, \likPar) \right] 
&= \frac{\Gaussian\left(\obs \given Am, \frac{1}{2} \likPar + ACA^\top \right)}{2^{\dimObs/2} \det(2\pi \likPar)^{1/2}} - 
\frac{\Gaussian\left(\obs \given Am, \frac{1}{2}[\likPar + ACA^\top] \right)}{2^{\dimObs/2} \det(2\pi[\likPar + ACA^\top])^{1/2}}
\end{align*}
\end{prop}

\begin{proof} 
The first result follows immediately from \Cref{Gaussian_convolution}. For the variance, we have 
\begin{align}
\Var\left[\Gaussian(\obs \given A \mu, \likPar) \right] 
&= \E\left[\Gaussian(\obs \given A \mu, \likPar)^2 \right] - \E\left[\Gaussian(\obs \given A \mu, \likPar) \right]^2 \nonumber \\
&= \E\left[\Gaussian(\obs \given A \mu, \likPar)^2 \right] - \Gaussian(\obs \given Am, \likPar + ACA^\top)^2. \label{two_terms_variance}
\end{align}
Starting with the first term, we apply \Cref{lemma:squared_Gaussian_density} and 
\Cref{Gaussian_convolution}, respectively, to obtain 
\begin{align*}
\E\left[\Gaussian(\obs \given A \mu, \likPar)^2 \right]
&= 2^{-\dimObs/2} \det(2\pi \likPar)^{-1/2} \E\left[\Gaussian\left(\obs \given A\mu, \frac{1}{2}\likPar \right)\right] \\
&= 2^{-\dimObs/2} \det(2\pi \likPar)^{-1/2} \Gaussian\left(\obs \given Am, \frac{1}{2}\likPar + ACA^\top \right).
\end{align*}
For the second term in \ref{two_terms_variance}, another application of \Cref{lemma:squared_Gaussian_density} gives
\begin{align*}
\Gaussian(\obs \given Am, \likPar + ACA^\top)^2
&= 2^{-\dimObs/2} \det(2\pi[\likPar + ACA^\top])^{-1/2} \Gaussian\left(\obs \given Am, \frac{1}{2}[\likPar + ACA^\top]\right).
\end{align*}
Plugging these expressions back into \ref{two_terms_variance} completes the proof. 
\end{proof}

\subsubsection{Log-Density Emulator}




\subsection{Marginal Approximation with Gaussian Likelihood: Forward Model Emulation}
The closed-form computations related to the marginal approximation with a Gaussian 
likelihood follow from standard results regarding the convolution of Gaussian densities.  








\subsection{Marginal Acceptance Probability}
In this section we derive an expression for 
\begin{align}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right], 
\end{align}
as considered in \ref{acc_prob_joint_marg}. We start by noting 
\begin{align}
\Em[\Ndesign]{\accProbRatio}(\Par, \propPar)
&\sim \LN\left(\log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par), \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right] \right),
\label{acc_ratio_LN}
\end{align}
where 
\begin{align*}
C \Def \frac{\priorDens(\propPar) \propDens(\propPar, \Par)}{\priorDens(\Par) \propDens(\Par, \propPar)}.
\end{align*}
Thus, the required computation reduces to computing the expectation of $\min\left\{1, Y \right\}$, where $Y$ is a log-normally distributed 
random variable. 

\begin{lemma} \label{lemma:exp_max_one_LN}
Let $Y \sim \LN(m, s^2)$. Then, 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right] &= \GaussianCDF\left(\frac{m}{s} \right) + \GaussianCDF\left(-\frac{m + s^2}{s} \right) e^{m + \frac{1}{2}s^2}
\end{align}
\end{lemma}

\begin{proof}
We have 
\begin{align}
\E\left[\min\left\{1, Y \right\} \right]
&= \int_{0}^{\infty} \min\{1, y\} \LN(y | m, s^2) dy \nonumber \\
&= \int_{-\infty}^{\infty} \min\{1, e^x\} \Gaussian(x | m, s^2) dx \nonumber \\
&= \int_{0}^{\infty} \Gaussian(x | m, s^2) dx +  \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} e^x \Gaussian(x | m, s^2) dx. \nonumber \\
&= \GaussianCDF(m/s) + \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx.  
\label{second_integral} 
\end{align}
For the integral in \ref{second_integral} we combine the exponential terms and complete the square. 
This yields 
\begin{align*}
&\int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} e^x \Exp{-\frac{1}{2s^2} (x - m)^2} dx \\
&= \Exp{-\frac{m^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2} \left[\frac{x^2}{2} - 2\left(\frac{m}{s^2} + 1\right)x  \right]} dx \\
&= \Exp{-\frac{m^2}{2s^2} + \frac{[m + s^2]^2}{2s^2}} \int_{-\infty}^{0} \frac{1}{\sqrt{2\pi s^2}} \Exp{-\frac{1}{2s^2}(x - [m+s^2])^2} dx \\
&= \Exp{m + \frac{1}{2}s^2} \GaussianCDF\left(-\frac{m+s^2}{s}\right).
\end{align*}
Plugging this back into \ref{second_integral} completes the proof. 
\end{proof}

We now apply \Cref{lemma:exp_max_one_LN} to obtain the expression for the marginal acceptance probability. 
\begin{prop} \label{prop:joint-marg-accept-prob}
\begin{align*}
\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) 
&= \E_{\llikEm[\Ndesign]} \left[\min\left\{1, \Em[\Ndesign]{\accProbRatio}(\Par, \propPar)\right\} \right] 
= w_1 + w_2 \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar), 
\end{align*}
where 
\begin{align*}
w_1 &= \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \Prob\left(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq \Exp{-\Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]}\right).
\end{align*}
\end{prop}

\begin{proof}
We recall from \ref{acc_ratio_LN} that $\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \sim \LN(m, s^2)$, with 
\begin{align*}
m &= \log C + \emMean[\Ndesign]{\llik}(\propPar) - \emMean[\Ndesign]{\llik}(\Par) \\
s^2 &= \Var\left[\llikEm[\Ndesign](\propPar) - \llikEm[\Ndesign](\Par) \right]. 
\end{align*}
We thus have 
\begin{align*}
e^{m + \frac{1}{2}s^2} = \E_{\llikEm}\left[\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \right] 
=  \llikEmJointMarg[\Ndesign]{\accProbRatio}(\Par, \propPar). 
\end{align*}
It remains to verify the expressions for the weights $w_1$ and $w_2$. Letting $Z \sim \Gaussian(0,1)$, 
we apply \Cref{lemma:exp_max_one_LN} to obtain
\begin{align*}
w_1 &= \GaussianCDF(m/s) = \Prob(Z \leq m/s) = \Prob(\Exp{m+sZ} \geq 1) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \geq 1) \\
w_2 &= \GaussianCDF(-(m+s^2)/s) = \Prob(\Exp{m + sZ} \leq e^{-s}) = \Prob(\Em[\Ndesign]{\accProbRatio}(\Par, \propPar) \leq e^{-s}).
\end{align*}
\end{proof}

\subsection{Transition Kernels of Approximate MCMC Algorithms} \label{transition_kernel_derivations}
We derive the expression for the \textit{MCWMH-joint} transition 
kernel $\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}$ given in \ref{MCWMH-joint-kernel}. 
Let $\Par \in \parSpace$ and $A \subset \parSpace$ a Borel set. 
We recall that the kernel for a standard MH algorithm is given by 
\begin{align}
\MarkovKernel(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar
+ [1 - \avgAccProbMH(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
where $\avgAccProbMH(\Par)$ is the overall acceptance probability, averaged over all proposals,
\begin{align}
\avgAccProbMH(\Par)
&= \int_{\parSpace} \propDens(\Par, \propPar) \accProbMH(\Par, \propPar) d\propPar. 
\end{align}
Denote $\parMat \Def \{\Par, \propPar\}$. 
The \textit{MCWMH-joint} algorithm replaces the exact log-likelihood evaluations 
$\llik(\parMat) \Def [\llik(\Par), \llik(\propPar)]^\top$ used to define $\propDens(\Par, \propPar)$ by 
sampled approximate values 
\begin{align}
\llikSamp_{\parMat} \Def [\llikSamp, \llikSampProp]^\top \sim \Gaussian(\emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)). 
\end{align}
We let $\Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})$ denote the approximate acceptance probability defined using 
the sampled values $\llikSamp_{\parMat}$. The probability of accepting a state in the set $A$, conditional on the sample $\llikSamp_{\parMat}$, 
is thus 
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat})
&=  \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat}) d\propPar. 
\end{align}
The unconditional probability follows from the law of total probability:
\begin{align}
\Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par)
&= \int_{\R^2} \Prob(\indexMCMC[\mcmcIndex+1]{\Par} \in A | \Par, \llikSamp_{\parMat}) \Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \\
&= \int_{\R^2} \int_{A} \propDens(\Par, \propPar) \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\propPar \ d\llikSamp_{\parMat} \\
&= \int_{A} \propDens(\Par, \propPar) \int_{\R^2}  \Em[\Ndesign]{\accProbMH}(\Par, \propPar | \llikSamp_{\parMat})  
\Gaussian(\llikSamp_{\parMat} | \emMean[\Ndesign]{\llik}(\parMat), \emKer[\Ndesign]{\llik}(\parMat)) d\llikSamp_{\parMat} \ d\propPar \label{flip_integral_order} \\
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar
\end{align}
where \ref{flip_integral_order} follows from Tonelli's theorem, and using the definition of $\llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)$
in \ref{acc_prob_joint_marg}. Setting $A \Def \parSpace$ in the above integral yields the overall acceptance probability 
\begin{align}
\llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par) \Def \int_{\parSpace} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar)  d\propPar. 
\end{align}
These quantities thus give 
\begin{align}
\llikEmJointMCWMH[\Ndesign]{\MarkovKernel}(\Par, A)
&= \int_{A} \propDens(\Par, \propPar) \llikEmJointMarg[\Ndesign]{\accProbMH}(\Par, \propPar) d\propPar
+ [1 - \llikEmJointMarg[\Ndesign]{\avgAccProbMH}(\Par)] \delta_{\Par}(A) \label{MH-kernel}, 
\end{align}
which follows from same derivations for the standard MH kernel \ref{MH-kernel} with the marginal acceptance probabilities substituted for
the original ones. The transition kernel for the \textit{MCWMH-ind} algorithm follows immediately by marginalizing with respect 
to $\Gaussian(\emMean[\Ndesign]{\llik}\left(\parMat), \diag\left\{\emKer[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\propPar) \right\}\right)$
in place of the full joint distribution. 

\subsection{Sequential Design Calculations}
We start by stating an identity for the inversion of partitioned matrices, which is very useful in updating GPs by 
conditioning on new design points. The generic result can be found in the lecture notes \cite{MinkaMatrixLectures}, 
but we specialize the statement to the GP setting.  

\subsubsection{Useful Lemmas}

\begin{prop} \label{partitioned-matrix-inverse}
Let $\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$ be a GP prior, with 
$\funcEm[\Ndesign] \Def \funcPrior | [\funcPrior(\designIn) = \func(\designIn)] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$  
the GP predictive distribution after conditioning on the design $(\designIn, \func(\designIn))$. Let 
$\designBatchIn$ be a set of $\Nbatch$ new design points. Define 
$\kerMat[\Ndesign] \Def \gpKerPrior(\designIn)$, $\kerMat[\Nbatch] \Def \gpKerPrior(\designBatchIn)$,
and $\kerMat[\Ndesign,\Nbatch] \Def \gpKerPrior(\designIn[\Ndesign], \designBatchIn)$.  
Then, letting 
$\designIn[\Naugment] \Def \designIn \cup \designBatchIn$, the inverse of the kernel matrix 
evaluated on the augmented design satisfies 
\begin{align}
\gpKerPrior(\designIn[\Naugment])^{-1}
&= \begin{pmatrix} \kerMat[\Ndesign] & \kerMat[\Ndesign,\Nbatch] \\
\kerMat[\Ndesign,\Nbatch]^\top & \kerMat[\Nbatch] \end{pmatrix}^{-1}
=  \begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix},
\end{align}
where 
\begin{align}
\tilde{K} = \kerMat[\Ndesign]^{-1} + \kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Nbatch,\Ndesign] \kerMat[\Ndesign]^{-1}.
\end{align}
Thus, assuming $\kerMat[\Ndesign]^{-1}$ has already been computed, $\gpKerPrior(\designIn[\Naugment])^{-1}$ can be constructed in an additional 
$\BigO(\Nbatch^3 + \Nbatch \Ndesign^2 + \Nbatch^2 \Ndesign)$ operations. 
\end{prop}

We repeatedly use the fact that $\funcPrior|[\funcPrior(\designIn[\Naugment]) = \func(\designIn[\Naugment])]$ and 
$\funcEm[\Ndesign] | [\funcEm[\Ndesign](\designBatchIn) = \func(\designBatchIn)]$ are equal in distribution ; 
i.e., conditioning the GP prior on the entire design 
is equivalent to sequentially conditioning on subsets of the design. For the sake of completeness, we provide the rigorous justification for this below.

\begin{lemma} \label{lemma:gp-condition-order}
Let $\funcPrior \sim \GP(\gpMeanPrior, \gpKerPrior)$ be a GP prior. Consider a set of $\Naugment$ design points $\{\designIn[\Naugment], \funcVal[\Naugment]\}$
partitioned as $\designIn[\Naugment] = \designIn[\Ndesign] \cup \designBatchIn$ and $\funcVal[\Naugment] = \funcVal[\Ndesign] \cup \funcVal[\Nbatch]$.
Let $\funcEm[\Ndesign] \Def \func | [\func(\designIn[\Ndesign]) = \funcVal[\Ndesign]] \sim \GP(\gpMean[\Ndesign], \gpKer[\Ndesign])$. 
Then the random process $\funcPrior | [\funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]]$ is equal in distribution to the random process
$\funcEm[\Ndesign] | [\funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]]$. 
\end{lemma} 

\begin{proof} 
Since both processes in question are Gaussian it suffices to check that 
\begin{align*}
\E[\funcPrior(\parMat) | \funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]] 
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]] \\
\Cov[\funcPrior(\parMat) | \funcPrior(\designIn[\Naugment]) = \funcVal[\Naugment]] 
&= \Cov[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]],
\end{align*}
for an arbitrary finite set of inputs $\parMat \subset \parSpace$. The quantities on the lefthand side are 
$\gpMean[\Naugment](\parMat)$ and $\gpKer[\Naugment](\parMat)$, by definition. We begin by expanding 
the expression $\gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}$, noting that we 
are borrowing the notation from \Cref{partitioned-matrix-inverse}. Applying the partitioned matrix 
inversion identity from \Cref{partitioned-matrix-inverse} yields 
\begin{align*}
\gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}
&= \begin{pmatrix} \gpKerPrior(\parMat, \designIn[\Ndesign]) &  \gpKerPrior(\parMat, \designIn[\Nbatch]) \end{pmatrix}
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix}.
\end{align*}
Denoting $\funcVal[\Ndesign]^\prime \Def \funcVal[\Ndesign] - \gpMeanPrior(\designIn[\Ndesign])$ and 
$\funcVal[\Nbatch]^\prime \Def \funcVal[\Nbatch] - \gpMeanPrior(\designIn[\Nbatch])$, the predictive mean $\gpMean[\Naugment](\parMat)$ is thus given by 
\begin{align*}
\gpMean[\Naugment](\parMat)
&= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}[\funcVal[\Naugment] - \gpMeanPrior(\designIn[\Naugment])] \\
&= \gpMeanPrior(\parMat) + \begin{pmatrix} \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix}^\top
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix} 
\begin{pmatrix}  \funcVal[\Ndesign]^\prime \\  \funcVal[\Nbatch]^\prime \end{pmatrix} \\
&= \gpMeanPrior(\parMat) + \gpKerPrior(\parMat, \designIn[\Ndesign])\kerMat[\Ndesign]^{-1}\funcVal[\Ndesign]^\prime + 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \funcVal[\Nbatch]^\prime - 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1} \funcVal[\Ndesign]^\prime \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch]^\prime - \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1} \funcVal[\Ndesign]^\prime] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch]^\prime - \gpMean[\Ndesign](\designIn[\Nbatch]) + \gpMeanPrior(\designIn[\Nbatch])] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1}[\funcVal[\Nbatch] - \gpMean[\Ndesign](\designIn[\Nbatch])] \\
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]] 
\end{align*}
where we have used the fact that the predictive covariance of the GP $\funcEm[\Ndesign]$ gives 
\[
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) = \gpKerPrior(\parMat, \designIn[\Nbatch]) - 
\gpKerPrior(\parMat, \designIn[\Ndesign])\kerMat[\Ndesign]^{-1} \gpKerPrior(\designIn[\Ndesign], \parMat).
\]
The covariance calculation proceeds similarly by replacing $\funcVal[\Ndesign]^\prime$ and $\funcVal[\Nbatch]^\prime$ with 
$\gpKerPrior(\designIn[\Ndesign], \parMat)$ and $\gpKerPrior(\designIn[\Nbatch], \parMat)$, respectively. We obtain 
\begin{align*}
\gpKer[\Naugment](\parMat)
&= \gpKerPrior(\parMat) - \gpKerPrior(\parMat, \designIn[\Naugment]) \kerMat[\Naugment]^{-1}[\funcVal[\Naugment] - \gpMeanPrior(\designIn[\Naugment])] \\
&= \gpKerPrior(\parMat) - \begin{pmatrix} \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix}^\top
\begin{pmatrix} \tilde{K} & -\kerMat[\Ndesign]^{-1} \kerMat[\Ndesign,\Nbatch] \gpKer[\Ndesign](\designBatchIn)^{-1} \\
-\gpKer[\Ndesign](\designBatchIn)^{-1} \kerMat[\Ndesign,\Nbatch]^\top  \kerMat[\Ndesign]^{-1} & \gpKer[\Ndesign](\designBatchIn)^{-1} \end{pmatrix} 
\begin{pmatrix}  \gpKerPrior(\designIn[\Ndesign], \parMat) \\  \gpKerPrior(\designIn[\Nbatch], \parMat) \end{pmatrix} \\
&= \gpKer[\Ndesign](\parMat) - 
\gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} [\gpKerPrior(\designIn[\Nbatch], \parMat) - \kerMat[\Ndesign,\Nbatch]^\top \kerMat[\Ndesign]^{-1}\gpKerPrior(\designIn[\Ndesign], \parMat)] \\
&= \gpKer[\Ndesign](\parMat) -  \gpKer[\Ndesign](\parMat, \designIn[\Nbatch]) \gpKer[\Ndesign](\designIn[\Nbatch])^{-1} \gpKer[\Ndesign](\designIn[\Nbatch], \parMat) \\
&= \Cov[\funcEm[\Ndesign](\parMat) | \funcEm[\Ndesign](\designIn[\Nbatch]) = \funcVal[\Nbatch]].
\end{align*}

\end{proof}

\subsubsection{Uncertainty in GP Predictive Mean Due to Unobserved Response}

\begin{proof} [Proof of \Cref{lemma:pred-mean-dist}]
Though the result is only required for a single input $\Par$, it is no more difficult to establish for a set of inputs $\parMat$. 
We recall that 
\begin{align*}
\gpMean[\Ndesign](\parMat | \designBatchFunc) 
&= \E[\funcEm[\Ndesign](\parMat) | \funcEm(\designBatchIn) = \designBatchFunc] \\
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} [\designBatchFunc - \gpMean[\Ndesign](\designBatchIn)],
\end{align*}
following from the GP predictive equations \ref{kriging_eqns}. Since, $\designBatchFunc|\parMat \sim \Gaussian(\gpMean[\Ndesign](\designBatchIn), \gpKer[\Ndesign](\designBatchIn))$, 
we see that $\gpMean[\Ndesign](\parMat | \designBatchFunc)$ is a linear function of a Gaussian random variable. It is thus Gaussian distributed, with mean and covariance
\begin{align*}
\E_{\designBatchFunc}[\gpMean[\Ndesign](\parMat | \designBatchFunc)]
&= \gpMean[\Ndesign](\parMat) + \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} [\gpMean[\Ndesign](\designBatchIn) - \gpMean[\Ndesign](\designBatchIn)] = \gpMean[\Ndesign](\parMat) \\
\Cov_{\designBatchFunc}[\gpMean[\Ndesign](\parMat | \designBatchFunc)]
&= \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1}  \gpKer[\Ndesign](\designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} \gpKer[\Ndesign](\designBatchIn, \parMat) \\
&=  \gpKer[\Ndesign](\parMat, \designBatchIn) \gpKer[\Ndesign](\designBatchIn)^{-1} \gpKer[\Ndesign](\designBatchIn, \parMat) \\
&= \gpKer[\Ndesign](\parMat) - \gpKer[\Naugment](\parMat).
\end{align*}
\end{proof}

\subsubsection{Integrated Conditional Variance Calculations: Log-Likelihood Emulation}

\begin{proof} [Proof of \Cref{lemma:evar}]
We start by noting that 
\begin{align*}
\Var[\llikEmRdmDens[\Ndesign](\Par) | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \Var[\priorDens(\Par) \Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \\
&= \priorDens(\Par)^2 \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik],
\end{align*}
so we will focus on the likelihood emulator, ignoring the prior for now. Since 
\begin{align*}
\Exp{\llikEm(\Par)} | [\llikEm[\Ndesign](\designBatchIn) = \designBatchLlik] \sim 
\LN(\emMean[\Ndesign]{\llik}(\Par| \designBatchLlik), \emKer[\Naugment]{\llik}(\Par)),
\end{align*}
we apply the formula for a log-normal variance to obtain 
\begin{align}
\Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \cst \Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)}, \label{formula_plug_in}
\end{align}
where 
\begin{align*}
\cst \Def \left[\Exp{\emKer[\Naugment]{\llik}(\Par)} -1 \right] \Exp{\emKer[\Naugment]{\llik}(\Par)}
\end{align*}
is not a function of the random variable $\designBatchLlik$. \Cref{lemma:pred-mean-dist} gives 
\begin{align*}
\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)
&\sim \LN(\emMean[\Ndesign]{\llik}(\Par), \emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)), 
\end{align*}
which implies 
\begin{align*}
\Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)}
&\sim \Gaussian(2\emMean[\Ndesign]{\llik}(\Par), 4[\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)]).
\end{align*}
Applying the formula for a log-normal mean thus yields 
\begin{align*}
\E_{\designBatchLlik} \left[\Exp{2\emMean[\Ndesign]{\llik}(\Par | \designBatchLlik)} \right]
&= \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \Exp{2[\emKer[\Ndesign]{\llik}(\Par) - \emKer[\Naugment]{\llik}(\Par)]} \\
&=  \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \varInflation(\Par; \designBatchIn), 
\end{align*}
where $\varInflation(\Par; \designBatchIn)$ is defined in \ref{var_inflation_factor}. Plugging this expression back 
into \ref{formula_plug_in} gives 
\begin{align*}
\E_{\designBatchLlik} \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \designBatchLlik]
&= \cst \Exp{2\emMean[\Ndesign]{\llik}(\Par)} \varInflation(\Par; \designBatchIn) \\
&= \Var[\Exp{\llikEm(\Par)} | \llikEm[\Ndesign](\designBatchIn) = \emMean[\Ndesign]{\llik}(\Par)] \varInflation(\Par; \designBatchIn).
\end{align*}
Multiplying both sides by $\priorDens(\Par)$ completes the proof, with the closed-form expression for the first term following 
immediately from the formula for a log-normal variance. 
\end{proof}

\subsubsection{Integrated Conditional Variance Calculations: Forward Model Emulation}

\begin{proof} [Proof of \Cref{prop:evar-fwd-emulation}]
We begin by noting that the squared prior density can be pulled out of the expectation as
\begin{align*}
\E_{\designBatchFwd} \Var[\postDens(\Par; \fwdEmCond{\designBatchFwd}) | \designBatchFwd]
&= \priorDens^2(\Par) \ \E_{\designBatchFwd}  \Var[\Gaussian\left(\obs |  \fwdEmCond{\designBatchFwd}(\Par), \likPar \right) | \designBatchFwd]. 
\end{align*}
The variance on the righthand side can be expanded as 
\begin{align}
\Var[\Gaussian\left(\obs |  \fwdEmCond{\designBatchFwd}(\Par), \likPar \right) | \designBatchFwd]
&= \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par) \right)}{\det(2\pi \likPar)^{1/2}}
- \frac{\Gaussian\left(\obs | \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right]  \right)}{\det(2\pi [\likPar + \emKer[\Naugment]{\fwd}(\Par)])^{1/2}}
\end{align}
following from an application of \Cref{eq:fwd-em-Gaussian}. The denominators in the above expression can be 
pulled out of the outer expectation and are seen to equal the denominators in the desired expression. We thus complete the proof by 
computing the expectation of the numerators with respect to 
$\designBatchFwd | \designBatchIn \sim \Gaussian(\emMean[\Ndesign]{\fwd}(\designBatchIn), \emKer[\Ndesign]{\fwd}(\designBatchIn))$. 
These expectations can be computed by noting \Cref{lemma:pred-mean-dist} and then applying \Cref{prop:Gaussian_marginal_moments}.
This yields,
\begin{align*}
\E_{\designBatchFwd}\left[\Gaussian\left(\obs \bigg| \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par) \right)\right]
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), 
\left[\frac{1}{2}\likPar + \emKer[\Naugment]{\fwd}(\Par)\right] + \left[\emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)\right] \right) \\
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par) \right)
\end{align*}
and
\begin{align*}
\E_{\designBatchFwd}\left[\Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par|\designBatchFwd), \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right]\right)\right] 
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \frac{1}{2} \left[\likPar + \emKer[\Naugment]{\fwd}(\Par)\right] + \left[\emKer[\Ndesign]{\fwd}(\Par) - \emKer[\Naugment]{\fwd}(\Par)\right] \right) \\
&= \Gaussian\left(\obs \big| \emMean[\Ndesign]{\fwd}(\Par), \left[\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right] - \frac{1}{2} \emKer[\Naugment]{\fwd}(\Par)\right),
\end{align*}
which completes the derivation of \Cref{fwd_evar1}. To obtain \Cref{fwd_evar2} we rearrange the covariances of the above expressions to obtain 
\begin{align*}
\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par) &= 
\left[\likPar +  \emKer[\Ndesign]{\fwd}(\Par) \right] - \frac{1}{2}\likPar 
= \CovComb(\Par) - \frac{1}{2}\likPar \\
\left[\frac{1}{2}\likPar + \emKer[\Ndesign]{\fwd}(\Par)\right] - \frac{1}{2} \emKer[\Naugment]{\fwd}(\Par)
&= \left[\likPar + \emKer[\Ndesign]{\fwd}(\Par) \right] - \frac{1}{2}\left[\likPar + \emKer[\Naugment]{\fwd}(\Par) \right] 
= \CovComb(\Par) - \frac{1}{2} \CovComb[\Naugment](\Par). 
\end{align*}

\end{proof}


% Questions and TODOs
\section{Questions and TODOs}
\subsection{Questions}
\begin{enumerate}
\item How to reliably use a log-normal emulator for Bayesian inference? 
	\begin{enumerate}
	\item Improve GP calibration (e.g., quadratic mean) 
	\item Use more robust statistics (e.g., interquartile range)
	\item Truncate proposal or prior. 
	\end{enumerate}
\item How to deal with highly concentrated/correlated posteriors? 
	\begin{enumerate}
	\item MALA or other samplers. 
	\end{enumerate}
\end{enumerate}

\subsection{Need to add}
\begin{enumerate}
\item Summary of results from noisy MCMC literature.
\item Try working out results that compare the ratio of the approx density at two points across different approximations; 
alternatively could consider deriving these results for the normalized densities. 
\item Include result that sample and marginal approx agree at the design points.
\item Include existence results (check existence result from that new paper)
\item Viewing noisy MCMC approaches as approximations to the sample-based posterior.
\item Different views of noisy MCMC approaches, including extending the state space. How does this alg compare to the 
marginal and mcwmh-ind algs?
\item Add some sort of theoretical result that demonstrates that the marginal approximation is extremely sensitive to the GP 
variance. Based on numerical experiments, seems like this result should be given with respect to the dynamic range of the 
log-likelihood. Also of course depends on how fast the GP variance grows away from the design points, so perhaps should 
consider fill distance or something like this as well.
\item Numerical experiment that considers the different ways to weight the integrated uncertainty criteria (i.e., targeting 
the unnormalized posterior density vs. using the approx posterior samples as weights).
\item Posterior consistency results for the noisy MCMC emulators; combine the noisy MCMC results with GP approximation 
results.
\item Evaluating calibration of GP-approximated posteriors relative to calibration of the underlying GP emulator.
\item Constrained GPs
\item Pathwise sampling approach to approximate the sample-based approximation.
\item Compare noisy MCMC vs. deterministic version that considers integrating over the acceptance prob. 
\item Analyze effect of incorporating GP covariance structure; does it result in posteriors closer to the sample-based posterior? 
\item Compare marginal and sample-based approx.
\item Compare marginal approx in log-likelihood vs. forward model setting. 
\item Analyze distribution of likelihood under forward model emulation [I think this is the exponential of a folded Gaussian random variable]. 
How does its tail compare to the lognormal tail? 
\end{enumerate}

\subsection{Emulator Ideas}
\begin{enumerate}
\item Sum of quadratic kernel, Gaussian kernel, and some sort of flat/linear kernel (i.e. something with very long lengthscales) to capture the 
part of the response surface that "flattens" out. 
\end{enumerate}

\subsection{Notation}
\begin{enumerate}
\item How to clean up notation for all of the different approximations being considered here? 
\end{enumerate}

\subsection{Numerical experiments:}
My plan is to have emulation in dynamical settings (ODEs) as the unifying theme here. VSEM can provide the core example but could also consider 
others, such as Lorenz-63 (see Hwanwoo Kim, Daniel Sanz-Alonso paper for the ODEs they consider). When introducing the dynamical setting,
cite Stuart/Schneider Earth System modeling 2.0 paper. Provide various examples of observation operators: time-averages (moments) of state 
variables, identity operator, many shorter time-averages (e.g., weekly/monthly averages), multi-objective settings of calibrating to multiple state 
variables. I should probably include Lorenz-63 to have a more familiar example to many audiences. 

\begin{enumerate}
\item 1D example with 1D output for basic illustration. 
	\begin{enumerate}
	\item VSEM with single varied parameter. 
	\item For 1D output, consider long time average of of a single state variable. This will allow us to compare forward model and log-likelihood emulation directly.
	\item Gaussian likelihood. 
	\item Compare emulator distributions, log-likelihood emulator distributions, and likelihood emulator distributions, and various posterior approximations. 
	\item Validation metrics: RMSE, MAE, CRPS, Log-Score, Coverage. 
	\end{enumerate}
\item 2D examples for basic illustration and consideration of different posterior characteristics: Gaussian, banana, unidentifiable, concentration level, bimodal. 
\item Extension of 2D densities to 6-10 dims (see Vehtari paper numerical experiments) 
\item Sequential design performance at different levels of posterior concentration. 
\item Integrated uncertainty criterion: incorporating the current posterior via the integrand or the measure?
\item Large batch, few iteration sequential design experiment. 
(4 different combinations we could consider) 
\end{enumerate}

\begin{enumerate}
\item 1D example for basic illustration. 
\item 2D examples for basic illustration and consideration of different posterior characteristics: Gaussian, banana, unidentifiable, concentration level, bimodal. 
\item Extension of 2D densities to 6-10 dims (see Vehtari paper numerical experiments) 
\item Sequential design performance at different levels of posterior concentration. 
\item Integrated uncertainty criterion: incorporating the current posterior via the integrand or the measure?
\item Large batch, few iteration sequential design experiment. 
(4 different combinations we could consider) 
\end{enumerate}

\subsection{Potential examples:}
\begin{enumerate}
\item Banana, unimodal, bimodal, unidentifiable
\item Heat equation (see Sinsbeck and Nowak) 
\item VSEM
\end{enumerate}

\subsection{Things to consider trying}
\begin{enumerate}
\item Perhaps give some context by discussing connections to Bayesian optimization and to log-likelihood approximation used in 
simulation-based inference. 
\item Hyperparameter marginalization: need to look into opportunities for closed-form hyperparameter marginalization during 
sequential design phase. 
\item Developing a design criterion that better aligns with the MHWMC procedure; e.g., something that targets the likelihood ratio. 
\item Implementing Higdon basis function approach for comparison. 
\item Nonnegative constrained GPs (may be able to do this by modifying the kergp optimization code and using nloptr's option to add constraints) 
\item GP-accelerated MALA 
\end{enumerate}

\subsection{Limitations of existing literature}
\begin{enumerate}
\item Very little discussion of case where likelihood parameters are unknown. 
\item Lack of emphasis on batch design (with some exceptions).
\item Little guidance on which approximation/design criterion to choose.
\item Vehtari fixes the marginal approx, and focuses instead on varying the design criterion, but notes that sampling from the marginal approx is problematic. 
\end{enumerate}

\subsection{Conjectures}
\begin{enumerate}
\item The MCWMH algorithms will perform better than sampling from the marginal approx in the log-likelihood emulation setting, especially when the GP is very uncertain. 
\end{enumerate}

\subsection{Consideration}
\begin{enumerate}
\item Literature typically focuses on convergence of the approx posterior. But in cases with very expensive computer models, one might have to stop pre-convergence. 
In these settings the comparison between the approximate posteriors becomes even more important. 
\end{enumerate}



\bibliography{prob_surrogates_bayes} 
% \bibliographystyle{ieeetr}

\end{document}







