
%  ---------------------------------------------------------------------------------------------------
% General/Background
%  ---------------------------------------------------------------------------------------------------

@misc{MinkaMatrixLectures,
  author = {Thomas P. Minka},
  title = {Old and New Matrix Algebra Useful for Statistics},
  month = {December},
  year = {2000}
}

@article{Stuart_BIP,
  title={Inverse problems: A Bayesian perspective},
  author={Stuart, A. M.},
  journal={Acta Numerica},
  volume={19},
  year={2010},
  pages={451-559},
  doi={10.1017/S0962492910000061}
}

% Reinforcement learning
@misc{BadiaRL,
      title={Never Give Up: Learning Directed Exploration Strategies}, 
      author={Adrià Puigdomènech Badia and Pablo Sprechmann and Alex Vitvitskyi and Daniel Guo and Bilal Piot and Steven Kapturowski and Olivier Tieleman and Martín Arjovsky and Alexander Pritzel and Andew Bolt and Charles Blundell},
      year={2020},
      eprint={2002.06038},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.06038}, 
}

@misc{ottjax,
      title={Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein}, 
      author={Marco Cuturi and Laetitia Meng-Papaxanthos and Yingtao Tian and Charlotte Bunne and Geoff Davis and Olivier Teboul},
      year={2022},
      eprint={2201.12324},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.12324}, 
}

% Reinforcement learning
@misc{LiuRL,
      title={Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices}, 
      author={Evan Zheran Liu and Aditi Raghunathan and Percy Liang and Chelsea Finn},
      year={2021},
      eprint={2008.02790},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2008.02790}, 
}

% Bandits
@InProceedings{banditsEmpirical,
author="Vermorel, Joann{\`e}s
and Mohri, Mehryar",
editor="Gama, Jo{\~a}o
and Camacho, Rui
and Brazdil, Pavel B.
and Jorge, Al{\'i}pio M{\'a}rio
and Torgo, Lu{\'i}s",
title="Multi-armed Bandit Algorithms and Empirical Evaluation",
booktitle="Machine Learning: ECML 2005",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="437--448",
isbn="978-3-540-31692-3"
}

% Bandit algorithms book
@book{LattimoreBandits, 
place={Cambridge}, 
title={Bandit Algorithms}, 
publisher={Cambridge University Press}, 
author={Lattimore, Tor and Szepesvári, Csaba}, 
year={2020}}

% Multifidelity modeling review
@article{multifidelityReview,
author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
title = {Survey of Multifidelity Methods in Uncertainty Propagation, Inference, and Optimization},
journal = {SIAM Review},
volume = {60},
number = {3},
pages = {550-591},
year = {2018},
doi = {10.1137/16M1082469},
URL = { https://doi.org/10.1137/16M1082469},
eprint = {https://doi.org/10.1137/16M1082469}}

% Proper scoring rules
@article{scoringRules,
author = {Tilmann Gneiting and Adrian E Raftery and},
title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
journal = {Journal of the American Statistical Association},
volume = {102},
number = {477},
pages = {359--378},
year = {2007},
publisher = {ASA Website},
doi = {10.1198/016214506000001437},
URL = {https://doi.org/10.1198/016214506000001437},
eprint = {https://doi.org/10.1198/016214506000001437}
}



@article{BARTReview,
   author = "Hill, Jennifer and Linero, Antonio and Murray, Jared",
   title = "Bayesian Additive Regression Trees: A Review and Look Forward", 
   journal= "Annual Review of Statistics and Its Application",
   year = "2020",
   volume = "7",
   number = "Volume 7, 2020",
   pages = "251-278",
   doi = "https://doi.org/10.1146/annurev-statistics-031219-041110",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041110",
   publisher = "Annual Reviews",
   issn = "2326-831X",
   type = "Journal Article"
  }
  
  @misc{BARTBO,
      title={On Uncertainty Estimation by Tree-based Surrogate Models in Sequential Model-based Optimization}, 
      author={Jungtaek Kim and Seungjin Choi},
      year={2022},
      eprint={2202.10669},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2202.10669}, 
}




%  ---------------------------------------------------------------------------------------------------
% Sampling/Initial design methods (low-discrepancy, space-filling, etc.)
%  ---------------------------------------------------------------------------------------------------

% Design of computer experiments: a review
@article{initDesignReview,
title = {Design of computer experiments: A review},
journal = {Computers & Chemical Engineering},
volume = {106},
pages = {71-95},
year = {2017},
note = {ESCAPE-26},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2017.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098135417302090},
author = {Sushant S. Garud and Iftekhar A. Karimi and Markus Kraft}
}

% Support points
@article{supportPoints,
author = {Simon Mak and V. Roshan Joseph},
title = {{Support points}},
volume = {46},
journal = {The Annals of Statistics},
number = {6A},
publisher = {Institute of Mathematical Statistics},
pages = {2562 -- 2592},
keywords = {Bayesian computation, energy distance, Monte Carlo, numerical integration, quasi-Monte Carlo, representative points},
year = {2018},
doi = {10.1214/17-AOS1629},
URL = {https://doi.org/10.1214/17-AOS1629}
}


% Stein points
@InProceedings{SteinPoints,
  title = 	 {Stein Points},
  author =       {Chen, Wilson Ye and Mackey, Lester and Gorham, Jackson and Briol, Francois-Xavier and Oates, Chris},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {844--853},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/chen18f/chen18f.pdf},
  url = 	 {https://proceedings.mlr.press/v80/chen18f.html},
  abstract = 	 {An important task in computational statistics and machine learning is to approximate a posterior distribution $p(x)$ with an empirical measure supported on a set of representative points $\{x_i\}_{i=1}^n$. This paper focuses on methods where the selection of points is essentially deterministic, with an emphasis on achieving accurate approximation when $n$ is small. To this end, we present Stein Points. The idea is to exploit either a greedy or a conditional gradient method to iteratively minimise a kernel Stein discrepancy between the empirical measure and $p(x)$. Our empirical results demonstrate that Stein Points enable accurate approximation of the posterior at modest computational cost. In addition, theoretical results are provided to establish convergence of the method.}
}


%  ---------------------------------------------------------------------------------------------------
% General Gaussian Processes (GPs) and Bayesian Optimization
%  ---------------------------------------------------------------------------------------------------

% Gaussian Processes for Machine Learning
@Inbook{gpML,
author="Rasmussen, Carl Edward",
editor="Bousquet, Olivier
and von Luxburg, Ulrike
and R{\"a}tsch, Gunnar",
title="Gaussian Processes in Machine Learning",
bookTitle="Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, T{\"u}bingen, Germany, August 4 - 16, 2003, Revised Lectures",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="63--71",
abstract="We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.",
isbn="978-3-540-28650-9",
doi="10.1007/978-3-540-28650-9_4",
url="https://doi.org/10.1007/978-3-540-28650-9_4"
}

@article{gpjax, 
doi = {10.21105/joss.04455}, 
url = {https://doi.org/10.21105/joss.04455}, 
year = {2022}, 
publisher = {The Open Journal}, 
volume = {7}, number = {75}, pages = {4455}, 
author = {Pinder, Thomas and Dodd, Daniel}, 
title = {GPJax: A Gaussian Process Framework in JAX}, 
journal = {Journal of Open Source Software} }

% Review of BayesOpt
@ARTICLE{reviewBayesOpt,
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
  journal={Proceedings of the IEEE}, 
  title={Taking the Human Out of the Loop: A Review of Bayesian Optimization}, 
  year={2016},
  volume={104},
  number={1},
  pages={148-175},
  keywords={Big data;Bayes methods;Linear programming;Decision making;Design of experiments;Optimization;Genomes;Statistical analysis;decision making;design of experiments;optimization;response surface methodology;statistical learning;genomic medicine;Decision making;design of experiments;optimization;response surface methodology;statistical learning},
  doi={10.1109/JPROC.2015.2494218}}
 
 % Epistemic vs Aleatoric uncertainty
 @article{epistemicAleatoric,
title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
journal = {Mach Learn},
volume = {110},
pages = {457-506},
year = {2021},
doi = {https://doi.org/10.1007/s10994-021-05946-3},
author = {Hüllermeier, E. and Waegeman, W.},
}
 
 
 % BOTorch
 @inproceedings{botorch,
  title = {{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},
  author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
  booktitle = {Advances in Neural Information Processing Systems 33},
  year = 2020,
  url = {http://arxiv.org/abs/1910.06403}
}

% Thompson sampling
@InProceedings{parallelBOThompson,
  title = 	 {Parallelised Bayesian Optimisation via Thompson Sampling},
  author = 	 {Kandasamy, Kirthevasan and Krishnamurthy, Akshay and Schneider, Jeff and Poczos, Barnabas},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {133--142},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/kandasamy18a/kandasamy18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/kandasamy18a.html}
}

 
% Fully Bayesian GPs for active learning
@inproceedings{fullyBayesianGPs,
author = {Riis, Christoffer and Antunes, Francisco and H\"{u}ttel, Frederik Boe and Azevedo, Carlos Lima and Pereira, Francisco C\^{a}mara},
title = {Bayesian active learning with fully Bayesian Gaussian processes},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
articleno = {882},
numpages = {13},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

% Bayesian Quadrature
@article{BayesQuadrature,
title = {Bayes–Hermite quadrature},
journal = {Journal of Statistical Planning and Inference},
volume = {29},
number = {3},
pages = {245-260},
year = {1991},
issn = {0378-3758},
doi = {https://doi.org/10.1016/0378-3758(91)90002-V},
url = {https://www.sciencedirect.com/science/article/pii/037837589190002V},
author = {A. O'Hagan},
keywords = {Bayesian quadrature, numerical integration, Gaussian process, product rule, Gaussian quadrature},
abstract = {Bayesian quadrature treats the problem of numerical integration as one of statistical inference. A prior Gaussian process distribution is assumed for the integrand, observations arise from evaluating the integrand at selected points, and a posterior distribution is derived for the integrand and the integral. Methods are developed for quadrature in Rp. A particular application is integrating the posterior density arising from some other Bayesian analysis. Simulation results are presented, to show that the resulting Bayes–Hermite quadrature rules may perform better than the conventional Gauss–Hermite rules for this application. A key result is derived for product designs, which makes Bayesian quadrature practically useful for integrating in several dimensions. Although the method does not at present provide a solution to the more difficult problem of quadrature in high dimensions, it does seem to offer real improvements over existing methods in relatively low dimensions.}
}

@inproceedings{quadratureLogGP,
 author = {Osborne, Michael and Garnett, Roman and Ghahramani, Zoubin and Duvenaud, David K and Roberts, Stephen J and Rasmussen, Carl},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Active Learning of Model Evidence Using Bayesian Quadrature},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf},
 volume = {25},
 year = {2012}
}


% Kernels for vector-valued functions
@BOOK{multiOutputKernels,
  author={Álvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  booktitle={Kernels for Vector-Valued Functions: A Review},
  year={2012},
  volume={},
  number={},
  pages={},
  keywords={Artificial Intelligence;Machine Learning;Computer Science},
  doi={10.1561/2200000036}}

% Stepwise uncertainty reduction 
@phdthesis{SURThesis,
author = {Chevalier, Clément},
year = {2013},
month = {09},
pages = {},
title = {Fast uncertainty reduction strategies relying on Gaussian process models}
}

% Supermartingale approach to stepwise uncertainty reduction for GPs. 
@article{supermartingaleSUR,
author = {Julien Bect and Fran{\c{c}}ois Bachoc and David Ginsbourger},
title = {{A supermartingale approach to Gaussian process based sequential design of experiments}},
volume = {25},
journal = {Bernoulli},
number = {4A},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
pages = {2883 -- 2919},
keywords = {Active learning, convergence, sequential design of experiments, stepwise uncertainty reduction, supermartingale, uncertainty functional},
year = {2019},
doi = {10.3150/18-BEJ1074},
URL = {https://doi.org/10.3150/18-BEJ1074}
}

% TODO: need to read. Study whether or not hyperparameters ought to be updated in sequential design methods.
@ARTICLE{gpHyperparameters,
AUTHOR={Sinsbeck, Michael  and Höge, Marvin  and Nowak, Wolfgang },
TITLE={Exploratory-Phase-Free Estimation of GP Hyperparameters in Sequential Design Methods—At the Example of Bayesian Inverse Problems},
JOURNAL={Frontiers in Artificial Intelligence},
VOLUME={3},
YEAR={2020},
URL={https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2020.00052},
DOI={10.3389/frai.2020.00052},
ISSN={2624-8212}}

% Active learning Deep GPs
@article{deepGPAL,
author = {Annie Sauer and Robert B. Gramacy and David Higdon and},
title = {Active Learning for Deep Gaussian Process Surrogates},
journal = {Technometrics},
volume = {65},
number = {1},
pages = {4--18},
year = {2023},
publisher = {ASA Website},
doi = {10.1080/00401706.2021.2008505},
URL = {  https://doi.org/10.1080/00401706.2021.2008505},
eprint = {https://doi.org/10.1080/00401706.2021.2008505}
}

% Deep GPs Vecchia
@article{deepGPVecchia,
author = {Annie Sauer and Andrew Cooper and Robert B. Gramacy and},
title = {Vecchia-Approximated Deep Gaussian Processes for Computer Experiments},
journal = {Journal of Computational and Graphical Statistics},
volume = {32},
number = {3},
pages = {824--837},
year = {2023},
publisher = {ASA Website},
doi = {10.1080/10618600.2022.2129662},
URL = {https://doi.org/10.1080/10618600.2022.2129662},
eprint = {https://doi.org/10.1080/10618600.2022.2129662}
}

% Pathwise conditioning
@misc{pathwiseConditioning,
      title={Pathwise Conditioning of Gaussian Processes}, 
      author={James T. Wilson and Viacheslav Borovitskiy and Alexander Terenin and Peter Mostowsky and Marc Peter Deisenroth},
      year={2021},
      eprint={2011.04026},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2011.04026}, 
}

% Efficiently sampling GP posteriors
@misc{samplingGPPosts,
      title={Efficiently Sampling Functions from Gaussian Process Posteriors}, 
      author={James T. Wilson and Viacheslav Borovitskiy and Alexander Terenin and Peter Mostowsky and Marc Peter Deisenroth},
      year={2020},
      eprint={2002.09309},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2002.09309}, 
}

% Random Fourier Features
@inproceedings{RFF,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2007}
}


% KL expansion for GP prior
@article{dimRedPolyChaos,
title = {Dimensionality reduction and polynomial chaos acceleration of Bayesian inference in inverse problems},
journal = {Journal of Computational Physics},
volume = {228},
number = {6},
pages = {1862-1902},
year = {2009},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2008.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0021999108006062},
author = {Youssef M. Marzouk and Habib N. Najm},
keywords = {Inverse problems, Bayesian inference, Dimensionality reduction, Polynomial chaos, Markov chain Monte Carlo, Galerkin projection, Gaussian processes, Karhunen–Loève expansion, RKHS},
abstract = {We consider a Bayesian approach to nonlinear inverse problems in which the unknown quantity is a spatial or temporal field, endowed with a hierarchical Gaussian process prior. Computational challenges in this construction arise from the need for repeated evaluations of the forward model (e.g., in the context of Markov chain Monte Carlo) and are compounded by high dimensionality of the posterior. We address these challenges by introducing truncated Karhunen–Loève expansions, based on the prior distribution, to efficiently parameterize the unknown field and to specify a stochastic forward problem whose solution captures that of the deterministic forward model over the support of the prior. We seek a solution of this problem using Galerkin projection on a polynomial chaos basis, and use the solution to construct a reduced-dimensionality surrogate posterior density that is inexpensive to evaluate. We demonstrate the formulation on a transient diffusion equation with prescribed source terms, inferring the spatially-varying diffusivity of the medium from limited and noisy data.}
}

@article{KLInvProbs,
author = {Li, Wei and Cirpka, Olaf A.},
title = {Efficient geostatistical inverse methods for structured and unstructured grids},
journal = {Water Resources Research},
volume = {42},
number = {6},
pages = {},
keywords = {geostatistical inference, Karhunen-Loève expansion, periodic embeddding},
doi = {https://doi.org/10.1029/2005WR004668},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2005WR004668},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2005WR004668},
year = {2006}
}

@article{precondMCMCKL,
author = {Efendiev, Y. and Hou, T. and Luo, W.},
title = {Preconditioning Markov Chain Monte Carlo Simulations Using Coarse-Scale Models},
journal = {SIAM Journal on Scientific Computing},
volume = {28},
number = {2},
pages = {776-803},
year = {2006},
doi = {10.1137/050628568},
URL = {https://doi.org/10.1137/050628568},
eprint = {https://doi.org/10.1137/050628568}}

@article{mcmcHydrology,
author = {Laloy, Eric and Vrugt, Jasper A.},
title = {High-dimensional posterior exploration of hydrologic models using multiple-try DREAM(ZS) and high-performance computing},
journal = {Water Resources Research},
volume = {48},
number = {1},
pages = {},
keywords = {DREAM, MCMC, high parameter dimensionality, multiple-try, parallel computing},
doi = {https://doi.org/10.1029/2011WR010608},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2011WR010608},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2011WR010608},
year = {2012}
}

@ARTICLE{conditionPermToPressure,
       author = {{Oliver}, Dean S. and {Cunha}, Luciane B. and {Reynolds}, Albert C.},
        title = "{Markov chain Monte Carlo methods for conditioning a permeability field to pressure data}",
      journal = {Mathematical Geology},
     keywords = {conditional simulation, Markov chain, Monte Carlo, sampling, pressure data, sensitivity, well test},
         year = 1997,
        month = mar,
       volume = {29},
       number = {1},
        pages = {61-91},
          doi = {10.1007/BF02769620},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1997MatG...29...61O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{polyChaosReview,
  author={Shen, Danfeng and Wu, Hao and Xia, Bingqing and Gan, Deqiang},
  journal={IEEE Systems Journal}, 
  title={Polynomial Chaos Expansion for Parametric Problems in Engineering Systems: A Review}, 
  year={2020},
  volume={14},
  number={3},
  pages={4500-4514},
  keywords={Mathematical model;Stochastic processes;Uncertainty;Chaos;Differential equations;Random variables;Computational modeling;Polynomial chaos expansion (PCE);parametric problem;stochastic collocation method (SCM);stochastic Galerkin method (SGM);uncertainty quantification (UQ)},
  doi={10.1109/JSYST.2019.2957664}}

@article{MarzoukPolyChaos,
title = {Stochastic spectral methods for efficient Bayesian solution of inverse problems},
journal = {Journal of Computational Physics},
volume = {224},
number = {2},
pages = {560-586},
year = {2007},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2006.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0021999106004839},
author = {Youssef M. Marzouk and Habib N. Najm and Larry A. Rahn},
keywords = {Inverse problems, Bayesian inference, Polynomial chaos, Monte Carlo, Markov chain Monte Carlo, Spectral methods, Galerkin projection, Diffusive transport},
abstract = {We present a reformulation of the Bayesian approach to inverse problems, that seeks to accelerate Bayesian inference by using polynomial chaos (PC) expansions to represent random variables. Evaluation of integrals over the unknown parameter space is recast, more efficiently, as Monte Carlo sampling of the random variables underlying the PC expansion. We evaluate the utility of this technique on a transient diffusion problem arising in contaminant source inversion. The accuracy of posterior estimates is examined with respect to the order of the PC representation, the choice of PC basis, and the decomposition of the support of the prior. The computational cost of the new scheme shows significant gains over direct sampling.}
}

@article{uribeKL,
title = {Bayesian inference of random fields represented with the Karhunen–Loève expansion},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {358},
pages = {112632},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2019.112632},
url = {https://www.sciencedirect.com/science/article/pii/S004578251930516X},
author = {Felipe Uribe and Iason Papaioannou and Wolfgang Betz and Daniel Straub}
}

@misc{deepEnsembles,
      title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}, 
      author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
      year={2017},
      eprint={1612.01474},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1612.01474}, 
}

@misc{epistemicNN,
      title={Epistemic Neural Networks}, 
      author={Ian Osband and Zheng Wen and Seyed Mohammad Asghari and Vikranth Dwaracherla and Morteza Ibrahimi and Xiuyuan Lu and Benjamin Van Roy},
      year={2023},
      eprint={2107.08924},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.08924}, 
}

@misc{BayesOptEpistemicNN,
      title={Approximate Thompson Sampling via Epistemic Neural Networks}, 
      author={Ian Osband and Zheng Wen and Seyed Mohammad Asghari and Vikranth Dwaracherla and Morteza Ibrahimi and Xiuyuan Lu and Benjamin Van Roy},
      year={2023},
      eprint={2302.09205},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.09205}, 
}

@misc{BayesOptNN,
      title={A Study of Bayesian Neural Network Surrogates for Bayesian Optimization}, 
      author={Yucen Lily Li and Tim G. J. Rudner and Andrew Gordon Wilson},
      year={2024},
      eprint={2305.20028},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.20028}, 
}


@InProceedings{BayesOptBayesLastLayer,
  title = 	 {Scalable Bayesian Optimization Using Deep Neural Networks},
  author = 	 {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2171--2180},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/snoek15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/snoek15.html}}

@misc{BayesLastLayer,
      title={Bayesian Layers: A Module for Neural Network Uncertainty}, 
      author={Dustin Tran and Michael W. Dusenberry and Mark van der Wilk and Danijar Hafner},
      year={2019},
      eprint={1812.03973},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.03973}, 
}

@misc{conformalSurrogate,
      title={Uncertainty Quantification of Surrogate Models using Conformal Prediction}, 
      author={Vignesh Gopakumar and Ander Gray and Joel Oskarsson and Lorenzo Zanisi and Stanislas Pamela and Daniel Giles and Matt Kusner and Marc Peter Deisenroth},
      year={2024},
      eprint={2408.09881},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.09881}, 
}

@misc{conformalGP,
      title={Conformal Approach To Gaussian Process Surrogate Evaluation With Coverage Guarantees}, 
      author={Edgar Jaber and Vincent Blot and Nicolas Brunel and Vincent Chabridon and Emmanuel Remy and Bertrand Iooss and Didier Lucor and Mathilde Mougeot and Alessandro Leite},
      year={2024},
      eprint={2401.07733},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2401.07733}, 
}

@article{conformalTwoStageDesign,
title = {Two-stage surrogate modeling for data-driven design optimization with application to composite microstructure generation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109436},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109436},
url = {https://www.sciencedirect.com/science/article/pii/S095219762401594X},
author = {Farhad Pourkamali-Anaraki and Jamal F. Husseini and Evan J. Pineda and Brett A. Bednarcyk and Scott E. Stapleton},
keywords = {Intelligent design optimization, Machine learning, Learner-evaluator framework, Conformal prediction, Prediction interval},
abstract = {Design optimization or inverse problems, which involve determining input parameters to achieve specific desired outputs, are essential in many engineering applications. Conventional artificial intelligence methods for solving inverse problems rely on single-stage surrogate modeling. However, these methods can be limited in their ability to fully represent the complex relationship between inputs and outputs, hindering a comprehensive exploration of potential solutions and overlooking valid alternatives. To address this challenge, this paper presents a novel two-stage framework that combines two distinct machine learning models: a “learner” and an “evaluator”. The learner identifies a subset of candidate inputs whose predicted outputs closely match the target. The evaluator then refines this selection by further narrowing the input space, resulting in more precise predictions. A key innovation is the incorporation of conformal inference, a statistical technique that quantifies prediction uncertainty in a distribution-free setting and is applicable to any machine learning model. The framework’s effectiveness is validated through extensive benchmark testing using a simulated data set and an engineering case study on artificial microstructure generation of fiber-reinforced composites. The results demonstrate that this two-stage approach significantly outperforms traditional single-stage methods, consistently delivering more accurate and reliable solutions. For instance, the framework consistently identifies input configurations that closely align with two desired descriptors across varying fiber counts, while single surrogate models yield solutions far from the targets without any precautions. This paper is concluded by discussing potential future enhancements, including the integration of deep learning models and strategies for addressing distribution shifts within the framework.}
}

@misc{conformalEvidentialSurrogate,
      title={ConfEviSurrogate: A Conformalized Evidential Surrogate Model for Uncertainty Quantification}, 
      author={Yuhan Duan and Xin Zhao and Neng Shi and Han-Wei Shen},
      year={2025},
      eprint={2504.02919},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2504.02919}, 
}

@misc{conformalBayesOpt,
      title={Bayesian Optimization with Conformal Prediction Sets}, 
      author={Samuel Stanton and Wesley Maddox and Andrew Gordon Wilson},
      year={2023},
      eprint={2210.12496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.12496}, 
}


@inbook{SAA,
author = {Kim, Sujin and Pasupathy, Raghu and Henderson, Shane},
year = {2015},
month = {01},
pages = {207-243},
title = {A Guide to Sample Average Approximation},
volume = {216},
isbn = {978-1-4939-1383-1},
doi = {10.1007/978-1-4939-1384-8_8}
}

%  ---------------------------------------------------------------------------------------------------
% Polynomial Chaos
%  ---------------------------------------------------------------------------------------------------

@article{PCEBIP,
title = {Limitations of polynomial chaos expansions in the Bayesian solution of inverse problems},
journal = {Journal of Computational Physics},
volume = {282},
pages = {138-147},
year = {2015},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2014.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0021999114007621},
author = {Fei Lu and Matthias Morzfeld and Xuemin Tu and Alexandre J. Chorin},
keywords = {Polynomial chaos expansion, Bayesian inverse problem, Monte Carlo sampling}
}


% Combine PCE and GP
@Article{PCEGPWind,
AUTHOR = {Marinescu, Marius and Olivares, Alberto and Staffetti, Ernesto and Sun, Junzi},
TITLE = {Polynomial Chaos Expansion-Based Enhanced Gaussian Process Regression for Wind Velocity Field Estimation from Aircraft-Derived Data},
JOURNAL = {Mathematics},
VOLUME = {11},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {1018},
URL = {https://www.mdpi.com/2227-7390/11/4/1018},
ISSN = {2227-7390},
DOI = {10.3390/math11041018}
}

@article{BayesianPCE1,
title = {Global sensitivity analysis: A Bayesian learning based polynomial chaos approach},
journal = {Journal of Computational Physics},
volume = {415},
pages = {109539},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2020.109539},
url = {https://www.sciencedirect.com/science/article/pii/S0021999120303132},
author = {Biswarup Bhattacharyya}
}

@article{BayesianPCE2,
title = {A fully Bayesian sparse polynomial chaos expansion approach with joint priors on the coefficients and global selection of terms},
journal = {Journal of Computational Physics},
volume = {488},
pages = {112210},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112210},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123003054},
author = {Paul-Christian Bürkner and Ilja Kröker and Sergey Oladyshkin and Wolfgang Nowak},
keywords = {Polynomial chaos expansion, Surrogate modeling, Bayesian inference, Uncertainty quantification, Shrinkage priors, Variable selection}
}

@misc{PCEGP2,
      title={Combining Gaussian processes and polynomial chaos expansions for stochastic nonlinear model predictive control}, 
      author={Bradford, Eric and Imsland, Lars},
      year={2021},
      eprint={2103.05441},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2103.05441}, 
}

% ---------------------------------------------------------------------------------------------------
% Applications with expensive computer models
% ---------------------------------------------------------------------------------------------------

@article{MakEddySims,
author = {Simon Mak and Chih-Li Sung and Xingjian Wang and Shiang-Ting Yeh and Yu-Hung Chang and V. Roshan Joseph and Vigor Yang and C. F. Jeff Wu and},
title = {An Efficient Surrogate Model for Emulation and Physics Extraction of Large Eddy Simulations},
journal = {Journal of the American Statistical Association},
volume = {113},
number = {524},
pages = {1443--1456},
year = {2018},
publisher = {ASA Website},
doi = {10.1080/01621459.2017.1409123},
URL = { https://doi.org/10.1080/01621459.2017.1409123},
eprint = {https://doi.org/10.1080/01621459.2017.1409123}
}

@article{compBioAgentBased,
  author       = {An, Gary and Fitzpatrick, Benjamin G. and Christley, Scott and Federico, Paolo and Kanarek, Adam and Neilan, Rachel M. and Oremland, Matthew and Salinas, Rebecca and Laubenbacher, Reinhard and Lenhart, Suzanne},
  title        = {Optimization and Control of Agent-Based Models in Biology: A Perspective},
  journal      = {Bulletin of Mathematical Biology},
  year         = {2017},
  volume       = {79},
  number       = {1},
  pages        = {63--87},
  month        = jan,
  doi          = {10.1007/s11538-016-0225-6},
  pmid         = {27826879},
  pmcid        = {PMC5209420},
  note         = {Epub 2016 Nov 8}
}

@article{yeastMatingSurrogate,
    doi = {10.1371/journal.pcbi.1006181},
    author = {Renardy, Marissa and Yi, Tau-Mu AND Xiu, Dongbin and Chou, Ching-Shan},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Parameter uncertainty quantification using surrogate models applied to a spatial model of yeast mating polarization},
    year = {2018},
    month = {05},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pcbi.1006181},
    pages = {1-26},
    number = {5}
}


%  ---------------------------------------------------------------------------------------------------
% MCMC
%  ---------------------------------------------------------------------------------------------------


% Log-likelihood emulation, consider uncertainty in MH acceptance ratio
@article{gpEmMCMC,
  author  = {Marko J{{\"a}}rvenp{{\"a}}{{\"a}} and Jukka Corander},
  title   = {Approximate Bayesian inference from noisy likelihoods with Gaussian process emulated MCMC},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {366},
  pages   = {1--55},
  url     = {http://jmlr.org/papers/v25/21-0421.html}
}

% Pseudo-marginal. 
@article{pseudoMarginalMCMC,
author = {Christophe Andrieu and Gareth O. Roberts},
title = {{The pseudo-marginal approach for efficient Monte Carlo computations}},
volume = {37},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {697 -- 725},
keywords = {auxiliary variable, convergence, marginal, Markov chain Monte Carlo},
year = {2009},
doi = {10.1214/07-AOS574},
URL = {https://doi.org/10.1214/07-AOS574}
}

% On the efficiency of pseudomarginal
@article{pseudoMarginalEfficiency,
author = {Sherlock, Chris and Thiery, Alexandre and Roberts, Gareth and Rosenthal, Jeffrey},
year = {2013},
month = {09},
pages = {},
title = {On the efficiency of pseudo-marginal random walk Metropolis algorithms},
volume = {43},
journal = {The Annals of Statistics},
doi = {10.1214/14-AOS1278}
}

% Correlated pseudomarginal algorithm
@misc{corrPM,
      title={The Correlated Pseudo-Marginal Method}, 
      author={George Deligiannidis and Arnaud Doucet and Michael K. Pitt},
      year={2017},
      eprint={1511.04992},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/1511.04992}, 
}

% Stability of Noisy Metropolis-Hastings
@misc{stabilityNoisyMH,
      title={Stability of Noisy Metropolis-Hastings}, 
      author={Felipe J. Medina-Aguayo and Anthony Lee and Gareth O. Roberts},
      year={2015},
      eprint={1503.07066},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

% Noisy Monte Carlo: approximate transition kernels
@misc{noisyMCMC,
      title={Noisy Monte Carlo: Convergence of Markov chains with approximate transition kernels}, 
      author={P. Alquier and N. Friel and R. Everitt and A. Boland},
      year={2014},
      eprint={1403.5496},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% Survey of noisy Monte Carlo 
@misc{noisyMCSurvey,
      title={A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning}, 
      author={F. Llorente and L. Martino and J. Read and D. Delgado},
      year={2021},
      eprint={2108.00490},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Function Space MCMC
@article{functionSpaceMCMC,
   title={MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster},
   volume={28},
   ISSN={0883-4237},
   url={http://dx.doi.org/10.1214/13-STS421},
   DOI={10.1214/13-sts421},
   number={3},
   journal={Statistical Science},
   publisher={Institute of Mathematical Statistics},
   author={Cotter, S. L. and Roberts, G. O. and Stuart, A. M. and White, D.},
   year={2013},
   month=aug }

% Randomized MCMC for Bayesian inverse problems
@article{randomizedMCMC,
title = {Randomized approaches to accelerate MCMC algorithms for Bayesian inverse problems},
journal = {Journal of Computational Physics},
volume = {440},
pages = {110391},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110391},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121002862},
author = {Arvind K. Saibaba and Pranjal Prasad and Eric {de Sturler} and Eric Miller and Misha E. Kilmer}
}

% Exchange Algorithm
@misc{exchangeAlg,
      title={MCMC for doubly-intractable distributions}, 
      author={Iain Murray and Zoubin Ghahramani and David MacKay},
      year={2012},
      eprint={1206.6848},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/1206.6848}
}

% Doubly intractable review
@misc{doublyIntractableReview,
      title={Bayesian Inference in the Presence of Intractable Normalizing Functions}, 
      author={Jaewoo Park and Murali Haran},
      year={2018},
      eprint={1701.06619},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/1701.06619}
}

% Curse and blessing of multimodal posteriors
@article{multimodalYao,
  author  = {Yuling Yao and Aki Vehtari and Andrew Gelman},
  title   = {Stacking for Non-mixing Bayesian Computations: The Curse and Blessing of Multimodal Posteriors},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {79},
  pages   = {1--45},
  url     = {http://jmlr.org/papers/v23/20-1426.html}
}

% Considers pseudomarginal and mcwmh algorithms for sampling from marginal approximation.
% And direct sampling approach for sampling from expected posterior.
@misc{garegnani2021NoisyMCMC,
      title={Sampling Methods for Bayesian Inference Involving Convergent Noisy Approximations of Forward Maps}, 
      author={Giacomo Garegnani},
      year={2021},
      eprint={2111.03491},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2111.03491}, 
}


%  ---------------------------------------------------------------------------------------------------
% General computer modeling
%  ---------------------------------------------------------------------------------------------------

@article{modularization,
author = {Liu, F. and Bayarri, M. and Berger, J.},
year = {2009},
month = {03},
pages = {},
title = {Modularization in Bayesian analysis, with emphasis on analysis of computer models},
volume = {4},
journal = {Bayesian Analysis},
doi = {10.1214/09-BA404}
}

@article{design_analysis_computer_experiments,
author = {Jerome Sacks and William J. Welch and Toby J. Mitchell and Henry P. Wynn},
title = {{Design and Analysis of Computer Experiments}},
volume = {4},
journal = {Statistical Science},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {409 -- 423},
keywords = {computer-aided design, Experimental design, kriging, response surface, spatial statistics},
year = {1989},
doi = {10.1214/ss/1177012413},
URL = {https://doi.org/10.1214/ss/1177012413}
}

@book{gramacy2020surrogates,
  title = {Surrogates: {G}aussian Process Modeling, Design and \
    Optimization for the Applied Sciences},
  author = {Robert B. Gramacy},
  publisher = {Chapman Hall/CRC},
  address = {Boca Raton, Florida},
  note = {\url{http://bobby.gramacy.com/surrogates/}},
  year = {2020}
}

@article{SanterCompExp,
author = {Thomas J. Satner and Brian J. Williams and William I. Notz},
title = {{The Design and Analysis of Computer Experiments}},
publisher = {Springer},
address = {New York, NY},
note = {\url{https://doi.org/10.1007/978-1-4939-8847-1}},
year = {2018}
}

% Entropy-based adaptive design for contour finding
@misc{cole2021entropybased,
      title={Entropy-based adaptive design for contour finding and estimating reliability}, 
      author={D. Austin Cole and Robert B. Gramacy and James E. Warner and Geoffrey F. Bomarito and Patrick E. Leser and William P. Leser},
      year={2021},
      eprint={2105.11357},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% Contour Estimation
@article{contourEstimation,
author = {Pritam Ranjan, Derek Bingham and George Michailidis},
title = {Sequential Experiment Design for Contour Estimation From Complex Computer Codes},
journal = {Technometrics},
volume = {50},
number = {4},
pages = {527--541},
year = {2008},
publisher = {Taylor \& Francis},
doi = {10.1198/004017008000000541},
URL = { 
        https://doi.org/10.1198/004017008000000541
},
eprint = { 
    
        https://doi.org/10.1198/004017008000000541
}
}

% Kennedy and O'Hagan
@article{KOH,
author = {Kennedy, Marc C. and O'Hagan, Anthony},
title = {Bayesian calibration of computer models},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {63},
number = {3},
pages = {425-464},
keywords = {Calibration, Computer experiments, Deterministic models, Gaussian process, Interpolation, Model inadequacy, Sensitivity analysis, Uncertainty analysis},
doi = {https://doi.org/10.1111/1467-9868.00294},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00294},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00294},
year = {2001}
}

% Review of computer model calibration
@article{computerModelCalibrationReview,
author = {Sung, Chih-Li and Tuo, Rui},
title = {A review on computer model calibration},
journal = {WIREs Computational Statistics},
volume = {16},
number = {1},
pages = {e1645},
keywords = {computer experiments, experimental design, Gaussian process, uncertainty quantification, unidentifiability},
doi = {https://doi.org/10.1002/wics.1645},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1645},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1645},
abstract = {Abstract Model calibration is crucial for optimizing the performance of complex computer models across various disciplines. In the era of Industry 4.0, symbolizing rapid technological advancement through the integration of advanced digital technologies into industrial processes, model calibration plays a key role in advancing digital twin technology, ensuring alignment between digital representations and real-world systems. This comprehensive review focuses on the Kennedy and O'Hagan (KOH) framework (Kennedy and O'Hagan, Journal of the Royal Statistical Society: Series B 2001; 63(3):425–464). In particular, we explore recent advancements addressing the challenges of the unidentifiability issue while accommodating model inadequacy within the KOH framework. In addition, we explore recent advancements in adapting the KOH framework to complex scenarios, including those involving multivariate outputs and functional calibration parameters. We also delve into experimental design strategies tailored to the unique demands of model calibration. By offering a comprehensive analysis of the KOH approach and its diverse applications, this review serves as a valuable resource for researchers and practitioners aiming to enhance the accuracy and reliability of their computer models. This article is categorized under: Statistical Models > Semiparametric Models Statistical Models > Simulation Models Statistical Models > Bayesian Models},
year = {2024}
}
 

% Uncertainty Quantification and Predictive Computational Science
@book{UQpredCompSci,
author = {Ryan G. McClarren},
title = {Uncertainty Quantification and Predictive Computational Science: A Foundation for Physical Scientists and Engineers},
publisher = {Springer Cham},
note = {\url{https://doi.org/10.1007/978-3-319-99525-0}},
year = {2018}
}

% Stochastic computer models review
@article{stochasticComputerModels,
author = {Evan Baker and Pierre Barbillon and Arindam Fadikar and Robert B. Gramacy and Radu Herbei and David Higdon and Jiangeng Huang and Leah R. Johnson and Pulong Ma and Anirban Mondal and Bianica Pires and Jerome Sacks and Vadim Sokolov},
title = {{Analyzing Stochastic Computer Models: A Review with Opportunities}},
volume = {37},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {64 -- 89},
keywords = {agent based model, Calibration, computer experiment, computer model, Emulator, Gaussian process, surrogates, uncertainty quantification},
year = {2022},
doi = {10.1214/21-STS822},
URL = {https://doi.org/10.1214/21-STS822}
}

% Importance of Model Discrepancy
@article{ModelDiscrepancy,
doi = {10.1088/0266-5611/30/11/114007},
url = {https://dx.doi.org/10.1088/0266-5611/30/11/114007},
year = {2014},
month = {oct},
publisher = {IOP Publishing},
volume = {30},
number = {11},
pages = {114007},
author = {Brynjarsdóttir, Jenný and OʼHagan, Anthony},
title = {Learning about physical parameters: the importance of model discrepancy},
journal = {Inverse Problems}
}


%  ---------------------------------------------------------------------------------------------------
% Log-Likelihood (or unnormalized log posterior) emulation with GPs
%  ---------------------------------------------------------------------------------------------------

% Log likelihood emulation used for importance sampling proposal distribution
@article{OakleyllikEm,
author = {Jeremy E. Oakley and Benjamin D. Youngman},
title = {Calibration of Stochastic Computer Simulators Using Likelihood Emulation},
journal = {Technometrics},
volume = {59},
number = {1},
pages = {80--92},
year = {2017},
publisher = {ASA Website},
doi = {10.1080/00401706.2015.1125391},
URL = { https://doi.org/10.1080/00401706.2015.1125391},
eprint = {https://doi.org/10.1080/00401706.2015.1125391}
}

% Emulating unnormalized log posterior density
@article{emPostDens,
author = {Dietzel, A. and Reichert, P.},
title = {Bayesian inference of a lake water quality model by emulating its posterior density},
journal = {Water Resources Research},
volume = {50},
number = {10},
pages = {7626-7647},
keywords = {Gaussian stochastic process emulator, posterior distribution, lake water quality model, structurally uncertain model, model complexity},
doi = {https://doi.org/10.1002/2012WR013086},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2012WR013086},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2012WR013086},
year = {2014}
}


% Emulate unnormalized log-posterior. The focus is on sequential design schemes that take the form of a modified upper confidence 
% bound optimization. Numerical examples (only in 1d and 3d) focused on parameter estimation for ODEs. Consider 
% GP mean approximation of posterior density. 
@misc{gp_surrogates_random_exploration,
      title={Enhancing Gaussian Process Surrogates for Optimization and Posterior Approximation via Random Exploration}, 
      author={Hwanwoo Kim and Daniel Sanz-Alonso},
      year={2024},
      eprint={2401.17037},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.17037}
}

% Emulate the likelihood. Propose a GP-accelerated MCMC scheme that tries to sample trajectories from the GP. 
@article{trainDynamics,
title = {Statistical inverse identification for nonlinear train dynamics using a surrogate model in a Bayesian framework},
journal = {Journal of Sound and Vibration},
volume = {458},
pages = {158-176},
year = {2019},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2019.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X19303633},
author = {D. Lebel and C. Soize and C. Fünfschilling and G. Perrin},
keywords = {Statistical inverse problem, Bayesian calibration, Surrogate model, High-speed train dynamics, Uncertainty quantification},
abstract = {This paper presents a Bayesian calibration method for a simulation-based model with stochastic functional input and output. The originality of the method lies in an adaptation involving the representation of the likelihood function by a Gaussian process surrogate model, to cope with the high computational cost of the simulation, while avoiding the surrogate modeling of the functional output. The adaptation focuses on taking into account the uncertainty introduced by the use of a surrogate model when estimating the parameters posterior probability distribution by MCMC. To this end, trajectories of the random surrogate model of the likelihood function are drawn and injected in the MCMC algorithm. An application on a train suspension monitoring case is presented.}
}

% Propose a method to approximately sample a prob distribution while also "spreading out" the points. Propose
% a sequential scheme that learns the response surface/prob density via a surrogate as the algorithm proceeds.
% In an example they fit the log-density with a GP.
@article{JosephMinEnergy,
author = {V. Roshan Joseph and Tirthankar Dasgupta and Rui Tuo and C. F. Jeff Wu and},
title = {Sequential Exploration of Complex Surfaces Using Minimum Energy Designs},
journal = {Technometrics},
volume = {57},
number = {1},
pages = {64--74},
year = {2015},
publisher = {ASA Website},
doi = {10.1080/00401706.2014.881749},
URL = { https://doi.org/10.1080/00401706.2014.881749},
eprint = {https://doi.org/10.1080/00401706.2014.881749}
}

% One idea they propose here is to use their minimum energy design method as an initial design to fit
% an emulator, similar to the calibrate, emulate, sample approach.
@article{JosephMEDSampling,
author = {V. Roshan Joseph and Dianpeng Wang and Li Gu and Shiji Lyu and Rui Tuo and},
title = {Deterministic Sampling of Expensive Posteriors Using Minimum Energy Designs},
journal = {Technometrics},
volume = {61},
number = {3},
pages = {297--308},
year = {2019},
publisher = {ASA Website},
doi = {10.1080/00401706.2018.1552203},
URL = { https://doi.org/10.1080/00401706.2018.1552203},
eprint = {https://doi.org/10.1080/00401706.2018.1552203}
}

% Emulate unnormalized log posterior. Use delayed-acceptance HMC. GP only used for proposal, exact posterior used for acceptance ratio. Uses GP classifier
% to find regions of parameter space where physical assumptions are violated. Adapt HMC parameters using BayesOpt. 
@article{MCMC_GP_proposal,
author = {Paun, L. Mihaela and Husmeier, Dirk},
title = {Markov chain Monte Carlo with Gaussian processes for fast parameter estimation and uncertainty quantification in a 1D fluid-dynamics model of the pulmonary circulation},
journal = {International Journal for Numerical Methods in Biomedical Engineering},
volume = {37},
number = {2},
pages = {e3421},
keywords = {classification, emulation, Gaussian processes, MCMC, pulmonary circulation, uncertainty quantification},
doi = {https://doi.org/10.1002/cnm.3421},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.3421},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cnm.3421},
abstract = {Abstract The past few decades have witnessed an explosive synergy between physics and the life sciences. In particular, physical modelling in medicine and physiology is a topical research area. The present work focuses on parameter inference and uncertainty quantification in a 1D fluid-dynamics model for quantitative physiology: the pulmonary blood circulation. The practical challenge is the estimation of the patient-specific biophysical model parameters, which cannot be measured directly. In principle this can be achieved based on a comparison between measured and predicted data. However, predicting data requires solving a system of partial differential equations (PDEs), which usually have no closed-form solution, and repeated numerical integrations as part of an adaptive estimation procedure are computationally expensive. In the present article, we demonstrate how fast parameter estimation combined with sound uncertainty quantification can be achieved by a combination of statistical emulation and Markov chain Monte Carlo (MCMC) sampling. We compare a range of state-of-the-art MCMC algorithms and emulation strategies, and assess their performance in terms of their accuracy and computational efficiency. The long-term goal is to develop a method for reliable disease prognostication in real time, and our work is an important step towards an automatic clinical decision support system.},
year = {2021}
}

% Utilize a plug-in estimator for the likelihood which computes a quantile of the emulator distribution, the idea being that something like the 90th percentile represents 
% a conservative estimate of the true likelihood. They also incorporate bound constraints to account for the fact that they are emulating a non-negative quantity (in their 
% case, the unnormalized log Gaussian likelihood). 
% Use a subsampling approach to sequential design similar to Istem's work. As a stopping criteria for the sequential design, they use a Cauchy-Schwarz divergence.
@misc{quantileApprox,
      title={Solving Bayesian Inverse Problems With Expensive Likelihoods Using Constrained Gaussian Processes and Active Learning}, 
      author={Maximilian Dinkel and Carolin M. Geitner and Gil Robalo Rei and Jonas Nitzler and Wolfgang A. Wall},
      year={2023},
      eprint={2312.08085},
      archivePrefix={arXiv},
      primaryClass={cs.CE}
}

% Emulate log-posterior. 
@article{llikRBF,
author = {Nikolay Bliznyuk and David Ruppert and Christine Shoemaker and Rommel Regis and Stefan Wild and Pradeep Mugunthan},
title = {Bayesian Calibration and Uncertainty Analysis for Computationally Expensive Models Using Optimization and Radial Basis Function Approximation},
journal = {Journal of Computational and Graphical Statistics},
volume = {17},
number = {2},
pages = {270-294},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/106186008X320681},
URL = {https://doi.org/10.1198/106186008X320681},
eprint = { https://doi.org/10.1198/106186008X320681}
}

% Bayesian quadrature. Emulate log-integrand with GP. 
@inproceedings{BayesQuadratureAL,
 author = {Osborne, Michael and Garnett, Roman and Ghahramani, Zoubin and Duvenaud, David K and Roberts, Stephen J and Rasmussen, Carl},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Active Learning of Model Evidence Using Bayesian Quadrature},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf},
 volume = {25},
 year = {2012}
}

% Bayesian quadrature for ratios
@InProceedings{BayesQuadRatios,
  title = 	 {Bayesian Quadrature for Ratios},
  author = 	 {Osborne, Michael and Garnett, Roman and Roberts, Stephen and Hart, Christopher and Aigrain, Suzanne and Gibson, Neale},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {832--840},
  year = 	 {2012},
  editor = 	 {Lawrence, Neil D. and Girolami, Mark},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v22/osborne12/osborne12.pdf},
  url = 	 {https://proceedings.mlr.press/v22/osborne12.html},
  abstract = 	 {We describe a novel approach to quadrature for ratios of probabilistic integrals, such as are used to compute posterior probabilities. It offers performance superior to Monte Carlo methods by exploiting a Bayesian quadrature framework. We improve upon previous Bayesian quadrature techniques by explicitly modelling the non-negativity of our integrands, and the correlations that exist between them. It offers most where the integrand is multi-modal and expensive to evaluate, as is commonplace in exoplanets research; we demonstrate the efficacy of our method on data from the Kepler spacecraft.}
}


% Istem paper using noisy MCMC approach. 
@Article{FerEmulation,
AUTHOR = {Fer, I. and Kelly, R. and Moorcroft, P. R. and Richardson, A. D. and Cowdery, E. M. and Dietze, M. C.},
TITLE = {Linking big models to big data: efficient ecosystem model calibration through Bayesian model emulation},
JOURNAL = {Biogeosciences},
VOLUME = {15},
YEAR = {2018},
NUMBER = {19},
PAGES = {5801--5830},
URL = {https://bg.copernicus.org/articles/15/5801/2018/},
DOI = {10.5194/bg-15-5801-2018}
}

% Log-likelihood emulation. MCMC scheme that runs exact model if GP variance is larger than a pre-set threshold, otherwise 
% uses GP mean approximation.
@misc{ActiveLearningMCMC,
      title={Integration of Active Learning and MCMC Sampling for Efficient Bayesian Calibration of Mechanical Properties}, 
      author={Leon Riccius and Iuri B. C. M. Rocha and Joris Bierkens and Hanne Kekkonen and Frans P. van der Meer},
      year={2024},
      eprint={2411.13361},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph},
      url={https://arxiv.org/abs/2411.13361}
}


% Use GPs to accelerate pseudo-marginal MCMC. Fit GP with quadratic mean.
@article{pseudoMarginalGP,
title = {Accelerating pseudo-marginal MCMC using Gaussian processes},
journal = {Computational Statistics & Data Analysis},
volume = {118},
pages = {1-17},
year = {2018},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167947317301950},
author = {Christopher C. Drovandi and Matthew T. Moores and Richard J. Boys},
keywords = {Gaussian processes, Likelihood-free methods, Markov processes, Particle Markov chain Monte Carlo, Pseudo-marginal methods, State space models}
}

%  ---------------------------------------------------------------------------------------------------
% Forward model emulation: marginal approximation
%  ---------------------------------------------------------------------------------------------------

% First consider an approach from propagating uncertainty in GP hyperparameters. Consider adding GP cov to the observation cov. 
% Then consider using a secondary GP fit to the residuals to correct the first one. 
% Sequential design consists of a sub-sampling approach similar to Istem's work. 
@article{hydrologicalModel,
author = {Zhang, Jiangjiang and Zheng, Qiang and Chen, Dingjiang and Wu, Laosheng and Zeng, Lingzao},
title = {Surrogate-Based Bayesian Inverse Modeling of the Hydrological System: An Adaptive Approach Considering Surrogate Approximation Error},
journal = {Water Resources Research},
volume = {56},
number = {1},
pages = {e2019WR025721},
doi = {https://doi.org/10.1029/2019WR025721},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019WR025721},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019WR025721},
note = {e2019WR025721 2019WR025721},
year = {2020}
}

@article{hydrologicalModel2,
author = {Zhang, Jiangjiang and Li, Weixuan and Zeng, Lingzao and Wu, Laosheng},
title = {An adaptive Gaussian process-based method for efficient Bayesian experimental design in groundwater contaminant source identification problems},
journal = {Water Resources Research},
volume = {52},
number = {8},
pages = {5971-5984},
keywords = {Bayesian experimental design, MCMC, source identification, Gaussian process, contaminant transport},
doi = {https://doi.org/10.1002/2016WR018598},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016WR018598},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2016WR018598},
year = {2016}
}


% Consider multiple posterior approximations and provide error bounds. 
@misc{StuartTeck1,
      title={Posterior Consistency for Gaussian Process Approximations of Bayesian Posterior Distributions}, 
      author={Andrew M. Stuart and Aretha L. Teckentrup},
      year={2016},
      eprint={1603.02004},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

% Consider llik and forward model emulation for PDE inverse problems. 
@misc{GP_PDE_priors,
      title={Gaussian processes for Bayesian inverse problems associated with linear partial differential equations}, 
      author={Tianming Bai and Aretha L. Teckentrup and Konstantinos C. Zygalakis},
      year={2023},
      eprint={2307.08343},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

% Random Forward Models: lot's of theory
@article{random_fwd_models,
   title={Random Forward Models and Log-Likelihoods in Bayesian Inverse Problems},
   volume={6},
   ISSN={2166-2525},
   url={http://dx.doi.org/10.1137/18M1166523},
   DOI={10.1137/18m1166523},
   number={4},
   journal={SIAM/ASA Journal on Uncertainty Quantification},
   publisher={Society for Industrial & Applied Mathematics (SIAM)},
   author={Lie, H. C. and Sullivan, T. J. and Teckentrup, A. L.},
   year={2018},
   month=jan, pages={1600–1629} 
 }
 
   
@article{TeckHyperpar,
author = {Teckentrup, Aretha L.},
title = {Convergence of Gaussian Process Regression with Estimated Hyper-Parameters and Applications in Bayesian Inverse Problems},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {8},
number = {4},
pages = {1310-1337},
year = {2020},
doi = {10.1137/19M1284816},
URL = {https://doi.org/10.1137/19M1284816},
eprint = {https://doi.org/10.1137/19M1284816}
}
   
% Example application of Emulate, calibrate, sample approach. 
@article{idealizedGCM,
author = {Dunbar, Oliver R. A. and Garbuno-Inigo, Alfredo and Schneider, Tapio and Stuart, Andrew M.},
title = {Calibration and Uncertainty Quantification of Convective Parameters in an Idealized GCM},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {13},
number = {9},
pages = {e2020MS002454},
keywords = {uncertainty quantification, model calibration, machine learning, general circulation model, parametric uncertainty, inverse problem},
doi = {https://doi.org/10.1029/2020MS002454},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002454},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002454},
note = {e2020MS002454 2020MS002454},
abstract = {Abstract Parameters in climate models are usually calibrated manually, exploiting only small subsets of the available data. This precludes both optimal calibration and quantification of uncertainties. Traditional Bayesian calibration methods that allow uncertainty quantification are too expensive for climate models; they are also not robust in the presence of internal climate variability. For example, Markov chain Monte Carlo (MCMC) methods typically require model runs and are sensitive to internal variability noise, rendering them infeasible for climate models. Here we demonstrate an approach to model calibration and uncertainty quantification that requires only model runs and can accommodate internal climate variability. The approach consists of three stages: (a) a calibration stage uses variants of ensemble Kalman inversion to calibrate a model by minimizing mismatches between model and data statistics; (b) an emulation stage emulates the parameter-to-data map with Gaussian processes (GP), using the model runs in the calibration stage for training; (c) a sampling stage approximates the Bayesian posterior distributions by sampling the GP emulator with MCMC. We demonstrate the feasibility and computational efficiency of this calibrate-emulate-sample (CES) approach in a perfect-model setting. Using an idealized general circulation model, we estimate parameters in a simple convection scheme from synthetic data generated with the model. The CES approach generates probability distributions of the parameters that are good approximations of the Bayesian posteriors, at a fraction of the computational cost usually required to obtain them. Sampling from this approximate posterior allows the generation of climate predictions with quantified parametric uncertainties.},
year = {2021}
}

@article{CESSoftware, 
doi = {10.21105/joss.06372}, 
url = {https://doi.org/10.21105/joss.06372}, 
year = {2024}, 
publisher = {The Open Journal}, volume = {9}, number = {97}, pages = {6372}, 
author = {Oliver R.A. Dunbar and Melanie Bieli and Alfredo Garbuno-Iñigo and Michael Howland and Andre Nogueira de Souza and Laura Anne Mansfield and Gregory L. Wagner and N. Efrat-Henrici}, 
title = {CalibrateEmulateSample.jl: Accelerated Parametric Uncertainty Quantification},
 journal = {Journal of Open Source Software} }

%  ---------------------------------------------------------------------------------------------------
% Log-likelihood emulation with non-GP models. 
%  ---------------------------------------------------------------------------------------------------

% Log likelihood emulation (as well as emulation of its gradient and Hessian) using neural operator. 
@misc{cao2024efficient,
      title={Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators}, 
      author={Lianghao Cao and Thomas O'Leary-Roseberry and Omar Ghattas},
      year={2024},
      eprint={2403.08220},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

%  ---------------------------------------------------------------------------------------------------
% Sequential design which seeks to target true posterior. 
%  ---------------------------------------------------------------------------------------------------

% Suggest choosing design points based on the posterior density, but oversampling the tails. 
@misc{briol2017sampling,
      title={On the Sampling Problem for Kernel Quadrature}, 
      author={Francois-Xavier Briol and Chris J. Oates and Jon Cockayne and Wilson Ye Chen and Mark Girolami},
      year={2017},
      eprint={1706.03369},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

% Provide justification for selecting design points in regions of significant posterior mass. 
@misc{StuartTeck2,
      title={Introduction To Gaussian Process Regression In Bayesian Inverse Problems, With New Results On Experimental Design For Weighted Error Measures}, 
      author={Tapio Helin and Andrew Stuart and Aretha Teckentrup and Konstantinos Zygalakis},
      year={2023},
      eprint={2302.04518},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

% Solving optimization problem. Emulate the log Euclidean error of a simulator with time-series output using BART. 
% Expected improvement sequential design. 
@misc{ranjan2016inverse,
      title={Inverse problem for time-series valued computer model via scalarization}, 
      author={Pritam Ranjan and Mark Thomas and Holger Teismann and Sujay Mukhoti},
      year={2016},
      eprint={1605.09503},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% Calibrate, Emulate, Sample. 
% Utilize Ensemble Kalman Inversion (EKI) or Ensemble Kalman Sampling (EKI) as cheap design method for 
% fititng a GP emulator. 
@article{CES,
	doi = {10.1016/j.jcp.2020.109716},
	url = {https://doi.org/10.1016\%2Fj.jcp.2020.109716},  
	year = 2021,
	month = {jan},  
	publisher = {Elsevier {BV}},  
	volume = {424},  
	pages = {109716},  
	author = {Emmet Cleary and Alfredo Garbuno-Inigo and Shiwei Lan and Tapio Schneider and Andrew M. Stuart},  
	title = {Calibrate, emulate, sample},  
	journal = {Journal of Computational Physics}
}

% Log-likelihood emulation. Sequential design using entropy of lognormal likelihood approx. They mention plugging 
 % in approximation to the density and running MCMC but don't explain how. 
@article{landslideCalibration,
	doi = {https://doi.org/10.1007/s10346-022-01857-z},
	url = {https://doi.org/10.1007/s10346-022-01857-z},  
	year = 2022,
	month = {April},  
	volume = {19},  
	pages = {2033–2045},  
	author = {Zhao, H. and Kowalski, J.},  
	title = {Bayesian active learning for parameter calibration of landslide run-out models.},  
	journal = {Landslides}
}

% Unnormalized log-posterior emulation. Sequential design using exponentiated variance and negative expected 
% divergence. The latter is typically computationally intractable. 
@article{Kandasamy_2017,
   title={Query efficient posterior estimation in scientific experiments via Bayesian active learning},
   volume={243},
   ISSN={0004-3702},
   url={http://dx.doi.org/10.1016/j.artint.2016.11.002},
   DOI={10.1016/j.artint.2016.11.002},
   journal={Artificial Intelligence},
   publisher={Elsevier BV},
   author={Kandasamy, Kirthevasan and Schneider, Jeff and Póczos, Barnabás},
   year={2017},
   month=feb, pages={45–56} }

% Requires density estimation. Propose an approach to deal with fast-varying densities that may not be conducive 
% to GP modeling. Active learning approach maximizing the log-normal entropy (entropy of unnormalized log-posterior
% emulator. 
@misc{wang2018adaptive,
      title={Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions}, 
      author={Hongqiao Wang and Jinglai Li},
      year={2018},
      eprint={1703.09930},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

% Bilds on the Wang et all 2018 paper to deal with multimodal posteriors.
@misc{adaptiveMultimodal,
      title={An adaptive Gaussian process method for multi-modal Bayesian inverse problems}, 
      author={Zhihang Xu and Xiaoyu Zhu and Daoji Li and Qifeng Liao},
      year={2024},
      eprint={2409.15307},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2409.15307}, 
}


% ---------------------------------------------------------------------------------------------------
% Other: surrogate modeling for Bayesian inverse problems
% ---------------------------------------------------------------------------------------------------

@article{Rynn_2019,
doi = {10.1088/1681-7575/aaf984},
url = {https://dx.doi.org/10.1088/1681-7575/aaf984},
year = {2019},
month = {jan},
publisher = {IOP Publishing},
volume = {56},
number = {1},
pages = {015018},
author = {Rynn, James A J and Cotter, Simon L and Powell, Catherine E and Wright, Louise},
title = {Surrogate accelerated Bayesian inversion for the determination of the thermal diffusivity of a material},
journal = {Metrologia}
}

@article{MCMCExpensivePosts,
author = {Mark Fielding and David J. Nott and Shie-Yui Liong and},
title = {Efficient MCMC Schemes for Computationally Expensive Posterior Distributions},
journal = {Technometrics},
volume = {53},
number = {1},
pages = {16--28},
year = {2011},
publisher = {ASA Website},
doi = {10.1198/TECH.2010.09195},
URL = {https://doi.org/10.1198/TECH.2010.09195},
eprint = {https://doi.org/10.1198/TECH.2010.09195}
}



% ---------------------------------------------------------------------------------------------------
% Emulating simulators with high-dimensional output: Basis functions
% ---------------------------------------------------------------------------------------------------

% Higdon et al PCA basis function approach to high-dim outputs. 
@article{HigdonBasis,
author = {Dave Higdon and James Gattiker and Brian Williams and Maria Rightley},
title = {Computer Model Calibration Using High-Dimensional Output},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {570-583},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214507000000888},
URL = {   
        https://doi.org/10.1198/016214507000000888
},
eprint = { 
        https://doi.org/10.1198/016214507000000888
}
}

@article{RougierHighDim,
author = {Jonathan Rougier},
title = {Efficient Emulators for Multivariate Deterministic Functions},
journal = {Journal of Computational and Graphical Statistics},
volume = {17},
number = {4},
pages = {827--843},
year = {2008},
publisher = {ASA Website},
doi = {10.1198/106186008X384032},
URL = {https://doi.org/10.1198/106186008X384032},
eprint = {https://doi.org/10.1198/106186008X384032}}

% High dimensional input and output
@misc{RemoteSensingEmulator,
      title={Computer Model Emulation with High-Dimensional Functional Output in Large-Scale Observing System Uncertainty Experiments}, 
      author={Pulong Ma and Anirban Mondal and Bledar Konomi and Jonathan Hobbs and Joon Song and Emily Kang},
      year={2020},
      eprint={1911.09274},
      archivePrefix={arXiv},
      primaryClass={stat.AP},
      url={https://arxiv.org/abs/1911.09274}, 
}

% High dimensional input
@article{ZhouHighDimInput,
title = {Surrogate modeling of high-dimensional problems via data-driven polynomial chaos expansions and sparse partial least square},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {364},
pages = {112906},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.112906},
url = {https://www.sciencedirect.com/science/article/pii/S004578252030089X},
author = {Yicheng Zhou and Zhenzhou Lu and Jinghan Hu and Yingshi Hu},
keywords = {Polynomial chaos expansion, Dimensionality reduction, Stochastic partial differential equations, Data driven}
}

% Wavelet basis. 
@article{emulate_functional_output,
author = {M. J. Bayarri and D. Walsh and J. O. Berger and J. Cafeo and G. Garcia-Donato and F. Liu and J. Palomo and R. J. Parthasarathy and R. Paulo and J. Sacks},
title = {{Computer model validation with functional output}},
volume = {35},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1874 -- 1906},
keywords = {Bayesian analysis, bias, Computer models, functional data, uncertain inputs, validation},
year = {2007},
doi = {10.1214/009053607000000163},
URL = {https://doi.org/10.1214/009053607000000163}
}

% Emulate coefficients in KL expansion, then use delayed-acceptance MCMC. 
@Article{functionValuedModels,
AUTHOR = {Albert, Christopher G. and Callies, Ulrich and Toussaint, Udo von},
TITLE = {Surrogate-Enhanced Parameter Inference for Function-Valued Models},
JOURNAL = {Physical Sciences Forum},
VOLUME = {3},
YEAR = {2021},
NUMBER = {1},
ARTICLE-NUMBER = {11},
URL = {https://www.mdpi.com/2673-9984/3/1/11},
ISSN = {2673-9984},
ABSTRACT = {We present an approach to enhance the performance and flexibility of the Bayesian inference of model parameters based on observations of the measured data. Going beyond the usual surrogate-enhanced Monte-Carlo or optimization methods that focus on a scalar loss, we place emphasis on a function-valued output of a formally infinite dimension. For this purpose, the surrogate models are built on a combination of linear dimensionality reduction in an adaptive basis of principal components and Gaussian process regression for the map between reduced feature spaces. Since the decoded surrogate provides the full model output rather than only the loss, it is re-usable for multiple calibration measurements as well as different loss metrics and, consequently, allows for flexible marginalization over such quantities and applications to Bayesian hierarchical models. We evaluate the method&rsquo;s performance based on a case study of a toy model and a simple riverine diatom model for the Elbe river. As input data, this model uses six tunable scalar parameters as well as silica concentrations in the upper reach of the river together with the continuous time-series of temperature, radiation, and river discharge over a specific year. The output consists of continuous time-series data that are calibrated against corresponding measurements from the Geesthacht Weir station at the Elbe river. For this study, only two scalar inputs were considered together with a function-valued output and compared to an existing model calibration using direct simulation runs without a surrogate.},
DOI = {10.3390/psf2021003011}
}

% Use proper orthogonal decomposition
@article{PODemulation,
author = {Yeh, Shiang-Ting and Wang, Xingjian and Sung, Chih-Li and Mak, Simon and Chang, Yu-Hung and Zhang, Liwei and Wu, Chi-Fang and Yang, Vigor},
year = {2017},
month = {07},
pages = {},
title = {Data-Driven Analysis and Common Proper Orthogonal Decomposition (CPOD)-Based Spatio-Temporal Emulator for Design Exploration}
}

% Neural network emulation for CLM, using basis vectors.
@Article{DagonCLM,
AUTHOR = {Dagon, K. and Sanderson, B. M. and Fisher, R. A. and Lawrence, D. M.},
TITLE = {A machine learning approach to emulation and biophysical parameter estimation with the Community Land Model, version 5},
JOURNAL = {Advances in Statistical Climatology, Meteorology and Oceanography},
VOLUME = {6},
YEAR = {2020},
NUMBER = {2},
PAGES = {223--244},
URL = {https://ascmo.copernicus.org/articles/6/223/2020/},
DOI = {10.5194/ascmo-6-223-2020}
}

% ---------------------------------------------------------------------------------------------------
% Parameter estimation for dynamical models. 
% ---------------------------------------------------------------------------------------------------

% Discuss calibrating parameters to time averages of state variables. 
@article{ESM_modeling_2pt0,
author = {Schneider, Tapio and Lan, Shiwei and Stuart, Andrew and Teixeira, João},
title = {Earth System Modeling 2.0: A Blueprint for Models That Learn From Observations and Targeted High-Resolution Simulations},
journal = {Geophysical Research Letters},
volume = {44},
number = {24},
pages = {12,396-12,417},
keywords = {Earth system models, parameterizations, data assimilation, machine learning, Kalman inversion, Markov chain Monte Carlo},
doi = {https://doi.org/10.1002/2017GL076101},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2017GL076101},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2017GL076101},
abstract = {Abstract Climate projections continue to be marred by large uncertainties, which originate in processes that need to be parameterized, such as clouds, convection, and ecosystems. But rapid progress is now within reach. New computational tools and methods from data assimilation and machine learning make it possible to integrate global observations and local high-resolution simulations in an Earth system model (ESM) that systematically learns from both and quantifies uncertainties. Here we propose a blueprint for such an ESM. We outline how parameterization schemes can learn from global observations and targeted high-resolution simulations, for example, of clouds and convection, through matching low-order statistics between ESMs, observations, and high-resolution simulations. We illustrate learning algorithms for ESMs with a simple dynamical system that shares characteristics of the climate system; and we discuss the opportunities the proposed framework presents and the challenges that remain to realize it.},
year = {2017}
}


% ---------------------------------------------------------------------------------------------------
% Other emulation techniques for dynamical models. 
% ---------------------------------------------------------------------------------------------------

% TODO: need to read
@article{dynamic_nonlinear_simulators_GP,
title = {Emulating dynamic non-linear simulators using Gaussian processes},
journal = {Computational Statistics \& Data Analysis},
volume = {139},
pages = {178-196},
year = {2019},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167947319301173},
author = {Hossein Mohammadi and Peter Challenor and Marc Goodfellow}
}

% TODO: need to read
@article{GP_dynamic_emulation,
    author = {Conti, S. and Gosling, J. P. and Oakley, J. E. and O'Hagan, A.},
    title = "{Gaussian process emulation of dynamic computer codes}",
    journal = {Biometrika},
    volume = {96},
    number = {3},
    pages = {663-676},
    year = {2009},
    month = {06},
    abstract = "{Computer codes are used in scientific research to study and predict the behaviour of complex systems. Their run times often make uncertainty and sensitivity analyses impractical because of the thousands of runs that are conventionally required, so efficient techniques have been developed based on a statistical representation of the code. The approach is less straightforward for dynamic codes, which represent time-evolving systems. We develop a novel iterative system to build a statistical model of dynamic computer codes, which is demonstrated on a rainfall-runoff simulator.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asp028},
    url = {https://doi.org/10.1093/biomet/asp028},
    eprint = {https://academic.oup.com/biomet/article-pdf/96/3/663/709193/asp028.pdf}
}

% TODO: need to read
@article{Bayesian_emulation_dynamic,
title = {Bayesian emulation of complex multi-output and dynamic computer models},
journal = {Journal of Statistical Planning and Inference},
volume = {140},
number = {3},
pages = {640-651},
year = {2010},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2009.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378375809002559},
author = {Stefano Conti and Anthony O’Hagan},
keywords = {Bayesian inference, Computer experiments, Dynamic models, Hierarchical models},
abstract = {Computer models are widely used in scientific research to study and predict the behaviour of complex systems. The run times of computer-intensive simulators are often such that it is impractical to make the thousands of model runs that are conventionally required for sensitivity analysis, uncertainty analysis or calibration. In response to this problem, highly efficient techniques have recently been developed based on a statistical meta-model (the emulator) that is built to approximate the computer model. The approach, however, is less straightforward for dynamic simulators, designed to represent time-evolving systems. Generalisations of the established methodology to allow for dynamic emulation are here proposed and contrasted. Advantages and difficulties are discussed and illustrated with an application to the Sheffield Dynamic Global Vegetation Model, developed within the UK Centre for Terrestrial Carbon Dynamics.}
}

% TODO: need to read
@article{emulate_dynamic_epidemic_model,
author = {Marian Farah and Paul Birrell and Stefano Conti and Daniela De Angelis},
title = {Bayesian Emulation and Calibration of a Dynamic Epidemic Model for A/H1N1 Influenza},
journal = {Journal of the American Statistical Association},
volume = {109},
number = {508},
pages = {1398-1411},
year  = {2014},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2014.934453},
URL = { 
    
        https://doi.org/10.1080/01621459.2014.934453
},
eprint = { 
        https://doi.org/10.1080/01621459.2014.934453
}
}

% TODO: need to read
@article{Liu_West_dynamic_emulation,
author = {Fei Liu and Mike West},
title = {{A dynamic modelling strategy for Bayesian computer model emulation}},
volume = {4},
journal = {Bayesian Analysis},
number = {2},
publisher = {International Society for Bayesian Analysis},
pages = {393 -- 411},
keywords = {Backward sampling, computer model emulation, Dynamic linear model, Forwarding filtering, Gaussian process, Markov chain Monte Carlo, time-varying autoregression},
year = {2009},
doi = {10.1214/09-BA415},
URL = {https://doi.org/10.1214/09-BA415}
}

% ---------------------------------------------------------------------------------------------------
% Integrated variance and related criteria
% ---------------------------------------------------------------------------------------------------

% Expected error reduction
@article{Roy2001,
author = {Roy, Nicholas and Mccallum, Andrew},
year = {2001},
month = {01},
pages = {},
title = {Toward optimal active learning through monte carlo estimation of error reduction}
}

% Active learning with expected error reduction
@misc{ALExpErrReduction,
title = {Active Learning with Expected Error Reduction},
author = {Steve Mussmann and Julia Reisler and Daniel Tsai and Ehsan Mousavi and Shayne O’Brien and Moises Goldszmidt},
year = {2022},
URL = {https://arxiv.org/abs/2211.09283}
}

% Integrated variance criterion targeting the GP directly, but weighted by the approx posterior; accompanying theory. 
% Also utilize marginal approximation in Gaussian forward model emulation setting. 
@misc{weightedIVAR,
      title={Sequential design for surrogate modeling in Bayesian inverse problems}, 
      author={Paul Lartaud and Philippe Humbert and Josselin Garnier},
      year={2024},
      eprint={2402.16520},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% Proposes integrated variance criterion for stochastic simulation models
@misc{SurerStochasticIVAR,
      title={Batch Sequential Experimental Design for Calibration of Stochastic Simulation Models}, 
      author={Özge Sürer},
      year={2025},
      eprint={2505.03990},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2505.03990}, 
}

% Log-likelihood emulation with IEVAR criterion. Marginal posterior approximation. 
@article{VehtariParallelGP,
author = {Marko J{\"a}rvenp{\"a}{\"a} and Michael U. Gutmann and Aki Vehtari and Pekka Marttinen},
title = {{Parallel Gaussian Process Surrogate Bayesian Inference with Noisy Likelihood Evaluations}},
volume = {16},
journal = {Bayesian Analysis},
number = {1},
publisher = {International Society for Bayesian Analysis},
pages = {147 -- 178},
keywords = {expensive likelihoods, Gaussian processes, likelihood-free inference, parallel computing, sequential experiment design, surrogate modelling},
year = {2021},
doi = {10.1214/20-BA1200},
URL = {https://doi.org/10.1214/20-BA1200}
}

% KOH framework. Closed-form integrated variance criteria.
@article{Koermer2024,
author = {Scott Koermer and Justin Loda and Aaron Noble and Robert B. Gramacy and},
title = {Augmenting a Simulation Campaign for Hybrid Computer Model and Field Data Experiments},
journal = {Technometrics},
volume = {66},
number = {4},
pages = {638--650},
year = {2024},
publisher = {ASA Website},
doi = {10.1080/00401706.2024.2345139},
URL = { https://doi.org/10.1080/00401706.2024.2345139},
eprint = { https://doi.org/10.1080/00401706.2024.2345139}
}

% Closed-form integrated variance criteria.
@misc{MakTargetedVar,
      title={Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters}, 
      author={John Joshua Miller and Simon Mak},
      year={2024},
      eprint={2403.03816},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2403.03816}, 
}

% Propose gradient-based optimization for sample-sum approximation of IVAR. 
@article{Mercer_kernels_IVAR,
author = {Gorodetsky, Alex and Marzouk, Youssef},
title = {Mercer Kernels and Integrated Variance Experimental Design: Connections Between Gaussian Process Regression and Polynomial Approximation},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {4},
number = {1},
pages = {796-828},
year = {2016},
doi = {10.1137/15M1017119},
URL = { 
    
        https://doi.org/10.1137/15M1017119
},
eprint = { 
        https://doi.org/10.1137/15M1017119
}
}

% KL based criteria. Emulate forward model.
@misc{VillaniAdaptiveGP,
      title={Adaptive Gaussian Process Regression for Bayesian inverse problems}, 
      author={Paolo Villani and Jörg Unger and Martin Weiser},
      year={2024},
      eprint={2404.19459},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2404.19459}
}

% Active Learning Cohn
@article{ALC,
title = {Neural Network Exploration Using Optimal Experiment Design},
journal = {Neural Networks},
volume = {9},
number = {6},
pages = {1071-1083},
year = {1996},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(95)00137-9},
url = {https://www.sciencedirect.com/science/article/pii/0893608095001379},
author = {David A. Cohn},
keywords = {Active learning, Exploration, Optimal experiment design, Queries, Uncertainty},
abstract = {I consider the question “How should one act when the only goal is to learn as much as possible?”. Building on the theoretical results of Fedorov (1972, Theory of Optimal Experiments, Academic Press) and MacKay (1992, Neural Computation, 4, 590–604), I apply techniques from optimal experiment design (OED) to guide the query/action selection of a neural network learner. I demonstrate that these techniques allow the learner to minimize its generalization error by exploring its domain efficiently and completely. I conclude that, while not a panacea, OED-based query/action selection has much to offer, especially in domains where its high computational costs can be tolerated. Copyright © 1996 Elsevier Science Ltd}
}

% Propose EIVAR-type criterion. 
@article{SinsbeckNowak,
author = {Sinsbeck, Michael and Nowak, Wolfgang},
title = {Sequential Design of Computer Experiments for the Solution of Bayesian Inverse Problems},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {5},
number = {1},
pages = {640-664},
year = {2017},
doi = {10.1137/15M1047659},
URL = { 
    
        https://doi.org/10.1137/15M1047659
},
eprint = {     
        https://doi.org/10.1137/15M1047659
},
abstract = { We present a sequential design strategy for efficient sampling of model functions during the solution of Bayesian inverse problems. The model function is assumed to be computationally expensive and therefore is described by a random field (such as a Gaussian process emulator). The sequential design strategy is a greedy one-step look ahead method, minimizing the Bayes risk with respect to a loss function measuring the quadratic \$L^2\$-error in the likelihood estimate. Four numerical examples demonstrate that the proposed sampling method is more efficient than space-filling, prior-based designs. }
}

% EIVAR paper; computes EIVAR for the Higdon et al PCA-based emulation approach. 
@misc{Surer2023sequential,
      title={Sequential Bayesian experimental design for calibration of expensive simulation models}, 
      author={Özge Sürer and Matthew Plumlee and Stefan M. Wild},
      year={2023},
      eprint={2305.16506},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% Proposes an EIVAR-like acquisition but uses KL-divergence instead of prior-weighted L2 
@inproceedings{KandasamyActiveLearning2015,
author = {Kandasamy, Kirthevasan and Schneider, Jeff and P\'{o}czos, Barnab\'{a}s},
title = {Bayesian Active Learning for Posterior Estimation},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {This paper studies active posterior estimation in a Bayesian setting when the likelihood is expensive to evaluate. Existing techniques for posterior estimation are based on generating samples representative of the posterior. Such methods do not consider efficiency in terms of likelihood evaluations. In order to be query efficient we treat posterior estimation in an active regression framework. We propose two myopic query strategies to choose where to evaluate the likelihood and implement them using Gaussian processes. Via experiments on a series of synthetic and real examples we demonstrate that our approach is significantly more query efficient than existing techniques and other heuristics for posterior estimation.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3605–3611},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

% Lot's of closed-form IVAR calculations. 
@article{Binois_2018,
   title={Replication or Exploration? Sequential Design for Stochastic Simulation Experiments},
   volume={61},
   ISSN={1537-2723},
   url={http://dx.doi.org/10.1080/00401706.2018.1469433},
   DOI={10.1080/00401706.2018.1469433},
   number={1},
   journal={Technometrics},
   publisher={Informa UK Limited},
   author={Binois, Mickaël and Huang, Jiangeng and Gramacy, Robert B. and Ludkovski, Mike},
   year={2018},
   month=sep, pages={7–23} 
  }


% Some closed-form IVAR expressions
% TODO: need to read this more closely 
@article{parallelSURExcursionSet,
author = {Clément Chevalier and Julien Bect and David Ginsbourger and Emmanuel Vazquez and Victor Picheny and Yann Richet},
title = {Fast Parallel Kriging-Based Stepwise Uncertainty Reduction With Application to the Identification of an Excursion Set},
journal = {Technometrics},
volume = {56},
number = {4},
pages = {455--465},
year = {2014},
publisher = {Taylor \& Francis},
doi = {10.1080/00401706.2013.860918},
URL = { 
        https://doi.org/10.1080/00401706.2013.860918
},
eprint = { 
        https://doi.org/10.1080/00401706.2013.860918
}
}

% Original stepwise uncertainty reduction paper
@article{BectSUR,
  title={Sequential design of computer experiments for the estimation of a probability of failure},
  author={Bect, Julien and Ginsbourger, David and Li, Ling and Picheny, Victor and Vazquez, Emmanuel},
  journal={Statistics and Computing},
  volume={22},
  number={3},
  pages={773-793},
  year={2011},
  month={April},
  publisher={Springer Science and Business Media LLC},
  doi={10.1007/s11222-011-9241-4},
  url={http://dx.doi.org/10.1007/s11222-011-9241-4},
  issn={1573-1375}
}

%  ---------------------------------------------------------------------------------------------------
% Ecological Applications
%  ---------------------------------------------------------------------------------------------------

@Manual{vsem,
    title = {BayesianTools: General-Purpose MCMC and SMC Samplers and Tools for Bayesian
Statistics},
    author = {Florian Hartig and Francesco Minunno and Stefan { Paul}},
    year = {2023},
    note = {R package version 0.1.8},
    url = {https://CRAN.R-project.org/package=BayesianTools}
  }

@article{FATES_CES,
   title={Inferring parameters in a complex land surface model by combining data assimilation and machine learning.},
   DOI={DOI: 10.22541/essoar.172070530.05098424/v1},
   journal={ESS Open Archive},
   author={Lasse Torben Keetz and Kristoffer Aalstad and Rosie A. Fisher and Christian Poppe Teran and Bibi S. Naz and Norbert Pirk and Yeliz Yilmaz and Olav Skarpaas},
   year={2024},
   month={July}
  }
  
 @article{CLMBayesianCalibration,
author = {Huang, Maoyi and Ray, Jaideep and Hou, Zhangshuan and Ren, Huiying and Liu, Ying and Swiler, Laura},
title = {On the applicability of surrogate-based Markov chain Monte Carlo-Bayesian inversion to the Community Land Model: Case studies at flux tower sites},
journal = {Journal of Geophysical Research: Atmospheres},
volume = {121},
number = {13},
pages = {7548-7563},
keywords = {Community Land Model, MCMC-Bayesian, surrogate, flux towers},
doi = {https://doi.org/10.1002/2015JD024339},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2015JD024339},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2015JD024339},
year = {2016}
}

@article{CLMSurrogates,
author = {Ray, J. and Hou, Z. and Huang, M. and Sargsyan, K. and Swiler, L.},
title = {Bayesian Calibration of the Community Land Model Using Surrogates},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {3},
number = {1},
pages = {199-233},
year = {2015},
doi = {10.1137/140957998},
URL = { https://doi.org/10.1137/140957998},
eprint = {https://doi.org/10.1137/140957998}
}

@article{
nearTermForecasts,
author = {Michael C. Dietze  and Andrew Fox  and Lindsay M. Beck-Johnson  and Julio L. Betancourt  and Mevin B. Hooten  and Catherine S. Jarnevich  and Timothy H. Keitt  and Melissa A. Kenney  and Christine M. Laney  and Laurel G. Larsen  and Henry W. Loescher  and Claire K. Lunch  and Bryan C. Pijanowski  and James T. Randerson  and Emily K. Read  and Andrew T. Tredennick  and Rodrigo Vargas  and Kathleen C. Weathers  and Ethan P. White },
title = {Iterative near-term ecological forecasting: Needs, opportunities, and challenges},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {7},
pages = {1424-1432},
year = {2018},
doi = {10.1073/pnas.1710231115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1710231115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1710231115}}


% Parameter estimation for land surface models
@unpublished{paramLSM,
  title = {Parameter Estimation in Land Surface Models: Challenges and Opportunities with Data Assimilation and Machine Learning},
  author = {
    Raoult, Nina and
    Douglas, Natalie and
    MacBean, Natasha and
    Kolassa, Jana and
    Quaife, Tristan and
    Roberts, Andrew G. and
    Fisher, Rosie A. and
    Fer, Istem and
    Bacour, Cédric and
    Dagon, Katherine and
    Hawkins, Linnia and
    Carvalhais, Nuno and
    Cooper, Elizabeth and
    Dietze, Michael and
    Gentine, Pierre and
    Kaminski, Thomas and
    Kennedy, Daniel and
    Liddy, Hannah M. and
    Moore, David and
    Peylin, Philippe and
    Pinnington, Ewan and
    Sanderson, Benjamin M. and
    Scholze, Marko and
    Seiler, Christian and
    Smallman, Thomas Luke and
    Vergopolan, Noemi and
    Viskari, Toni and
    Williams, Mathew and
    Zobitz, John
  },
  note = {ESS Open Archive, October 08, 2024},
  doi = {10.22541/essoar.172838640.01153603/v1},
  year = {2024}
}


%  ---------------------------------------------------------------------------------------------------
% Uncertainty Propagation for surrogates
%  ---------------------------------------------------------------------------------------------------

@article{BilionisBayesSurrogates,
  title={Solution of inverse problems with limited forward solver evaluations: a Bayesian perspective},
  author={Ilias Bilionis and Nicholas Zabaras},
  journal={Inverse Problems},
  year={2013},
  volume={30 015004},
  doi = {10.1088/0266-5611/30/1/015004}
}

@article{BurknerSurrogate,
  title={Uncertainty Quantification and Propagation in Surrogate-based Bayesian Inference},
  author={Philipp Reiser and Javier Enrique Aguilar and Anneli Guthke and Paul-Christian Burkner},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.05153},
  url={https://api.semanticscholar.org/CorpusID:266149965}
}

% BurknerTwoStep
@misc{BurknerTwoStep,
      title={Efficient Uncertainty Propagation in Bayesian Two-Step Procedures}, 
      author={Svenja Jedhoff and Hadi Kutabi and Anne Meyer and Paul-Christian Bürkner},
      year={2025},
      eprint={2505.10510},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2505.10510}, 
}

% Cuts in Bayesian graphical models
@article{PlummerCut,
author = {Plummer, Martyn},
title = {Cuts in Bayesian graphical models},
year = {2015},
issue_date = {January   2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {0960-3174},
url = {https://doi.org/10.1007/s11222-014-9503-z},
doi = {10.1007/s11222-014-9503-z},
abstract = {The cut function defined by the OpenBUGS software is described as a "valve" that prevents feedback in Bayesian graphical models. It is shown that the MCMC algorithm applied by OpenBUGS in the presence of a cut function does not converge to a well-defined limiting distribution. However, it may be improved by using tempered transitions. The cut algorithm is compared with multiple imputation as a gold standard in a simple example.},
journal = {Statistics and Computing},
month = jan,
pages = {37–43},
numpages = {7},
keywords = {Multiple imputation, Cutting feedback, Bayesian inference}
}


% applications to cut inference
@article{yvesCut,
 ISSN = {13697412, 14679868},
 URL = {https://www.jstor.org/stable/26937887},
 abstract = {Markov chain Monte Carlo (MCMC) methods provide consistent approximations of integrals as the number of iterations goes to ∞. MCMC estimators are generally biased after any fixed number of iterations. We propose to remove this bias by using couplings of Markov chains together with a telescopic sum argument of Glynn and Rhee. The resulting unbiased estimators can be computed independently in parallel.We discuss practical couplings for popular MCMC algorithms.We establish the theoretical validity of the estimators proposed and study their efficiency relative to the underlying MCMC algorithms. Finally, we illustrate the performance and limitations of the method on toy examples, on an Ising model around its critical temperature, on a high dimensional variable-selection problem, and on an approximation of the cut distribution arising in Bayesian inference for models made of multiple modules.},
 author = {Pierre E. Jacob and John O’Leary and Yves F. Atchadé},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {3},
 pages = {pp. 543--600},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Unbiased Markov chain Monte Carlo methods with couplings},
 urldate = {2025-12-14},
 volume = {82},
 year = {2020}
}


% pkpd cut inference
@article{lunnCut,
   title={Combining MCMC with 'sequential' PKPD modelling},
   DOI={10.1007/s10928-008-9109-1},
   journal={J Pharmacokinet Pharmacodyn},
   author={Lunn D, Best N, Spiegelhalter D, Graham G, Neuenschwander B.},
   year={2009},
   month=jan}

% General framework for modularized Bayesian inference
@article{cutInference,
   title={A general framework for cutting feedback within modularized Bayesian inference},
   ISSN={1467-9868},
   url={http://dx.doi.org/10.1093/jrsssb/qkaf012},
   DOI={10.1093/jrsssb/qkaf012},
   journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
   publisher={Oxford University Press (OUP)},
   author={Liu, Yang and Goudie, Robert J B},
   year={2025},
   month=mar }

% Better together? Statistical learning in models made of modules 
@article{moduleModels,
author = {Jacob, Pierre and Murray, Lawrence and Holmes, Chris and Robert, Christian},
year = {2017},
month = {08},
pages = {},
title = {Better together? Statistical learning in models made of modules},
doi = {10.48550/arXiv.1708.08719}
}

@misc{generalizedCut,
      title={Cutting feedback and modularized analyses in generalized Bayesian inference}, 
      author={David T. Frazier and David J. Nott},
      year={2023},
      eprint={2202.09968},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/2202.09968}, 
}

% Variational semi-modular inference
@misc{cutVar,
      title={Scalable Semi-Modular Inference with Variational Meta-Posteriors}, 
      author={Chris U. Carmona and Geoff K. Nicholls},
      year={2022},
      eprint={2204.00296},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2204.00296}, 
}

% Variational cut inference
@misc{cutVar2,
      title={Variational inference for cutting feedback in misspecified models}, 
      author={Xuejun Yu and David J. Nott and Michael Stanley Smith},
      year={2022},
      eprint={2108.11066},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2108.11066}, 
}

% Multiple imputation in medical research
@article{multipleImputationMedical,
author = {Hayati, Panteha and Lee, Katherine and Simpson, Julie},
year = {2015},
month = {12},
pages = {30},
title = {The rise of multiple imputation: A review of the reporting and implementation of the method in medical research Data collection, quality, and reporting},
volume = {15},
journal = {BMC medical research methodology},
doi = {10.1186/s12874-015-0022-1}
}

% Statistical analysis with missing data
@book{missingData,
  title     = {Statistical Analysis with Missing Data},
  edition   = {3rd},
  author    = {Roderick J. A. Little and Donald B. Rubin},
  year      = {2019},
  publisher = {Wiley},
  series    = {Wiley Series in Probability and Statistics},
  doi       = {10.1002/9781119482260},
  isbn      = {9781119482260},
  note      = {First published 12 April 2019},
}

% Stochastic approximation cut algorithm
@article{SAACut,
   title={Stochastic approximation cut algorithm for inference in modularized Bayesian models},
   volume={32},
   ISSN={1573-1375},
   url={http://dx.doi.org/10.1007/s11222-021-10070-2},
   DOI={10.1007/s11222-021-10070-2},
   number={1},
   journal={Statistics and Computing},
   publisher={Springer Science and Business Media LLC},
   author={Liu, Yang and Goudie, Robert J. B.},
   year={2021},
   month=dec }

% BayesBag for robust inference and model criticism
@misc{BayesBag,
      title={Robust Inference and Model Criticism Using Bagged Posteriors}, 
      author={Jonathan H. Huggins and Jeffrey W. Miller},
      year={2020},
      eprint={1912.07104},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1912.07104}
}

% BayesBag for model selection
@misc{BayesBag2,
      title={Reproducible Model Selection Using Bagged Posteriors}, 
      author={Jonathan H. Huggins and Jeffrey W. Miller},
      year={2021},
      eprint={2007.14845},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2007.14845}
}


% Stand in for our noisy MCMC paper
@unpublished{surrogateNoisyMCMC,
  title={Uncertainty Propagation and Active Learning for Bayesian Inference with Surrogate Models},
  author={Andrew G. Roberts and Jonathan Huggins and Michael Dietze},
  journal={ArXiv},
  year={2025},
  volume={},
  url={}
}

% Stand in for our review paper
@unpublished{reviewPaper,
  title={Probabilistic Surrogates for Bayesian Inverse Problems: Posterior Approximation and Sequential Design},
  author={Andrew G. Roberts and Jonathan Huggins and Michael Dietze},
  journal={ArXiv},
  year={2025},
  volume={},
  url={}
}


%  ---------------------------------------------------------------------------------------------------
% Other adaptive approaches
%  ---------------------------------------------------------------------------------------------------

% GP model for log-likelihood. Use maxvar targeting variance in unnormalized posterior
% density for sequential design.
@article{AlawiehIterativeGP,
title = {Iterative construction of Gaussian process surrogate models for Bayesian inference},
journal = {Journal of Statistical Planning and Inference},
volume = {207},
pages = {55-72},
year = {2020},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2019.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S037837581930103X},
author = {Leen Alawieh and Jonathan Goodman and John B. Bell},
keywords = {Gaussian process regression, Active learning, Surrogate models, Bayesian inference, MCMC}
}

% Local polynomial surrogates constructed within MCMC scheme
@article{Li_2014,
   title={Adaptive Construction of Surrogates for the Bayesian Solution of Inverse Problems},
   volume={36},
   ISSN={1095-7197},
   url={http://dx.doi.org/10.1137/130938189},
   DOI={10.1137/130938189},
   number={3},
   journal={SIAM Journal on Scientific Computing},
   publisher={Society for Industrial & Applied Mathematics (SIAM)},
   author={Li, Jinglai and Marzouk, Youssef M.},
   year={2014},
   month=jan, pages={A1163–A1186} }

% Another local surrogate approach within MCMC algorithm
@article{ConradLocalExactMCMC,
author = {Patrick R. Conrad and Youssef M. Marzouk and Natesh S. Pillai and Aaron Smith and},
title = {Accelerating Asymptotically Exact MCMC for Computationally Intensive Models via Local Approximations},
journal = {Journal of the American Statistical Association},
volume = {111},
number = {516},
pages = {1591--1607},
year = {2016},
publisher = {ASA Website},
doi = {10.1080/01621459.2015.1096787},
URL = { https://doi.org/10.1080/01621459.2015.1096787},
eprint = {https://doi.org/10.1080/01621459.2015.1096787}
}

% Run MCMC with plug-in mean, and then use LHS design wrt a prior-approx posterior 
% mixture to determine the new design.
@article{turbulenceModelAdaptiveLHS,
author = {Zeng, Fanzhi and Zhang, Wei and Li, Jinping and Zhang, Tianxin and Yan, Chao},
year = {2022},
month = {03},
pages = {3502-3516},
title = {Adaptive Model Refinement Approach for Bayesian Uncertainty Quantification in Turbulence Model},
volume = {60},
journal = {AIAA Journal},
doi = {10.2514/1.J060889}
}

% EL Gaussian setting with fully Bayesian GP
@article{Takhtaganov2018AdaptiveBayesianGP,
  title={Adaptive Gaussian process surrogates for Bayesian inference},
  author={Timur Takhtaganov and Juliane M{\"u}ller},
  journal={ArXiv},
  year={2018},
  volume={abs/1809.10784},
  url={https://api.semanticscholar.org/CorpusID:52895082}
}

% Take into account simulator error due to discretization via simple Gaussian error model
@article{Semler_2023,
   title={Adaptive Gaussian process regression for efficient building of surrogate models in inverse problems},
   volume={39},
   ISSN={1361-6420},
   url={http://dx.doi.org/10.1088/1361-6420/ad0028},
   DOI={10.1088/1361-6420/ad0028},
   number={12},
   journal={Inverse Problems},
   publisher={IOP Publishing},
   author={Semler, Phillip and Weiser, Martin},
   year={2023},
   month=oct, pages={125003} }

% Extension of Semler_2023
@misc{Semler2024GradientEnhanced,
      title={Adaptive Gradient Enhanced Gaussian Process Surrogates for Inverse Problems}, 
      author={Phillip Semler and Martin Weiser},
      year={2024},
      eprint={2404.01864},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2404.01864}, 
}

@misc{villani2024posteriorsamplingadaptivegaussian,
      title={Posterior sampling with Adaptive Gaussian Processes in Bayesian parameter identification}, 
      author={Paolo Villani and Daniel Andrés-Arcones and Jörg F. Unger and Martin Weiser},
      year={2024},
      eprint={2411.17858},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2411.17858}, 
}

@article{DelayedAcceptance,
author = {J. Andrés Christen and Colin Fox and},
title = {Markov chain Monte Carlo Using an Approximation},
journal = {Journal of Computational and Graphical Statistics},
volume = {14},
number = {4},
pages = {795--810},
year = {2005},
publisher = {ASA Website},
doi = {10.1198/106186005X76983},
URL = {https://doi.org/10.1198/106186005X76983},
eprint = {https://doi.org/10.1198/106186005X76983}
}


% ---------------------------------------------------------------------------------------------------
% Approximate Bayesian Computation (ABC)
% ---------------------------------------------------------------------------------------------------

% An approximate likelihood perspective on ABC methods
@article{ABCApproxLik,
author = {George Karabatsos and Fabrizio Leisen},
title = {{An approximate likelihood perspective on ABC methods}},
volume = {12},
journal = {Statistics Surveys},
number = {none},
publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
pages = {66 -- 104},
keywords = {Approximate Bayesian Computation, approximate likelihood, Bootstrap likelihood, empirical likelihood},
year = {2018},
doi = {10.1214/18-SS120},
URL = {https://doi.org/10.1214/18-SS120}
}

% Likelihood-free inference perspective. Emulate likelihood with neural network ensemble. Use MaxVar and mutual information acquisitions to acquire new points. 
@article{Lueckmann2018LikelihoodfreeIW,
  title={Likelihood-free inference with emulator networks},
  author={Jan-Matthis Lueckmann and Giacomo Bassetto and Theofanis Karaletsos and Jakob H. Macke},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.09294},
  url={https://api.semanticscholar.org/CorpusID:43920918}
}

% Uncertainty-aware amortized inference
@misc{BurknerAmortized,
      title={Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for Computationally Expensive Models}, 
      author={Stefania Scheurer and Philipp Reiser and Tim Brünnette and Wolfgang Nowak and Anneli Guthke and Paul-Christian Bürkner},
      year={2025},
      eprint={2505.08683},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2505.08683}, 
}

% Application to ABC. Emulate log-likelihood with quadratic mean function.
@InProceedings{llikEmABC,
  title = 	 {{Accelerating ABC methods using Gaussian processes}},
  author = 	 {Wilkinson, Richard},
  booktitle = 	 {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1015--1023},
  year = 	 {2014},
  editor = 	 {Kaski, Samuel and Corander, Jukka},
  volume = 	 {33},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Reykjavik, Iceland},
  month = 	 {22--25 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v33/wilkinson14.pdf},
  url = 	 {https://proceedings.mlr.press/v33/wilkinson14.html}
}

% Use GP to emulate log-discrepancy function in ABC. Include quadratic trend.
@article{ABCGP,
  author  = {Michael U. Gutmann and Jukka Cor and er},
  title   = {Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {125},
  pages   = {1--47},
  url     = {http://jmlr.org/papers/v17/15-017.html}
}


%
%
% TODO: not yet categorized
%
%


% ---------------------------------------------------------------------------------------------------
% Emulating dynamic simulators / simulators with high-dimensional output
% ---------------------------------------------------------------------------------------------------

@article{FadikarAgentBased,
author = {Fadikar, Arindam and Higdon, Dave and Chen, Jiangzhuo and Lewis, Bryan and Venkatramanan, Srinivasan and Marathe, Madhav},
title = {Calibrating a Stochastic, Agent-Based Model Using Quantile-Based Emulation},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {6},
number = {4},
pages = {1685-1706},
year = {2018},
doi = {10.1137/17M1161233},
URL = {https://doi.org/10.1137/17M1161233},
eprint = {https://doi.org/10.1137/17M1161233}
}


@Article{acp-11-12253-2011,
AUTHOR = {Lee, L. A. and Carslaw, K. S. and Pringle, K. J. and Mann, G. W. and Spracklen, D. V.},
TITLE = {Emulation of a complex global aerosol model to quantify sensitivity to uncertain parameters},
JOURNAL = {Atmospheric Chemistry and Physics},
VOLUME = {11},
YEAR = {2011},
NUMBER = {23},
PAGES = {12253--12273},
URL = {https://acp.copernicus.org/articles/11/12253/2011/},
DOI = {10.5194/acp-11-12253-2011}
}



@article{REICHERT20111638,
title = {Mechanism-based emulation of dynamic simulation models: Concept and application in hydrology},
journal = {Computational Statistics & Data Analysis},
volume = {55},
number = {4},
pages = {1638-1655},
year = {2011},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2010.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167947310003993},
author = {P. Reichert and G. White and M.J. Bayarri and E.B. Pitman},
keywords = {Dynamic model, Emulator, Optimization, Sensitivity analysis, Statistical inference},
abstract = {Many model-based investigation techniques, such as sensitivity analysis, optimization, and statistical inference, require a large number of model evaluations to be performed at different input and/or parameter values. This limits the application of these techniques to models that can be implemented in computationally efficient computer codes. Emulators, by providing efficient interpolation between outputs of deterministic simulation models, can considerably extend the field of applicability of such computationally demanding techniques. So far, the dominant techniques for developing emulators have been priors in the form of Gaussian stochastic processes (GASP) that were conditioned with a design data set of inputs and corresponding model outputs. In the context of dynamic models, this approach has two essential disadvantages: (i) these emulators do not consider our knowledge of the structure of the model, and (ii) they run into numerical difficulties if there are a large number of closely spaced input points as is often the case in the time dimension of dynamic models. To address both of these problems, a new concept of developing emulators for dynamic models is proposed. This concept is based on a prior that combines a simplified linear state space model of the temporal evolution of the dynamic model with Gaussian stochastic processes for the innovation terms as functions of model parameters and/or inputs. These innovation terms are intended to correct the error of the linear model at each output step. Conditioning this prior to the design data set is done by Kalman smoothing. This leads to an efficient emulator that, due to the consideration of our knowledge about dominant mechanisms built into the simulation model, can be expected to outperform purely statistical emulators at least in cases in which the design data set is small. The feasibility and potential difficulties of the proposed approach are demonstrated by the application to a simple hydrological model.}
}


@article{doi:10.1137/120900915,
author = {Williamson, Daniel and Blaker, Adam T.},
title = {Evolving Bayesian Emulators for Structured Chaotic Time Series, with Application to Large Climate Models},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {2},
number = {1},
pages = {1-28},
year = {2014},
doi = {10.1137/120900915},
URL = { 
        https://doi.org/10.1137/120900915
},
eprint = {   
        https://doi.org/10.1137/120900915
},
    abstract = { We develop Bayesian dynamic linear model Gaussian processes for emulation of time series output for computer models that may exhibit chaotic behavior, but where this behavior retains some underlying structure. The statistical technology is particularly suited to emulating the time series output of large climate models that exhibit this feature and where we want samples from the posterior of the emulator to evolve in the same way as dynamic processes in the computer model do. The methodology combines key features of good uncertainty quantification (UQ) methods such as using complex mean functions to capture large-scale signals within parameter space, with dynamic linear models in a way that allows UQ to borrow strength from the Bayesian time series literature. We present an MCMC algorithm for sampling from the posterior of the emulator parameters when the roughness lengths of the Gaussian process are unknown. We discuss an interpretation of the results of this algorithm that allows us to use MCMC to fix the correlation lengths, making future online samples from the emulator tractable when used in practical applications where online MCMC is infeasible. We apply this methodology to emulate the Atlantic Meridional Overturning Circulation (AMOC) as a time series output of the fully coupled non--flux-adjusted atmosphere-ocean general circulation model HadCM3. }
}


@article{calibrationSpatioTemporal,
author = { Matthew T.   Pratola  and  Stephan R.   Sain  and  Derek   Bingham  and  Michael   Wiltberger  and  E.   Joshua   Rigler },
title = {Fast Sequential Computer Model Calibration of Large Nonstationary Spatial-Temporal Processes},
journal = {Technometrics},
volume = {55},
number = {2},
pages = {232-242},
year  = {2013},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.2013.775897},
URL = { 
        https://doi.org/10.1080/00401706.2013.775897
},
eprint = { 
    
        https://doi.org/10.1080/00401706.2013.775897
}
}

@article{PERRIN2020106728,
title = {Adaptive calibration of a computer code with time-series output},
journal = {Reliability Engineering & System Safety},
volume = {196},
pages = {106728},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106728},
url = {https://www.sciencedirect.com/science/article/pii/S0951832018311232},
author = {G. Perrin},
keywords = {Bayesian framework, Computer experiment, Dynamic simulator, Gaussian process, Multi-output},
abstract = {Simulation plays a major role in the conception, the optimization and the certification of complex systems. Of particular interest here is the calibration of the parameters of computer models from high-dimensional physical observations. When the run times of these computer codes is high, this work focuses on the numerical challenges associated with the statistical inference. In particular, several adaptations of the Gaussian Process Regression (GPR) to the high-dimensional or functional output case are presented for the emulation of computer codes from limited data. Then, an adaptive procedure is detailed to minimize the calibration parameters uncertainty at the minimal computational cost. The proposed method is eventually applied to two applications that are based on dynamic simulators.}
}




% ---------------------------------------------------------------------------------------------------
% Scaling GPs. 
% ---------------------------------------------------------------------------------------------------

% TODO: need to read
@misc{cole2021locally,
      title={Locally induced Gaussian processes for large-scale simulation experiments}, 
      author={D. Austin Cole and Ryan Christianson and Robert B. Gramacy},
      year={2021},
      eprint={2008.12857},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% TODO: need to read 
@article{doi:10.1080/10618600.2014.914442,
author = {Robert B. Gramacy and Daniel W. Apley},
title = {Local Gaussian Process Approximation for Large Computer Experiments},
journal = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {2},
pages = {561-578},
year  = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2014.914442},
URL = { 
    
        https://doi.org/10.1080/10618600.2014.914442
},
eprint = {    
        https://doi.org/10.1080/10618600.2014.914442
}
}



% Classic paper on batch BayesOpt; Kriging believer and constant liar heuristics for approximating multi-points EI. 
@Inbook{Ginsbourger2010,
author="Ginsbourger, David
and Le Riche, Rodolphe
and Carraro, Laurent",
editor="Tenne, Yoel
and Goh, Chi-Keong",
title="Kriging Is Well-Suited to Parallelize Optimization",
bookTitle="Computational Intelligence in Expensive Optimization Problems",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="131--162",
abstract="The optimization of expensive-to-evaluate functions generally relies on metamodel-based exploration strategies. Many deterministic global optimization algorithms used in the field of computer experiments are based on Kriging (Gaussian process regression). Starting with a spatial predictor including a measure of uncertainty, they proceed by iteratively choosing the point maximizing a criterion which is a compromise between predicted performance and uncertainty. Distributing the evaluation of such numerically expensive objective functions on many processors is an appealing idea. Here we investigate a multi-points optimization criterion, the multipoints expected improvement ({\$}q-{\{}{\backslash}mathbb E{\}}I{\$}), aimed at choosing several points at the same time. An analytical expression of the {\$}q-{\{}{\backslash}mathbb E{\}}I{\$}is given when q{\thinspace}={\thinspace}2, and a consistent statistical estimate is given for the general case. We then propose two classes of heuristic strategies meant to approximately optimize the {\$}q-{\{}{\backslash}mathbb E{\}}I{\$}, and apply them to the classical Branin-Hoo test-case function. It is finally demonstrated within the covered example that the latter strategies perform as good as the best Latin Hypercubes and Uniform Designs ever found by simulation (2000 designs drawn at random for every q{\thinspace}∈ [1,10]).",
isbn="978-3-642-10701-6",
doi="10.1007/978-3-642-10701-6_6",
url="https://doi.org/10.1007/978-3-642-10701-6_6"
}


% Orthogonal array based LHS; discussion of different methods for batch sequential design. Emphasis on space-filling. 
@article{LOEPPKY20101452,
title = {Batch sequential designs for computer experiments},
journal = {Journal of Statistical Planning and Inference},
volume = {140},
number = {6},
pages = {1452-1464},
year = {2010},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2009.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378375809003747},
author = {Jason L. Loeppky and Leslie M. Moore and Brian J. Williams},
keywords = {Computer experiment, Gaussian process, Random function, Latin hypercube sample, Maximin distance, Entropy},
abstract = {Computer models simulating a physical process are used in many areas of science. Due to the complex nature of these codes it is often necessary to approximate the code, which is typically done using a Gaussian process. In many situations the number of code runs available to build the Gaussian process approximation is limited. When the initial design is small or the underlying response surface is complicated this can lead to poor approximations of the code output. In order to improve the fit of the model, sequential design strategies must be employed. In this paper we introduce two simple distance based metrics that can be used to augment an initial design in a batch sequential manner. In addition we propose a sequential updating strategy to an orthogonal array based Latin hypercube sample. We show via various real and simulated examples that the distance metrics and the extension of the orthogonal array based Latin hypercubes work well in practice.}
}

% Federov Exchange
@book{FederovExchange,
author = {Fedorov, Valerii},
year = {1972},
month = {01},
pages = {},
title = {Theory of Optimal Experiments Designs}
}

% Federov Exchange in discrete space
@article{WynnDiscreteExchange,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239871},
 author = {Henry P. Wynn},
 journal = {The Annals of Mathematical Statistics},
 number = {5},
 pages = {1655--1664},
 publisher = {Institute of Mathematical Statistics},
 title = {The Sequential Generation of D-Optimum Experimental Designs},
 urldate = {2025-05-27},
 volume = {41},
 year = {1970}
}

% Thesis on batch Bayesian optimization
@phdthesis{batchBOThesis,
  author       = {Nathan Hunt},
  title        = {Batch Bayesian Optimization},
  school       = {Massachusetts Institute of Technology},
  year         = {2020},
  type         = {Ph.D. thesis}, 
  url          = {https://hdl.handle.net/1721.1/128591}
}

% TODO: need to read
@article{PANDITA2021114007,
title = {Surrogate-based sequential Bayesian experimental design using non-stationary Gaussian Processes},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {385},
pages = {114007},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.114007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521003388},
author = {Piyush Pandita and Panagiotis Tsilifis and Nimish M. Awalgaonkar and Ilias Bilionis and Jitesh Panchal},
keywords = {Design of experiments, Non-stationary Gaussian Processes, Expected Information Gain, Sequential designs, Bayesian inference},
abstract = {Inferring arbitrary quantities of interest (QoI) using limited computational or, in realistic scenarios, financial budgets, is a challenging problem that requires sophisticated strategies for the optimal allocation of the available resources. Bayesian optimal experimental design identifies the optimal set of design locations for the purpose of solving a parameter inference problem and the optimality criterion is typically associated with maximizing the worth of information in the experimental measurements. Sequential design strategies further identify the optimal design in a sequential manner, starting from a initial budget and iteratively selecting new optimal points until either an accuracy threshold is reached, or a cost limit is exceeded. In this paper, we present a generic sequential Bayesian experimental design framework that relies on maximizing an information theoretic design criterion, namely the Expected Information Gain, in order to infer QoIs formed as nonlinear operators acting on black-box functions. Our framework relies on modeling the underlying response function using non-stationary Gaussian Processes, thus enabling efficient sampling from the QoI in order to provide Monte Carlo estimators for the design criterion. We demonstrate the performance of our method on an engineering problem of steel wire manufacturing and compare it with two classic approaches: uncertainty sampling and expected improvement.}
}


% TODO: need to read
@article{RoshanMinEnergy,
author = {Joseph, V. Roshan and Dasgupta, Tirthankar and Tuo, Rui and Wu, Chi-Fang},
year = {2014},
month = {11},
pages = {00-00},
title = {Sequential Exploration of Complex Surfaces Using Minimum Energy Designs},
volume = {57},
journal = {Technometrics},
doi = {10.1080/00401706.2014.881749}
}

% TODO: need to read
@article{Huan2013,
	doi = {10.1016/j.jcp.2012.08.013},
	url = {https://doi.org/10.1016%2Fj.jcp.2012.08.013},  
	year = 2013,
	month = {jan},  
	publisher = {Elsevier {BV}},  
	volume = {232},  
	number = {1},  
	pages = {288--317},
	author = {Xun Huan and Youssef M. Marzouk},  
	title = {Simulation-based optimal Bayesian experimental design for nonlinear systems},  
	journal = {Journal of Computational Physics}
}

% TODO: need to read
@misc{gramacy2010particle,
      title={Particle learning of Gaussian process models for sequential design and optimization}, 
      author={Robert B. Gramacy and Nicholas G. Polson},
      year={2010},
      eprint={0909.5262},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

% TODO: need to read 
@misc{zhang2019distancedistributed,
      title={Distance-distributed design for Gaussian process surrogates}, 
      author={Boya Zhang and D. Austin Cole and Robert B. Gramacy},
      year={2019},
      eprint={1812.02794},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% TODO: need to read 
@misc{koermer2023active,
      title={Active Learning for Simulator Calibration}, 
      author={Scott Koermer and Justin Loda and Aaron Noble and Robert B. Gramacy},
      year={2023},
      eprint={2301.10228},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

% Derive closed-form expression for multi-point EI. 
@inproceedings{Chevalier2013,
author = {Chevalier, Clément},
year = {2013},
month = {01},
pages = {},
title = {Fast Computation of the Multi-Points Expected Improvement with Applications in Batch Selection},
isbn = {978-3-642-44972-7},
doi = {10.1007/978-3-642-44973-4_7}
}

% Derive an expression for the gradient of the closed-form multi-point EI. 
@inproceedings{Marmin2015,
author = {Marmin, Sébastien and Chevalier, Clément and Ginsbourger, David},
year = {2015},
month = {03},
pages = {},
title = {Differentiating the Multipoint Expected Improvement for Optimal Batch Design},
isbn = {978-3-319-27925-1},
doi = {10.1007/978-3-319-27926-8_4}
}

% Hierarchical EI
@misc{chen2023hierarchical,
      title={A hierarchical expected improvement method for Bayesian optimization}, 
      author={Zhehui Chen and Simon Mak and C. F. Jeff Wu},
      year={2023},
      eprint={1911.07285},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}





