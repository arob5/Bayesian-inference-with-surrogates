\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{caption, subcaption} % Captions and sub-figures. 
\usepackage{fancyvrb} % For writing using verbatim font inline. 
% \usepackage[demo]{graphicx}

% For tables
\usepackage{array}
\usepackage{booktabs}

% Bibliography
\bibliographystyle{plainnat}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Tables. 
\usepackage{multirow}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{{./figures/}}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Uncertainty Propagation and Active Learning for Bayesian Inference with Surrogate Models}
\author{Andrew Roberts}

\begin{document}

\maketitle

Simulation-based computer models have emerged as a key tool for studying complex systems 
within the physical, biological, and engineering sciences. Such models often have uncertain 
parameters that must be estimated from data. Bayesian methods are commonly employed
to quantify parameter uncertainty in these calibration tasks.
However, standard Bayesian inference algorithms such as Markov chain Monte Carlo (MCMC) 
are often hindered by the computational cost of the simulation model.
An increasingly popular approach to deal with this issue is to train a surrogate model 
(i.e., emulator) as a computationally thrifty approximation of the expensive computer code. 
While the use of surrogates offers the potential for significant computational savings, 
they introduce a new source of uncertainty within the Bayesian model. Recent work 
has emphasized the importance of propagating this uncertainty in surrogate-based 
Bayesian workflows \citep{reviewPaper,BurknerSurrogate,CES,FerEmulation}.

Common surrogate models such as Gaussian processes 
\citep{gpML,gramacy2020surrogates} and probabilistic 
neural networks \citep{deepEnsembles,BayesOptNN} produce predictions in the form 
of a probability distribution, thus quantifying the uncertainty in the surrogate. 
These models may be built to approximate various target maps, including the 
simulator itself, low-dimensional summaries of the simulator output \citep{CES}, or 
the log-likelihood defining the statistical model 
\citep{FerEmulation,VehtariParallelGP,OakleyllikEm}. In all of these cases, the 
probabilistic surrogate ultimately induces a stochastic approximation of the 
unnormalized posterior density, which is the key input to standard inference
algorithms. A variety of methods have proposed to construct a deterministic 
posterior approximation using the resulting random density
\citep{StuartTeck1,SinsbeckNowak,VehtariParallelGP,BurknerSurrogate,
BurknerTwoStep,garegnani2021NoisyMCMC,FerEmulation}. 

In practice, surrogate models are typically constructed sequentially by running 
batches of simulator evaluations at different inputs over a sequence of 
rounds. The task of choosing the new points at which to run the simulator 
presents an active learning (i.e., sequential design) problem. Recent work 
has emphasized that the new inputs should be selected with respect to 
the target goal of posterior approximation, which requires more 
simulation effort in regions of high posterior mass 
\citep{StuartTeck2,SinsbeckNowak,VehtariParallelGP,Surer2023sequential,
gp_surrogates_random_exploration,weightedIVAR,adaptiveMultimodal}. 
A variety of objective functions have been proposed to guide the selection
of inputs in this context \citep{SinsbeckNowak,VehtariParallelGP,
Surer2023sequential, gp_surrogates_random_exploration,weightedIVAR}.

[\todo: add short paragraph on batch design]

\paragraph{Contributions}
The main contribution of this paper is a probabilistic pipeline for surrogate-based Bayesian 
inference, including (i.) algorithms for constructing posterior approximations that propagate 
the surrogate uncertainty, and (ii.) active learning algorithms for refining the surrogate model
with regard to the goal of improving the posterior approximation.
To address the former, we start by studying the strengths and limitations of
various posterior approximations proposed in the literature. Guided by this
investigation, we propose a new approach to surrogate-based approximate inference 
rooted in noisy approximations to MCMC algorithms \citep{noisyMCMC}.
We next address the batch sequential design question 

Unlike many existing methods, we develop a framework that is agnostic to both the predictive
distribution of the surrogate and the particular form of the Bayesian statistical model. In particular, 
we assume only that samples can be drawn from the surrogate predictive distribution. 
We also propose workflows applicable to large-scale applications, where
computational costs allow for only a few batches of model runs.  


including algorithms for propagating surrogate uncertainty and sequentially
refining the surrogate model. 

The main contribution of this paper is introduce a new framework for conducting surrogate-based
posterior inference based around approximating the acceptance probability of Metropolis-Hastings
algorithms. In particular, we develop practical noisy MCMC algorithms that require only the ability
to sample from the predictive distribution of the surrogate model. We also propose several (batch) 
sequential design algorithms that target improvement in the posterior approximation. 
Finally, we provide practical recommendations on emulating unnormalized posterior densities, 
including prior predictive checks for avoiding common pathologies. 

% Surrogates for Bayesian Inverse Problems
\section{Surrogates for Bayesian Inverse Problems}

\subsection{Bayesian Inference Setting}
Our general goal is to estimate parameters $\Par \in \parSpace \subseteq \R^{\dimPar}$ given 
observations $\obs \in \obsSpace \subseteq \R^{\dimObs}$ within a Bayesian framework.
Assuming a prior density $\priorDens(\Par)$ and likelihood $p(\obs \given \Par)$, 
we seek to characterize the posterior distribution 
\begin{align}
&\postDensNorm(\Par) \Def p(\Par \given \obs) = \frac{1}{\normCst} \priorDens(\Par) p(\obs \given \Par), 
&&\normCst = \int_{\parSpace} \priorDens(\Par) p(\obs \given \Par) d\Par, \label{post_dens_generic}
\end{align}
where $\normCst$ is an (intractable) normalizing constant. 
Let $\postDens(\Par) \Def \priorDens(\Par) p(\obs \given \Par)$ denote the unnormalized posterior 
density, which plays a central role in our development. 
We assume that pointwise evaluations of  $\postDens(\Par)$ can be computed, but at significant 
computational expense. This renders standard iterative optimization and sampling algorithms 
(e.g., MCMC) infeasible, as each iteration requires a new density evaluation $\postDens(\Par)$.
To address this computational bottleneck, typical surrogate workflows ultimately induce 
a (random) approximation to $\postDens(\Par)$.
In the following, we motivate this setup by introducing common surrogate modeling strategies used 
in solving Bayesian inverse problems, and describe how each approach results in a 
stochastic approximation to $\postDens(\Par)$.

\subsection{Bayesian Inverse Problems}
The challenge posed by computationally expensive density evaluations $\postDens(\Par)$ commonly 
arises in the Bayesian approach to inverse problems \citep{Stuart_BIP}. In this setting, 
the likelihood often takes the form $\obs = \fwd(\Par) + \noise$ for some forward model
$\fwd: \parSpace \to \obsSpace$. For a concrete example, we consider the problem of estimating the 
parameters in a system of ordinary differential equations (ODEs)
\begin{align}
\frac{d}{d\Time} \state(\Time, \Par) &= \odeRHS(\state(\Time, \Par), \Par), &&x(\timeStart) = \stateIC, \label{ode_ivp}
\end{align}
where the dynamics depend on parameters $\Par$. Each value for $\Par$ implies a different state trajectory
$[\state(\Time, \Par)]_{\timeStart \leq \Time \leq \timeEnd}$, which we encode by the
map $\solutionOp: \Par \mapsto [\state(\Time, \Par)]_{\timeStart \leq \Time \leq \timeEnd.}$. The goal is then 
to identify the parameters that yield state trajectories that are in agreement with observed data 
$\obs$, which is assumed to be some noise-corrupted function $\obsOp$ of the true trajectory. Thus, the 
likelihood is of the form 
\begin{align}
&\obs = \fwd(\Par) + \noise, &&\fwd \Def \obsOp \circ \solutionOp.
\end{align}
In practice, the ODE is solved numerically so $\solutionOp$ represents the map induced by a numerical 
solver. Therefore, in this setting the computational cost of computing $\postDens(\Par)$ stems from the 
dependence of the likelihood on $\fwd(\Par)$, and in particular on the solver $\solutionOp(\Par)$.

\subsection{Common Surrogate Strategies} \label{sec:surrogate-examples}
Given the cost of computing $\postDens(\Par)$, we seek to approximate the posterior
using minimal queries to the posterior density. In addition, in many practical
settings it is necessary that these density evaluations be computed in parallel. 
A popular approach satisfying these criteria involves generating an 
\textit{initial design} $\{(\Par_{\idxDesign}, \postDens(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$ 
(in parallel), fitting a regression model $\postEm[\Ndesign](\Par)$ to approximate 
the map $\Par \mapsto \postDens(\Par)$, and then using the approximation $\postEm[\Ndesign](\Par)$
in place of the exact density. We refer to $\postEm[\Ndesign](\Par)$ as the 
\textit{surrogate density}, with the subscript indicating the number of design points
used for training. Typically, the 


A common solution to this computational bottleneck is to learn a statistical approximation of the map 
$\Par \mapsto \postDens(\Par)$, which we refer to synonymously as a \textit{surrogate} or \textit{emulator}
model. Learning the surrogate typically consists of an initial offline step where training data 
is generated by running the expensive simulator at a set of input parameter values. A regression model 
is then fit to this training set to learn an approximation of the input-output relationship 
$\Par \mapsto \postDens(\Par)$. Our interest is in \textit{probabilistic} surrogate models, which output
a prediction for $\postDens(\Par)$ in the form of a probability distribution, as opposed to just a point estimate.
We let $\postEm[\Ndesign](\Par)$ denote the random variable representing the surrogate prediction 
of $\postDens(\Par)$, with the subscript indicating the number of exact simulation runs that were used 
to train the surrogate. We refer to the distribution of $\postEm[\Ndesign](\Par)$ as the surrogate 
\textit{predictive distribution} at input $\Par$; we allow this distribution to be arbitrary, and potentially 
only accessible through samples. Note that the map $\Par \mapsto \postDens(\Par)$ is typically not 
approximated directly; rather, an emulator is fit to a quantity on which $\postDens(\Par)$ depends 
(e.g., the log-likelihood or an underlying mechanistic model), which then induces a random approximation 
of $\postDens(\Par)$. Our framework thus seeks to be agnostic to the particular quantity that is 
being emulated. Concrete examples of typical surrogate modeling workflows are given in \cref{sec:surrogate-examples}.

Given $\postEm[\Ndesign]$, a fixed random approximation of $\postDens$, a natural question is how to 
utilize this surrogate in approximating the posterior distribution in \cref{post_dens_generic}. \Cref{sec:post-approx}
is dedicated to this question, in which we summarize previous approaches and present our proposed method.




We focus on the Bayesian inference setting in which pointwise evaluations of the unnormalized posterior
density 
\begin{equation}
\postDens(\Par) \Def \priorDens(\Par) p(\obs \given \Par) \label{post_dens_generic}
\end{equation}
are available, but expensive owing to the cost of computing the likelihood $p(\obs \given \Par)$. This setting 
commonly arises in the Bayesian approach to inverse problems \citep{Stuart_BIP}. 
Consider a \textit{forward model} $\fwd: \parSpace \subseteq \R^{\dimPar} \to \R^{\dimObs}$ describing 
some system of interest, parameterized by input parameters $\Par \in \parSpace$. In addition, suppose that 
we have noisy observations $\obs \in \obsSpace \subseteq \R^{\dimObs}$ of the output signal that $\fwd(\Par)$ 
seeks to approximate. The \textit{inverse problem} concerns learning the parameter values $\Par$ such
 that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating} the model so that it agrees with the observations. 
 The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on $\obs \given \Par$. We assume that this distribution 
admits a density with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs \given \Par) = \Exp{\llik(\Par)}$. The notation $\llik(\Par)$ suppresses the dependence 
on $\obs$, as the observed data will be fixed throughout. We start by focusing inference only on the 
calibration parameter $\Par$, assuming that other likelihood parameters (e.g., noise covariance) 
are fixed. We discuss inference for such nuisance parameters in \Cref{section_lik_par}.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution
\begin{align}
\postDensNorm(\Par) \Def p(\Par \given \obs) = \frac{1}{\normCst}\postDens(\Par) = \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant given by 
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. We will also find it useful to introduce the notation
\begin{equation}
\lpost(\Par) \Def \log \postDens(\Par) = \log \priorDens(\Par) + \llik(\Par) \label{eq:lpost}
\end{equation}
for the logarithm of the unnormalized posterior density.
When relevant, we will make explicit the dependence on the forward 
model by writing $\llik(\Par; \fwd)$, $\lpost(\Par; \fwd)$, and $\postDens(\Par; \fwd)$. 

In addition to considering 
generic likelihoods, we will give special attention to the additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{inv_prob_Gaussian} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par; \fwd) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) 
= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)), \label{llik_Gaussian}
\end{align}
as this model features prominently in the inverse problems literature.
In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are serial in nature, often requiring $\sim 10^5 - 10^7$ iterations, with each 
iteration involving an evaluation of the unnormalized posterior density $\postDens(\Par; \fwd)$. 
In the inverse problem context, this computational requirement is often prohibitive when the forward model 
evaluations $\fwd(\Par)$ incur significant computational cost, as each density evaluation requires the 
expensive computation $\fwd(\Par)$. Motivated by this problem, a large body of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\lpost(\Par)$ (note that approximating the former induces 
an approximation of the latter). We focus on surrogate models that take the form of statistical 
regression models approximating the maps $\Par \mapsto \fwd(\Par)$ or $\Par \mapsto \lpost(\Par)$.
\footnote{This is in contrast to other surrogate modeling strategies (e.g., reduced-order modeling)
that exploit the specific structure of the forward model; see, e.g., \todo.}
We refer to methods that explicitly model the former map as \textit{forward model emulation}, and those that 
model the latter as \textit{log-density emulation}. We also include methods that emulate the log-likelihood
map $\Par \mapsto \llik(\Par)$ in this latter category (see \Cref{sec:llik_vs_lpost}).
Throughout this review, we will typically view the functions
$\fwd(\Par)$, $\llik(\Par)$, and $\lpost(\Par)$ as computationally expensive black-boxes. We will occasionally refer to 
such maps as \textit{simulators}, owing to the fact that in typical applications evaluating these maps requires
running an expensive computer code. The following section provides a concrete example of such a simulation
model stemming from the numerical solution of differential equations.


\bibliography{prob_surrogates_bayes} 
% \bibliographystyle{ieeetr}

\end{document}







