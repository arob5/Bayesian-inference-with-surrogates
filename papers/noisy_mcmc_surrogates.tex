\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{caption, subcaption} % Captions and sub-figures. 
\usepackage{fancyvrb} % For writing using verbatim font inline. 
% \usepackage[demo]{graphicx}

% For tables
\usepackage{array}
\usepackage{booktabs}

% Bibliography
\bibliographystyle{plainnat}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Tables. 
\usepackage{multirow}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{{./figures/}}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{rmk}{Remark}

% Ensure cleverref knows how to handle citing of multiple subsubsections
\crefname{subsubsection}{section}{sections}
\Crefname{subsubsection}{Section}{Sections}

% Title and author
\title{Uncertainty Propagation in Surrogate-Based Bayesian Inference}
\author{Andrew Roberts, Jonathan Huggins, Michael Dietze}

\begin{document}

\maketitle

\begin{abstract}
Standard Bayesian inference schemes are infeasible for inverse problems 
with computationally expensive forward models. A common solution is to 
replace the model with a cheaper surrogate. To avoid overconfident 
conclusions, it is essential to acknowledge the surrogate approximation by propagating 
its uncertainty. However, at present there is no standard method for uncertainty 
propagation in this context. To fill this gap, we propose a particular uncertainty-aware 
posterior approximation as the baseline, which is justified by Bayesian decision theoretic
arguments.  
\end{abstract}

\section{Introduction}
Simulation-based computer models are key tools for studying complex systems within 
the physical, biological, and engineering sciences. Such models often have 
uncertain parameters that must be estimated (i.e., calibrated) using observational data.
Quantifying the uncertainty in these estimated values is crucial for downstream 
decision making. While Bayesian methods are particularly well-suited to this task, 
standard Bayesian inference algorithms such as Markov chain Monte Carlo (MCMC) 
are hindered by the computational cost of the simulation model. A popular solution 
is to use a small set of expensive simulations to train a statistical approximation 
of the simulator. This emulator (i.e., surrogate) is then used as a drop-in replacement 
for the true computer model, enabling the application of algorithms like MCMC. 
This modular surrogate-based Bayesian workflow has seen widespread use across
a variety of applications \citep{FerEmulation,FadikarAgentBased,idealizedGCM,
trainDynamics,FATES_CES,CLMBayesianCalibration}.

Despite significant advances in surrogate modeling, fitting a 
highly accurate emulator under a limited computational budget remains a challenging task.
Given this reality, it is inevitable that emulator errors will propagate to estimates
of the calibration parameters. Ignoring these errors can lead to overconfident results 
with miscalibrated uncertainties \citep{BilionisBayesSurrogates,BurknerSurrogate}.
It is thus crucial to acknowledge and propagate this additional source of uncertainty in 
surrogate-based Bayesian workflows. 
Probabilistic surrogates such as Gaussian processes (GPs; \citet{gpML,gramacy2020surrogates}) 
and probabilistic neural networks \citep{deepEnsembles,BayesOptNN} provide a notion of 
predictive uncertainty that be can utilized to this end.

While in principle surrogate and calibration parameters can be learned 
jointly (e.g., \citet{KOH}), in practice it is more common to conduct inference 
for these quantities in two distinct stages \citep{modularization,PlummerCut}.
Such decoupling has several practical benefits, but leaves open the 
question as to the ``correct'' approach for propagating surrogate uncertainty 
within the posterior approximation in the second stage.
A variety of uncertainty-aware posterior 
approximations have been proposed, but little guidance exists on choosing a particular 
method \citep{reviewPaper,BilionisBayesSurrogates,StuartTeck1,VehtariParallelGP,
BurknerSurrogate,BurknerTwoStep,FerEmulation}. Moreover, previous studies have 
explicitly cited computational challenges as a key factor in determining their 
approach \citep{VehtariParallelGP,StuartTeck2}. In this paper, we start by setting 
aside the computational considerations in order to specify a theoretically-justified 
baseline posterior approximation. We then analyze existing methods as 
approximations to this baseline, and provide a new MCMC scheme that outperforms 
these approximations. 

\subsection{Contributions.}
\begin{itemize}
\item We adopt a Bayesian decision theoretic viewpoint to formalize the question of 
choosing the correct uncertainty propagation method. We show that, with respect 
to common loss functions, the Bayes' estimator is given by a mixture distribution 
termed the \textit{expected posterior (EP)}.
\item We analyze an existing alternative, the \textit{expected unnormalized posterior (EUP)},
as an approximation to the EP. We clarify when this approximation is reasonable and 
when it can deviate significantly from the EP baseline.
\item We provide additional insights tailored to two popular modeling setups: 
(i.) emulating the log-likelihood or log-posterior with a GP, and (ii.) emulating an 
underlying forward model with a GP under an additive Gaussian error structure. 
We show that the EUP is generally a robust approximation to the EP in 
the latter case, but can suffer pathological behavior in the former.
\item We provide two variants of an approximate MCMC algorithm that directly 
target the EP, and numerically demonstrate favorable performance compared 
to the EUP approximation. In the case that the surrogate is a function of a GP, we 
show that our algorithm can be viewed as a correlated pseudo-marginal scheme,
which leverages a preconditioned Crank-Nicholson proposal.
\end{itemize}

\subsection{Outline.}
\Cref{sec:surrogates-intro} introduces the surrogate-based Bayesian workflow.
In \Cref{sec:compare-unc-prop} we derive the EP as a Bayes' estimator, and 
analyze the EUP as an EP approximation. Additional results are given for
two common modeling setups involving GP emulators.
\Cref{sec:computation} presents an approximate MCMC scheme targeting 
the EP, and describes connections with alternative inference algorithms. 
\Cref{sec:experiments} contains numerical experiments, and 
\Cref{sec:conclusion} concludes. Proofs are given in the appendix.

% Surrogates for Bayesian Inverse Problems
\section{Surrogates for Bayesian Inference} \label{sec:surrogates-intro}
We begin by introducing the Bayesian inference setting, including the challenges
associated with Bayesian inverse problems involving expensive forward models. 
We then describe the common two-stage surrogate modeling pipeline, and highlight 
several different strategies for integrating surrogates within a Bayesian analysis.

\subsection{Bayesian Inference Setting}
We consider the general goal of estimating parameters $\Par \in \parSpace \subseteq \R^{\dimPar}$ given 
observations $\obs \in \obsSpace \subseteq \R^{\dimObs}$ within a Bayesian framework.
A Bayesian model consists of a joint probability distribution $p(\Par, \obs)$, defined by 
specifying a prior density $\priorDens(\Par)$ and likelihood function $\lik(\Par; \obs)$.
The goal is then to summarize the posterior distribution 
\begin{align}
&\postDensNorm(\Par) \Def p(\Par \given \obs) = \frac{1}{\normCst} \priorDens(\Par) \lik(\Par; \obs), 
&&\normCst = \int_{\parSpace} \priorDens(\Par) \lik(\Par; \obs) d\Par. \label{eq:post_dens_generic}
\end{align}
While closed-form characterizations are typically thwarted by the intractable normalizing constant
$\normCst$, posterior samples can be simulated using MCMC algorithms, which only require 
access to pointwise evaluations of the unnormalized density 
$\postDens(\Par) \Def \priorDens(\Par) \lik(\Par; \obs)$.
However, such methods commonly require $10^5 - 10^7$ iterations, with each iteration 
involving a query to the density $\postDens(\Par)$.
In various engineering and scientific applications, computing $\lik(\Par; \obs)$ (and thus
$\postDens(\Par)$) requires running an expensive computer simulation. 
This renders MCMC infeasible in this setting, motivating the need for inference schemes
that use only a small set of evaluations of $\postDens(\Par)$.
 
\subsection{Bayesian Inverse Problems} \label{sec:bip}
The challenge posed by computationally expensive density evaluations $\postDens(\Par)$ commonly 
arises in the Bayesian approach to inverse problems \citep{Stuart_BIP}. In this setting, 
the likelihood often takes the form $\obs = \fwd(\Par) + \noise$ for some forward model
$\fwd: \parSpace \to \obsSpace$. For a concrete example, we consider the problem of estimating the 
parameters in a system of ordinary differential equations (ODEs)
\begin{align}
\frac{d}{d\Time} \state(\Time, \Par) &= \odeRHS(\state(\Time, \Par), \Par), &&x(\timeStart) = \stateIC, \label{ode_ivp}
\end{align}
where the dynamics depend on parameters $\Par$. Each value for $\Par$ implies a different solution trajectory
$[\state(\Time, \Par)]_{\timeStart \leq \Time \leq \timeEnd}$, which we encode by the
map $\solutionOp: \Par \mapsto [\state(\Time, \Par)]_{\timeStart \leq \Time \leq \timeEnd.}$. The goal is then 
to identify the parameters that yield trajectories in agreement with observed data 
$\obs$, which is assumed to be some noise-corrupted function $\obsOp$ of the true trajectory. Thus, the 
likelihood is of the form 
\begin{align}
&\obs = \fwd(\Par) + \noise, &&\fwd \Def \obsOp \circ \solutionOp. \label{eq:additive-noise}
\end{align}
In practice, the ODE is solved numerically so $\solutionOp$ represents the map induced by a numerical 
solver. Therefore, in this setting the computational cost of computing $\postDens(\Par)$ stems from the 
dependence of the likelihood on $\fwd(\Par)$, and in particular on the solver $\solutionOp(\Par)$.

\subsection{Surrogate Targets for Bayesian Inference} \label{sec:surrogates-Bayes}
Given the cost of computing $\postDens(\Par)$, we seek to approximate
the posterior using a small set of queries to the posterior density. 
Surrogates address this problem by approximating some underlying 
\textit{target map} $\Par \mapsto \target(\Par) \in \targetRange$, which in turn 
induces a cheap approximation of $\postDens(\Par)$.
Different target maps may be emulated in order to
accelerate Bayesian inference, so long as $\target(\Par)$ is sufficient 
to compute $\postDens(\Par)$ without requiring additional simulator runs.
Two common examples are detailed in \Cref{sec:fwd-em, sec:ldens-em}.
We write $\postDens(\Par; \target)$ and $\normCst(\target)$ to indicate the 
dependence of the posterior density and normalizing constant on $\target$, 
with the assumption that $\postDens(\Par; \target)$ is relatively 
quick to compute once the expensive computation $\target(\Par)$ is
obtained. In addition, we slightly abuse notation by using $\lik(\target(\Par); \obs)$ 
to denote the likelihood parameterized as a function of the target quantity. 

In many applications, computational constraints impose a budget of 
$\Ndesign$ evaluations of $\target$, where 
$\Ndesign$ is typically much smaller than that required by standard posterior
inference algorithms. The queries $\{(\Par_n, \target(\Par_n))\}_{n=1}^{\Ndesign}$
are then leveraged to fit a regression or interpolation model $\targetEm$,
such that $\targetEm(\Par)$ provides a prediction of $\target(\Par)$.
In order to quantify the uncertainty introduced via this approximation, we consider 
emulators that provide predictions in the form of a probability distribution; 
i.e., $\targetEm(\Par)$ is random and thus $\targetEm$ is a random function. 
Let $\emDist(\cdot \given \Par)$ and $\emDist$ denote their respective distributions,
and $\emE$ the expectation with respect to $\emDist$.
Using $\targetEm$ as a drop-in replacement for 
$\target$ implies that $\postDens(\Par; \targetEm)$ and $\normCst(\targetEm)$
are univariate random variables, while $\postDens(\cdot; \targetEm)$ 
is a random function.

The manner in which the uncertainty in $\targetEm$ propagates to 
$\postDens(\cdot; \targetEm)$ depends on the target $\target$,
the predictive distribution $\emDist$, and the particular form of the 
density $\postDens$. Below, we highlight two general choices 
of $\target$ common in the literature. This categorization has also 
been explored in \citet{StuartTeck1,StuartTeck2,GP_PDE_priors}. See
\citet{reviewPaper} for a discussion of practical considerations in choosing 
the emulator target. 

\subsubsection{Forward Model Emulation} \label{sec:fwd-em}
In the Bayesian inverse problem setting in \Cref{sec:bip},
a natural approach is to target the underlying forward model 
$\Par \mapsto \fwd(\Par)$ (i.e., choose $\target \Def \fwd$), a strategy 
we refer to as \textit{forward model emulation}.
This method consists of fitting a surrogate $\targetEm$ to the design 
$\{(\Par_n, \fwd(\Par_n))\}_{n=1}^{\Ndesign}$ and then using $\targetEm$
in place of $\fwd$. Much previous work has considered this strategy 
in the context of the additive noise model in \Cref{eq:additive-noise},
under the Gaussian assumption $\noise \sim \Gaussian(0, \likPar)$
\citep{StuartTeck1,GP_PDE_priors,hydrologicalModel,hydrologicalModel2,
Surer2023sequential,VillaniAdaptiveGP,weightedIVAR,idealizedGCM,CES}.
In this special case, the induced (unnormalized) posterior density surrogate takes the form
\begin{align}
\postEm(\Par) &= \priorDens(\Par) \Gaussian(\obs \given \targetEm(\Par), \likPar). \label{eq:post-em-fwd-Gaussian}
\end{align}

\subsubsection{Log-Density Emulation} \label{sec:ldens-em}
Other lines of research have instead targeted the log-likelihood $\Par \mapsto \log \lik(\target(\Par); \obs)$
\citep{VehtariParallelGP,FATES_CES,trainDynamics,quantileApprox,
ActiveLearningMCMC,FerEmulation,StuartTeck1,random_fwd_models,
GP_PDE_priors,OakleyllikEm,JosephMinEnergy,AlawiehIterativeGP}
or the (unnormalized) log-posterior 
$\Par \mapsto \log \left\{\priorDens(\Par)\lik(\target(\Par); \obs)\right\}$
\citep{emPostDens,Kandasamy_2017,llikRBF,gp_surrogates_random_exploration,
landslideCalibration}.

We collectively refer to these strategies as \textit{log-density emulation}. In the 
log-likelihood case, an emulator $\targetEm$ is fit to a design 
$\{(\Par_n, \log \lik(\target(\Par_n); \obs)\}_{n=1}^{\Ndesign}$
and induces a posterior density surrogate 
\begin{align}
\postEm(\Par) &= \priorDens(\Par) \Exp{\targetEm(\Par)}. \label{eq:post-em-llik}
\end{align}
The log-posterior case is quite similar, except that the effect of the prior is also 
approximated by the emulator, so the induced posterior surrogate simply takes 
the form $\postEm(\Par) = \Exp{\targetEm(\Par)}$.

% Comparing Uncertainty Propagation Methods
\section{The Expected Posterior} \label{sec:EP}
The second stage in the modular surrogate workflow consists of using the 
trained emulator $\targetEm$ to approximate the posterior $\postDensNorm$.
A simple approximation may be constructed by plugging in the surrogate mean
\begin{equation}
\postApproxNormMean(\Par) \Def \postDens(\Par; \emE[\targetEm]) / \normCst(\emE[\targetEm]),
\label{eq:mean-approx}
\end{equation}
but this ignores the emulator uncertainty, resulting in overconfident 
posterior inference. This poses the question of defining a posterior approximation 
that correctly propagates the uncertainty in $\targetEm$.
Given the lack of a unifying probabilistic model across the two inference stages, 
proper uncertainty quantification is not automatically given by standard Bayesian 
conditioning. Consequently, various uncertainty propagation methods have been 
proposed, each resulting in different posterior inferences 
\citep{BilionisBayesSurrogates,StuartTeck1,VehtariParallelGP,BurknerSurrogate,
FerEmulation}.
We identify and justify a mixture distribution termed the 
\textit{expected posterior (EP)} as the correct distribution to target in 
modular surrogate-based inference. 

\subsection{Decision Theoretic Derivation} \label{sec:decision-theoretic}
Irrespective of the underlying target $\target$, the probabilistic emulator $\targetEm$ 
induces a random approximation of the posterior defined by plugging $\targetEm$
in place of $\target$; this yields
\begin{align}
\postDensNorm(\Par; \targetEm) 
&\Def \frac{\postEm(\Par; \targetEm)}{\normCst(\targetEm)},
&&\normCst(\targetEm) \Def \int_{\parSpace} \postEm(\Par; \targetEm) d\Par, \label{eq:random-post}
\end{align}
which is referred to as the ``sample'' approximation in \citet{StuartTeck1}. We use the succinct
shorthand $\postEm(\cdot) \Def \postDens(\cdot; \targetEm)$, 
$\postNormEm(\cdot) \Def \postDensNorm(\cdot; \targetEm)$, and 
$\normCstEm \Def \normCst(\targetEm)$ when referencing the underlying 
emulator is not necessary.

We view the challenge of uncertainty propagation as that of constructing a deterministic 
probability distribution that summarizes the uncertainty encoded in $\postNormEm$.
To identify such a distribution, we adopt a Bayesian decision theoretic viewpoint and consider
the Bayes' estimator
\begin{equation}
\qDens_\star \in \argmin_{\qDens \in \qSpace} \emE[\loss(\postNormEm, q)],
\label{eq:variational-opt}
\end{equation}
for a loss function $\loss$ and space of densities $\qSpace$ over $\parSpace$. The following 
result provides the unique minimizer $\qDens_\star$ with respect to two common losses.

\begin{prop} \label{prop:EP-variational}
If the loss $\loss(\postNormEm, q)$ is chosen as the forward Kullback-Leibler (KL) divergence 
$\KL{\postNormEm}{q}$ or squared $L_2$ error $\norm{\postNormEm - q}_{L_2(\parSpace)}^2$
then the optimization problem in \Cref{eq:variational-opt} is solved uniquely by 
\begin{equation}
\qDens_\star(\Par) = 
\emE \left[\postNormEm(\Par) \right]
= \int \postDensNorm(\Par; \targetTraj) \emDist(d\targetTraj).
\label{eq:ep-approx}
\end{equation}
\end{prop}

We thus take $\qDens_\star$ as the baseline for surrogate-based uncertainty propagation. 
This distribution has been considered previously in various 
contexts, but is not widely used in the surrogate modeling literature
\citep{trainDynamics,BurknerSurrogate,garegnani2021NoisyMCMC}.
This is likely due in part to computational difficulties, which we address in \Cref{sec:computation}.
Following \citet{BurknerSurrogate}, we refer to $\qDens_\star$ as the \textit{expected posterior (EP)},
denoted by $\postApproxEP \Def \qDens_\star$.

\subsection{Hierarchical Formulation}
The EP arises as the marginal of the 
joint distribution $\emDist(d\targetTraj) \postDensNorm(\Par; \targetTraj) d\Par$, which 
can be understood via the hierarchical model 
\begin{align}
&\targetTraj  \sim \emDist, 
&&\Par \given \targetTraj \sim \postDensNorm(\cdot; \targetTraj).
\label{eq:ep-prob-model}
\end{align}
This perspective highlights the interpretation of the EP as a mixture of posteriors,
with each ensemble member $\postDensNorm(\Par; \targetTraj)$ induced by 
a particular emulator realization $\targetTraj$ that is weighted according to $\emDist$.
We assume throughout that $\targetEm$ is constructed such that 
trajectories of $\postNormEm$ are almost surely integrable, implying the sampling procedure
in \Cref{eq:ep-prob-model} is well-defined. See 
\citet{StuartTeck1,StuartTeck2,random_fwd_models,garegnani2021NoisyMCMC} for additional
technical conditions ensuring existence.

In the case that $\targetEm$ is a forward model emulator (\Cref{sec:fwd-em}), the EP 
represents the marginal posterior $\Par \given \obs$ under the hierarchical model 
\begin{align*}
&\targetTraj  \sim \emDist \\
&\Par \sim \priorDens \\
&\obs \given \targetTraj, \Par \sim \lik(\targetTraj(\Par); \obs).
\label{eq:ep-prob-model-fwd}
\end{align*}

\subsection{Cut Posterior} \label{sec:cut}
Our above formulation is agnostic to how the emulator is constructed. The 
randomness in $\targetEm$ may be derived from Bayesian methods 
(e.g., GPs, BART, Bayesian neural networks) or non-Bayesian alternatives
(e.g., deep ensembles, epistemic neural networks). In this section, we show 
that in the former case the EP can be viewed as a cut posterior distribution,
as defined in the modular Bayes literature 
\citep{PlummerCut,cutInference,moduleModels,cutVar,cutVar2}.

In particular, suppose that the surrogate model is defined by specifying 
a prior $\targetTraj \sim \emDist[0]$ and likelihood $\lik(\targetTraj; \emObs)$,
where $\emObs \Def \{(\Par_n, \target(\Par_n))\}_{n=1}^{\Ndesign}$ denotes
the emulator training data. The common example of a conjugate GP model 
corresponds to $\emDist[0] = \GP(\gpMean[0], \gpKer[0])$ and 
$\lik(\targetTraj; \emObs) = \Gaussian(\target(\Par_{1:\Ndesign}) \given \targetTraj(\Par_{1:\Ndesign}), \tau^2 I)$.
This setup encompasses standard parametric Bayesian models as well.
We can thus consider the joint Bayesian model 
\begin{equation}
\jointKOH(d\Par, d\targetTraj, d\obs, d\emObs) \Def
\priorDens(\Par) \lik(\targetTraj(\Par); \obs) \lik(\targetTraj; \emObs) 
\emDist[0](d\targetTraj) \, d\Par \, d\obs \, d\emObs
\label{eq:koh-joint}
\end{equation}
over all unknowns $(\targetTraj, \Par)$. This fully Bayesian (non-modular) model 
is akin to the framework proposed in the seminal work of \citet{KOH}. Subsequent 
studies showed that allowing the observational data $\obs$ to indirectly inform 
the emulator can produce counterintuitive results, contributing to adoption of 
the modular framework \citep{modularization}. The following result shows that 
the EP can be viewed as an optimal approximation to the fully Bayesian model, 
subject to the constraint that $\obs$ is not allowed to inform $\targetTraj$.

\begin{prop} \label{prop:kl-cut-op}
Let $\jointKOH^{\obs, \emObs}$ denote the distribution of $(\Par, \targetTraj)$
given $(\obs, \emObs)$ under the joint $\jointKOH$. Also, let $\emDist$ denote 
the distribution of $\targetTraj$ given $\emObs$ under the joint 
$\emDist[0](d\targetTraj) \lik(\targetTraj; \emObs) d\emObs$. Then, 
\begin{equation}
\emDist(d\targetTraj) \postDensNorm(\Par; \targetTraj)d\Par
= \argmin_{\qMeas \in \qSpace_{\mathrm{cut}}} \KL{\qMeas}{\jointKOH^{\obs, \emObs}},
\label{eq:kl-cut-opt}
\end{equation}
where 
\begin{equation}
\qSpace_{\mathrm{cut}} \Def 
\left\{\qMeas(d\Par, d\targetTraj) : \int \qMeas(d\Par, \cdot) = \emDist(\cdot) \right\}.
\end{equation}
\end{prop} 
The optimum in \Cref{eq:kl-cut-opt} is precisely the joint distribution noted in the previous 
section, and also corresponds to the cut posterior with respect to the fully Bayesian model.
This gives a second variational justification for the EP, complementing the result in 
\Cref{prop:EP-variational}.

\paragraph{Related work.} In the surrogate modeling literature the EP has been considered 
in \citet{trainDynamics,BurknerSurrogate,garegnani2021NoisyMCMC}. In 
\citet{StuartTeck2,VehtariParallelGP}, the EP is briefly noted but deemed computationally impractical. 
This difference of opinion can be explained by the fact that these latter two papers are focused 
on GP surrogates, which present additional challenges stemming from the inability to exactly 
sample surrogate trajectories $\targetTraj \sim \emDist$. On the other hand, 
\citet{BurknerSurrogate,garegnani2021NoisyMCMC} appear to implicitly assume the use of 
finite-dimensional surrogate models for which sampling trajectories is straightforward.
See \Cref{sec:computation} for additional computational details.

In the Bayesian modularization literature, there is a wide body of work 
on the cut posterior, which is equivalent to the EP 
when a fully Bayesian reference model is considered (\Cref{sec:cut})
\citep{PlummerCut,cutInference,moduleModels}.
Various papers have justified the cut posterior as a reverse KL divergence minimizer, 
akin to \Cref{prop:kl-cut-op} \citep{cutVar,cutVar2,moduleModels}. 

The hierarchical sampling view of the EP in \Cref{eq:ep-prob-model} also 
corresponds to a Bayesian multiple imputation algorithm, typically applied
in missing data problems \citep{multipleImputationMedical,missingData}.
The notion of aggregating multiple posterior distributions is also used in 
contexts other than modular inference, including for robustness to model 
misspecification \citep{BayesBag,BayesBag2}.

\section{Approximating the Expected Posterior} \label{sec:approximating-ep}
Having justified the EP as the baseline target distribution for uncertainty 
propagation, we now turn to the practical question of conducting EP-based
inference. While directly sampling the EP is typically infeasible, one natural 
approach is to consider approximate computation (see \Cref{sec:computation}).
An alternative route, which is the focus of this section, is to explicitly target a
different distribution that is more amenable to standard inference algorithms.
Citing the difficulties of EP-based inference, previous research has 
adopted this latter approach, 
eschewing the EP in favor of approximations of the \textit{unnormalized}
density surrogate $\postEm$ \citep{StuartTeck1,StuartTeck2,VehtariParallelGP}.
However, these works stop short of studying how such alternative approximations 
relate to the EP. 

In this section, we introduce the most popular of these approximations, the 
\textit{expected unnormalized posterior (EUP)}, and demonstrate 
how the EUP can be viewed as an approximation to the EP.
We analyze when this approximation is reasonable, and when the EUP may 
deviate significantly from the EP baseline. We highlight practical takeaways 
in popular special cases involving GP log-density and forward model emulators.

\subsection{The Expected Unnormalized Posterior} \label{sec:eup}
Notice that, unlike $\postNormEm(\Par)$, the unnormalized density surrogate
$\postEm(\Par)$ depends only on the single-point prediction $\targetEm(\Par)$
rather than the full emulator $\funcEm$. The EUP is defined by computing
a pointwise expectation of $\targetEm(\Par)$ and then normalizing post-hoc:
\begin{equation}
\postApproxMarg(\Par) \Def 
\frac{\emE \left[\postEm(\Par) \right]}{\int_{\parSpace} \emE\left[\postEm(\Par) \right] d\Par}
= \frac{\emE\left[\postEm(\Par) \right]}{\emE[\normCstEm]}. \label{eq:post-approx-EL} 
\end{equation}
The equality in \Cref{eq:post-approx-EL} follows from
changing the order of integration, courtesy of Tonelli's theorem \citep{StuartTeck1}. The EUP
is a marginal of the joint distribution 
$\emDist(d\targetTraj) \postDens(\Par; \targetTraj) / \emE[\normCst(\targetTraj)] d\Par$. 
When $\funcEm$ is a forward model emulator, this implies that the EUP is the 
marginal posterior $\Par \given \obs$ under the hierarchical model 
\begin{align}
\Par &\sim \priorDens \label{eq:el-prob-model} \\
\gamma \given \Par &\sim \emDist(\cdot \given \Par) \nonumber \\
\obs \given \gamma &\sim \lik(\gamma; \cdot), \nonumber
\end{align}
which can be interpreted as an inverse problem on an extended parameter space.

\subsection{Comparison of the EP and EUP}
Notice in \Cref{eq:post-approx-EL} that the EUP is a ratio estimator, and differs from 
the EP due to the nonlinearity of the normalization operation. The EUP can thus 
be derived from the EP by invoking two approximations: 
(i.) treating $\postEm(\Par)$ and $\normCstEm^{-1}$ as independent; and 
(ii.) assuming $\emE[\normCstEm^{-1}] \approx \emE[\normCstEm]^{-1}$.
The below result quantifies the effect of these two approximations.

\begin{prop} \label{prop:ep-eup-pw-err}
The pointwise difference between the EP and EUP is given by
\begin{equation}
\postApproxEP(\Par) - \postApproxMarg(\Par)
= \emE[\postEm(\Par)] \jgap + \Cov[\postEm(\Par), \normCstEm^{-1}],
\label{eq:ep-eup-pw-err}
\end{equation}
where $\jgap \Def \emE[\normCstEm^{-1}] - \emE[\normCstEm]^{-1}$
is the ``Jensen gap.''
\end{prop}

By Jensen's inequality $\jgap \geq 0$, implying that the Jensen gap represents a 
$\Par$-independent positive bias in the difference between EP and EUP, modulated by the magnitude of 
$\emE[\postEm(\Par)]$. Since both distributions integrate to one, any positive 
biases must be balanced by negative biases at other values of $\Par$.
The second term in \Cref{eq:ep-eup-pw-err} will be negative for ``influential'' $\Par$ values, 
meaning that larger realizations of $\postEm(\Par)$ correspond to larger values of 
$\normCstEm$. The influence of a parameter value will typically increase when 
$\postEm(\Par)$ is large on average, highly variable, and is positively correlated other 
$\postEm(\Par^\prime)$. The latter property may be satisfied, for example, when using 
a GP emulator with a long lengthscale. Based on this logic, we expect the EUP to inflate 
influential regions and depress non-influential regions, relative to the EP. This can lead 
the EUP to be more peaked, while the EP is more smooth. The pointwise error in 
\Cref{prop:ep-eup-pw-err} can be integrated to obtain the following $L_1$ bound 
between $\postApproxEP$ and $\postApproxMarg$. Multiplying both sides by $1/2$
yields a total variation bound.

\begin{prop} \label{prop:ep-eup-TV-err}
Let $\jgap$ be defined as in \Cref{prop:ep-eup-pw-err}. Then,
\begin{equation}
\norm{\postApproxEP - \postApproxMarg}_{L_1(\parSpace)}
\leq \emE[\normCstEm] \jgap + \int \Cov[\postEm(\Par), \normCstEm^{-1}] d\Par
\label{eq:ep-eup-TV-err}
\end{equation}
\end{prop}

Decreasing the variance of $\normCstEm$ will shrink both terms in 
\Cref{eq:ep-eup-TV-err}. This may occur when there is little 
uncertainty in $\targetEm$, the unnormalized posterior $\postDens(\cdot; \targetTraj)$
is insensitive to $\targetTraj$, or $\normCstEm$ is insensitive to the variability 
in $\postEm$. In the first case, note that if $\funcEm$ is heavily 
concentrated around its mean, then both the EP and EUP will bear a close resemblance
to the plug-in mean approximation in \Cref{eq:mean-approx}.
In special cases, the two terms in \Cref{eq:ep-eup-TV-err} perfectly balance so that 
the EP and EUP agree. For example, this occurs when 
$\postEm(\Par) = \omega g(\Par)$, where $\omega$ is a random constant and 
$g(\Par)$ a deterministic function. 

\subsubsection{Gaussian Log-Density Emulation Setting} \label{sec:ldens-Gaussian}
We now consider the particular example from \Cref{sec:ldens-em},
where $\postEm(\Par) = \Exp{\targetEm(\Par)}$
and $\targetEm \sim \GP(\gpMean, \gpKer)$ is a GP emulator; that is, 
$\E[\targetEm(\Par)] = \gpMean(\Par)$ and 
$\Cov[\targetEm(\Par),\targetEm(\Par^\prime)] = \gpKer(\Par, \Par^\prime)$ for all 
$\Par, \Par^\prime \in \parSpace$. We also make use of the shorthand 
$\gpKer(\Par) \Def \gpKer(\Par, \Par)$. This setup encompasses both the log-likelihood
and log-posterior emulation settings via suitable definitions of $\gpMean$ and $\gpKer$.
GP log-density emulators have been used in various studies 
\citep{VehtariParallelGP,FATES_CES,trainDynamics,quantileApprox,ActiveLearningMCMC,FerEmulation,
StuartTeck1,random_fwd_models,GP_PDE_priors,OakleyllikEm,JosephMinEnergy,AlawiehIterativeGP}.

In this setting, the pointwise distribution of the induced posterior surrogate is log-normal
$\postEm(\Par) \sim \LN(\gpMean(\Par), \gpKer(\Par))$, implying that
\begin{equation}
\postApproxNormMarg(\Par) 
\propto \Exp{\gpMean(\Par) + \frac{1}{2}\gpKer(\Par)}
= \postApproxMean(\Par) \Exp{\frac{1}{2}\gpKer(\Par)}.
\label{eq:EUP-Gaussian-ldens}
\end{equation} 
The expression in \Cref{eq:EUP-Gaussian-ldens} shows that the EUP inflates the plug-in mean 
density in uncertain regions \citep{StuartTeck2,GP_PDE_priors}. In particular, 
 \begin{equation}
 \frac{\postApproxNormMarg(\Par^\prime)/\postApproxNormMarg(\Par)}{\postApproxNormMean(\Par^\prime)/\postApproxNormMean(\Par)}
 = \Exp{\frac{1}{2}\left[\gpKer(\Par^\prime) - \gpKer(\Par) \right]},
 \label{eq:unc-infl-factor}
 \end{equation}
showing that the degree of inflation scales with the emulator variance; that is, if 
$\postApproxNormMean(\Par)=\postApproxNormMean(\Par^\prime)$, then the EUP will give 
always give higher weight to the location with higher emulator uncertainty.

Unfortunately, the exponential scaling in \Cref{eq:EUP-Gaussian-ldens} can result in
undesirable degrees of uncertainty inflation. For example, suppose that 
the standard deviation of $\targetEm(\Par^\prime)$ is twice that of $\targetEm(\Par)$, so that 
the expression in \Cref{eq:unc-infl-factor} reduces to $\Exp{\frac{3}{2}\gpKer(\Par)}$.
If $\gpKer(\Par) = 4$, this means
$\Exp{\frac{3}{2}\gpKer(\Par)} \approx 400$, vastly magnifying the modest difference 
in emulator uncertainty. Practically, this can result in $\postApproxNormMarg$ concentrating 
on small subsets of $\parSpace$ where emulator uncertainty is the largest.
With respect to \Cref{eq:ep-eup-pw-err}, this manifests as a large negative covariance 
between $\postEm(\Par)$ and $\normCstEm$ in uncertain regions, implying 
$\postApproxNormMarg(\Par) \gg \postApproxEP(\Par)$. 

Given this extreme sensitivity to emulator uncertainty, we recommend against the use of the EUP
approximation in this setting. \citet{VehtariParallelGP} reach a similar conclusion, and propose
the use of an alternative approximation defined via a pointwise quantile of $\postEm(\Par)$.
This quantile approximation is also considered in \citet{quantileApprox,FATES_CES}.
While this approximation is more robust, we highlight that the true underlying issue is often 
a poorly calibrated emulator. For example, many common likelihoods (e.g., Gaussian) 
have known upper bounds, which are ignored by the GP emulator $\targetEm$. The simple 
adjustment of enforcing this bound constraint within $\targetEm$ can mitigate the aforementioned issues.
\citet{quantileApprox} employs such a bound-constrained GP log-density emulator.

\subsubsection{Gaussian Forward Model Emulation Setting} \label{sec:fwd-Gaussian} 
We next return to the setting in \Cref{sec:fwd-em}, where
\begin{align}
&\obs = \fwd(\Par) + \noise, &&\noise \sim \Gaussian(0, \likPar)
\end{align}
with prior $\Par \sim \priorDens$ independent of $\noise$. We consider the case 
where $\fwd$ is replaced with a (potentially multi-output) GP emulator 
$\targetEm \sim \GP(\gpMean, \gpKer)$. This setup has been considered frequently in various 
applications \citep{Surer2023sequential,weightedIVAR,StuartTeck2,GP_PDE_priors,CES,
idealizedGCM,villani2024posteriorsamplingadaptivegaussian,hydrologicalModel,hydrologicalModel2}.

The unnormalized posterior surrogate takes the form 
$\postEm(\Par) = \priorDens(\Par)\Gaussian(\obs \mid \targetEm(\Par), \likPar)$, implying
\begin{equation}
\postApproxNormMarg(\Par) \propto \priorDens(\Par) \Gaussian(\obs \mid \gpMean(\Par), \likPar + \gpKer(\Par)),
\label{eq:EUP-Gaussian-fwd}
\end{equation}
following from the formula for the convolution of two Gaussians [\todo: add to appendix].
A key observation in this setting is that $\postEm(\Par) \in [0, B_{\likPar}]$, where 
$B_{\likPar} \Def \sup_{u \in \parSpace} \priorDens(\Par) / \det(2\pi\likPar)^{1/2}$.
[\todo: need to finish this thought. The conjecture here is that the EUP in this setting is 
more robust in this setting due to the bound.

Unlike in the Gaussian log-density case, the EUP does not strictly inflate the plug-in 
mean approximation in regions of larger uncertainty. This is due to the fact that 
the Gaussian likelihood is bounded above. When $\gpMean(\Par)$ and $\obs$
are close, then an increase in $\gpKer(\Par)$ may actually deflate the density 
at $\Par$. The same increase in $\gpKer(\Par)$ will inflate the density at values 
of $\Par$ where $\gpMean(\Par)$ and $\obs$ are far apart.
 
\paragraph{Related work.} The EUP is proposed in the forward model emulator setting 
in \citet{BilionisBayesSurrogates}, motivated by the extended parameter space viewpoint
in \Cref{eq:el-prob-model}. \citet{StuartTeck2,CES} also note this perspective
in the particular Gaussian setting of \Cref{eq:post-em-fwd-Gaussian}.
In \citet{SinsbeckNowak}, the EUP is justified as the distribution 
$q$ that minimizes $\emE\left[\norm{\postEm - q}_{L_2(\parSpace)}^2 \right]$.
\citet{StuartTeck1,StuartTeck2,VehtariParallelGP} also highlight this Bayesian 
decision theoretic justification. 
In contrast with the EP, the optimality is only guaranteed for the estimate of the 
\textit{unnormalized} posterior.
As shown in \Cref{prop:EP-variational}, the EP is the minimizer when the complete 
normalized distributions are considered.

The EUP is referred to as the ``marginal'' approximation in 
\citet{StuartTeck1,StuartTeck2,random_fwd_models,TeckHyperpar},
in which it is analyzed theoretically for both log-density and forward 
model emulators. \citet{VehtariParallelGP} highlight pathological 
behavior of the EUP for GP log-density emulators, and recommend 
against its use in this setting. To our knowledge, \citet{BurknerSurrogate}
is the only previous work to directly compare the EP and EUP (which 
they call the ``expected likelihood''). Their comparison is limited to 
numerical results in case studies involving forward model emulators.
The closed-form EUP expressions in 
\Cref{eq:EUP-Gaussian-ldens,eq:EUP-Gaussian-fwd} have been 
noted in a variety of works 
\citep{StuartTeck1,StuartTeck2,VehtariParallelGP,weightedIVAR,
GP_PDE_priors,Surer2023sequential,Takhtaganov2018AdaptiveBayesianGP}. 
The EUP has seen wide 
use in various applications involving both forward model 
and log-density emulators 
\citep{weightedIVAR,GP_PDE_priors,CES,idealizedGCM,
villani2024posteriorsamplingadaptivegaussian,hydrologicalModel,hydrologicalModel2}.
 
% Approximate Computation for the EP
\section{Approximate Computation for the Expected Posterior} \label{sec:computation}
The adoption of the EP in the surrogate modeling setting has been 
inhibited by computational challenges, as noted in 
\citet{VehtariParallelGP,StuartTeck2}. We first clarify the limitations
of existing inference methods, and then introduce an efficient MCMC 
approximation that alleviates these difficulties.

\subsection{Sampling Trajectories}
In light of the hierarchical model in \Cref{eq:ep-prob-model}, the following 
algorithm can in principle be applied to directly sample $\postApproxEP$.  

\begin{algorithm}[H]
    \caption{Direct sampling from $\postApproxEP$}
    \label{alg:ep}
    \begin{algorithmic}[1]
    \Function{sampleEP}{$\llikEmRdm[\Ndesign]{\postDensNorm}, \NSample, M$}     
        \For{$\sampleIndex \gets 1, \dots, \NSample$} 
        		\State $\targetTraj^{(\sampleIndex)} \sim \emDist$ \Comment{Sample emulator trajectory}
		\State $\Par^{(\sampleIndex, 1)}, \dots, \Par^{(\sampleIndex, M)} \overset{iid}{\sim} \postDensNorm(\cdot; \targetTraj^{(\sampleIndex)})$ \Comment{Sample posterior given trajectory}
	\EndFor
	\State \Return $\{\Par^{(\sampleIndex, m)}\}_{1 \leq \sampleIndex \leq \NSample, \ 1 \leq m \leq M}$
	\EndFunction
    \end{algorithmic}
\end{algorithm}
If one sample is drawn from each posterior trajectory (i.e., $M=1$) then the resulting samples are 
independent. Otherwise, \Cref{alg:ep} produces dependent samples identically distributed according to
$\postApproxEP$. In practice, directly sampling $\Par \given \targetTraj \sim \postDensNorm(\cdot; \targetTraj)$
is rarely possible, so this inner sampling step is replaced by an MCMC algorithm.
The resulting sampling scheme is sometimes called \textit{Metropolis within Monte Carlo (MwMC)} 
\citep{garegnani2021NoisyMCMC}. MwMC has the downside of requiring $\NSample \gg 1$
MCMC runs, but this may be less of an issue in modern parallel computing environments \citep{BurknerSurrogate}. 
Moreover, methods have been developed to reduce the size of $\NSample$ required to achieve a
desired level of precision \citep{BurknerTwoStep}. 

The more significant issue is the outer sampling step, which requires simulating surrogate 
trajectories $\targetEm \sim \emDist$. While not a problem for finite-dimensional 
surrogate models (e.g., linear models), this presents major challenges for surrogates 
derived from GPs. Given the popularity GP surrogates, 
this computational bottleneck must be resolved for the EP to be broadly accessible in 
surrogate-based Bayesian workflows. Standard remedies suffer from poor scalability,
limited applicability, or bespoke numerical implementations. For example, naive 
approximations that discretize $\parSpace$ are limited to low-dimensional settings.
Finite-rank GP approximations offer an alternative, but are dependent on the 
particular form of the surrogate \citep{pathwiseConditioning}. In theory, one 
could retain an infinite-dimensional GP representation by constructing 
GP trajectories ``just-in-time'' within the MCMC algorithm; that is, iteratively
condition the GP at each value of $\Par$ considered within the MCMC run.
However, this approach is well-known to suffer from significant numerical 
instability \citep{pathwiseConditioning}. Finally, we highlight the method of 
\citet{trainDynamics}, which is to our knowledge the only existing work to 
attempt EP-based inference with a GP surrogate. 
Their method consists of approximating GP 
trajectories by sampling the surrogate only at a finite grid of points, 
and then approximating the trajectory as the GP mean, conditional on the 
sampled values at these points. Our MCMC method avoids the challenging 
task of choosing an appropriate conditioning set.

\subsection{Approximate MCMC}
In this section, we present two variants of an approximate MCMC algorithm that 
overcome the limitations of alternative methods discussed above. The algorithms seek 
to target the joint distribution 
$\emDist(d\targetTraj) \postDens(\Par; \targetTraj)/\normCst(\targetTraj) d\Par$, which admits
$\postApproxEP$ as a marginal. First, consider a Metropolis-within-Gibbs (MwG)
scheme that alternates between $\Par$ and $\targetTraj$ updates. 
For a fixed $\targetTraj$, the $\Par$ update must leave the distribution 
$\postDensNorm(\Par; \targetTraj) d\Par$ invariant, which can be achieved 
with a standard Metropolis-Hastings (MH) step. For a fixed $\Par$, the $\targetTraj$
update must leave the distribution 
$\emDist(d\targetTraj) \postDens(\Par; \targetTraj)/\normCst(\targetTraj)$
invariant. This step faces two issues: (i.) the fact that $\targetTraj$ is potentially 
infinite-dimensional, and (ii.) the fact that we cannot compute 
$\normCst(\targetTraj)$. The former can be addressed by so-called 
function-space MCMC methods \citep{functionSpaceMCMC}. If we utilize an
MH step with a $\emDist$-reversible proposal distribution 
$\propDist(\targetTraj, \cdot)$, then the MH acceptance probability reduces to
\begin{equation}
\accProbMH(\targetTraj, \tilde{\targetTraj})
= \min\left(1, \frac{\postDens(\Par; \tilde{\targetTraj})}{\postDens(\Par; \targetTraj)} \cdot 
\frac{\normCst(\targetTraj)}{\normCst(\tilde{\targetTraj})} \right),
\label{eq:MH-ratio-mwg}
\end{equation}
where $\tilde{\targetTraj} \sim \propDist(\targetTraj, \cdot)$ is the proposed value.
If $\funcEm \sim \GP(\gpMean, \gpKer)$
\footnote{More generally, this applies when $\postEm$ arises as a transformation
of an underlying Gaussian. The emulator predictive distribution need not be Gaussian.}, 
then we can take $\propDist$ as
the preconditioned Crank-Nicholson (pCN; \citet{functionSpaceMCMC}) proposal
\begin{equation}
\tilde{\targetTraj} \Def \gpMean + \pcnCor (\targetTraj  - \gpMean) + \sqrt{1 - \pcnCor^2} \xi, 
\qquad \xi \sim \GP(0, \gpKer).
\end{equation}
To address the issue regarding the intractable normalizing constant ratio in 
\Cref{eq:MH-ratio-mwg}, we might try to apply techniques from the doubly 
intractable MCMC literature \citep{doublyIntractableReview}. However, exact 
MCMC schemes such as the exchange algorithm \citep{exchangeAlg} require 
directly sampling $\postDensNorm(\cdot; \tilde{\targetTraj})$,
which is infeasible in this context. To avoid the challenging task of estimating the 
normalizing constant ratio, we instead invoke the simple approximation 
$\normCst(\targetTraj)/\normCst(\tilde{\targetTraj}) \approx 1$, which is reasonable when 
the proposal $\tilde{\targetTraj}$ does not differ significantly from the current
state $\targetTraj$. We can thus control the error in the approximation by slowing
down the mixing speed of the $\targetTraj$-chain.

\begin{algorithm}[H]
    \caption{Metropolis-within-Gibbs Approximation to $\postApproxEP$}
    \label{alg:mwg-ep}
    \begin{algorithmic}[1] 
    \State \textbf{Input:} Current state $(\Par, \targetTraj_\Par)$
    \State \textbf{Output:} Updated state $(\Par^\prime, \targetTraj^\prime_{\Par^\prime})$
     \State Propose $\tilde{\Par} \sim \propDist(\Par, \cdot)$
     \State Propose $(\tilde{\targetTraj}_\Par, \tilde{\targetTraj}_{\propPar}) \sim \emDist(\cdot, \cdot \given \Par, \propPar)$ \Comment{Begin $\targetTraj$ update}
     \State $\alpha_{\targetTraj} \gets \min\left\{1, \postDens(\tilde{\targetTraj}_\Par) / \postDens(\targetTraj_\Par) \right\}$
      	\If{$\mathrm{Unif}(0,1) < \alpha_{\targetTraj}$}
                \State $(\targetTraj^\prime_\Par, \targetTraj^\prime_{\propPar}) \gets (\tilde{\targetTraj}_\Par, \tilde{\targetTraj}_{\propPar})$ 
            \Else
                \State $\targetTraj^\prime_{\Par} \gets \targetTraj_{\Par}$
                \State $\targetTraj^\prime_{\propPar} \gets \emDist(\cdot \given \targetEm(\Par) = \targetTraj_\Par)$
            \EndIf
      \State $\alpha_{\Par} \gets \min\left\{1, \priorDens(\propPar )\lik(\targetTraj^\prime_{\propPar}; \obs) / \priorDens(\Par) \lik(\targetTraj^\prime_{\Par}; \obs) \right\}$ \Comment{Begin $\Par$ update}
             \If{$\mathrm{Unif}(0,1) < \alpha_{\Par}$}
                \State $\Par^\prime \gets \propPar$
                \State $\targetTraj^\prime_{\Par^\prime} \gets \targetTraj^\prime_{\propPar}$ 
            \Else
            	\State $\Par^\prime \gets \Par$
		\State $\targetTraj^\prime_{\Par^\prime} \gets \targetTraj^\prime_{\Par}$
            \EndIf
    \end{algorithmic}
\end{algorithm}

\section{Numerical Experiments} \label{sec:experiments}
\section{Conclusion} \label{sec:conclusion}
















Despite the widespread use of surrogates in various application domains, 
surrogate uncertainty is often ignored or propagated in an ad hoc manner.
This can be explained in part due to poor conceptual understanding 
of the consequences of different uncertainty propagation methods
(\Cref{sec:compare-unc-prop}), but also to a lack of general-purpose software
for computation. Previous work has often relied on specific assumptions that
enable closed-form computations, with algorithms geared towards
particular emulators or likelihoods [\todo: cite]. Recently, \citep{BurknerSurrogate}
proposed a generic MCMC pipeline rooted in standard probabilistic programming
workflows. While a step in the right direction, a potential drawback of their approach
is that their MCMC methods only approximately target the desired distributions 
due to their use of fixed Monte Carlo estimates (i.e., sample average approximations).
Another limitation is that their algorithms do not directly accommodate 
infinite-dimensional surrogates such as GPs, owing to the requirement of 
sampling trajectories of $\funcEm$.

We aim to address these challenges, focusing primarily on MCMC schemes for the EL
and EP approximations that are applicable to generic probabilistic surrogate models 
and likelihood functions. While general-purpose computation for the EL approximation 
has been studied \citep{garegnani2021NoisyMCMC}, existing methods for EP inference
are largely limited to the direct sampling scheme in \Cref{alg:ep}. As noted in 
\citep{VehtariParallelGP,StuartTeck2}, this approach is impractical for GP emulators.
We present a new MCMC method that is applicable to the GP setting, and is
shown to provide a close approximation of the EP distribution in various experiments.

Prior to presenting this algorithm, we provide a thorough review of MCMC methods
targeting the EL and EP distributions, or approximations to these distributions. 
We emphasize connections between pseudo-marginal MCMC \citep{pseudoMarginalMCMC}, 
``noisy'' MCMC \citep{noisyMCMC}, and inference schemes for cut 
posteriors \citep{PlummerCut}. We also highlight an ``approximate computation'' 
perspective to the uncertainty propagation question. Typically, inference proceeds
by first defining a target distribution and then designing an MCMC scheme that
converges to this distribution. Alternatively, one might skip the first step and instead 
alter a standard MCMC scheme by strategically injecting noise within the 
algorithm to account for the surrogate uncertainty. Assuming the altered algorithm 
still converges to a well-defined posterior distribution, then this approach yields 
a strategy for implicitly defining an uncertainty-aware posterior approximation.
In the spirit of this approach, we consider several intuitive alterations of Metropolis-Hastings
algorithms to account for emulator uncertainty. In some cases, the resulting algorithms
coincide with existing methods. While intuitively appealing, we show that care must 
be taken when adopting this approach, as certain design choices can arbitrarily change the 
degree to which uncertainty is propagated.

\begin{algorithm}
    \caption{Direct sampling from $\postApproxEP$}
    \label{alg:ep}
    \begin{algorithmic}[1]
    \Function{sampleEP}{$\llikEmRdm[\Ndesign]{\postDensNorm}, \NSample, M$}     
        \For{$\sampleIndex \gets 1, \dots, \NSample$} 
        		\State $\targetTraj^{(\sampleIndex)} \sim \emDist$ \Comment{Sample emulator trajectory}
		\State $\Par^{(\sampleIndex, 1)}, \dots, \Par^{(\sampleIndex, M)} \overset{iid}{\sim} \postDensNorm(\cdot; \targetTraj^{(\sampleIndex)})$ \Comment{Sample posterior given trajectory}
	\EndFor
	\State \Return $\{\Par^{(\sampleIndex, m)}\}_{1 \leq \sampleIndex \leq \NSample, \ 1 \leq m \leq M}$
	\EndFunction
    \end{algorithmic}
\end{algorithm}

\paragraph{Notation.} Throughout this section, we define MCMC algorithms
based on the steps performed for a single iteration of computation. For any 
state variable $\theta$, we write $\theta$ and $\theta^\prime$ for the current
and updated state, respectively.

\paragraph{TODO: some themes to stress throughout this section:}
\begin{enumerate}
\item Approximate computation approaches (no target density defined beforehand); appealing but urge caution
\item How computation differs between log-density and forward model emulator approaches.
\item Whether algorithms are applicable to the infinite-dimensional emulator case (e.g., GPs)
\item Emphasizing the difference between the typical application of pseudo-marginal/noisy (trying to target underlying true
distribution) vs. our use of these algorithms here (trying to propagate uncertainty). While there is a clear notion of what is
"correct" in the former case, this is less clear in the latter. For example, in MCwM increasing $M$ leads to a 
more correct algorithm in the sense of tending towards the pseudo-marginal target. However, in the uncertainty
propagation setting one could argue that lower values of $M$ result in propagating uncertainty in a more 
reasonable way.
\end{enumerate}

\subsection{Expected Likelihood}
We start by summarizing algorithms proposed for EL-based inference. Recall the target distribution in this 
case is $\postApproxNormMarg(\Par) \propto \E_{\Ndesign}[\postEm(\Par)]$. If the expectation in this 
expression is available in closed form, then off-the-shelf samplers can be applied. This is the case in the 
Gaussian settings considered in \Cref{sec:ldens-Gaussian,sec:fwd-Gaussian}. However, alternative 
algorithms are required to generalize to more complicated emulators and likelihoods. 
As a starting point for general-purpose algorithms, we might consider the fact that the extended model 
in \Cref{eq:el-prob-model} admits $\postApproxNormMarg$ as a marginal posterior. We can thus 
consider an MCMC scheme targeting the joint density
\begin{equation}
p(\Par, \gamma \given \obs) \propto \priorDens(\Par) \lik(\gamma; \obs) \emDist(\gamma \given \Par). 
\end{equation}
If we consider a joint Metropolis-Hastings update for $(\Par, \gamma)$ using a proposal of the form
\begin{equation}
\propDens(\tilde{\Par}, \tilde{\gamma} \given \Par, \gamma)
\Def \propDens_{\Par}(\tilde{\Par} \given \Par) \emDist(\tilde{\gamma} \given \tilde{\Par}),
\end{equation}
then the standard Metropolis-Hastings acceptance probability reduces to
\begin{equation}
\accProbMH(\tilde{\Par}, \tilde{\gamma} \given \Par, \gamma)
= \min\left\{1, \frac{\priorDens(\propPar) \lik(\tilde{\gamma}; \obs)\propDens_{\Par}(\Par \given \tilde{\Par})}{\priorDens(\Par)\lik(\gamma; \obs)\propDens_{\Par}(\tilde{\Par} \given \Par)} \right\}
\end{equation}
This auxiliary variable Metropolis-Hastings scheme is precisely a pseudo-marginal algorithm 
targeting $\postApproxNormMarg$.
We could have alternatively arrived at this method by noting that $\postDens(\Par; \gamma)$ is an unbiased 
estimator of $\postEm(\Par)$ when $\gamma \sim \emDist(\cdot \given \Par)$. The pseudo-marginal method
can be viewed as a noisy alteration of the standard Metropolis-Hastings algorithm, such that exact evaluations 
of $\postDens(\Par)$ are replaced with realizations of $\postEm(\Par)$. The key to ensuring 
$\postApproxNormMarg$-invariance is that 
only a new sample of $\postEm(\tilde{\Par})$ is drawn at each iteration; 
the sampled value of $\postEm(\Par)$ is recycled from the previous iteration. The algorithm can be made more 
efficient (while still targeting $\postApproxNormMarg$), by averaging multiple sampled values. In particular, in place
of $\postDens(\propPar)$, we use 
$\hat{\postDens}^M(\propPar) \Def M^{-1} \sum_{m=1}^{M} \postDens(\propPar; \targetTraj^{(m)})$ where 
$\targetTraj^{(m)} \overset{\mathrm{iid}}{\sim} \emDist(\cdot; \tilde{\Par})$.

The pseudo-marginal approach to EL-based inference is noted in \citep{StuartTeck1} and studied 
in depth in \citep{garegnani2021NoisyMCMC}. The latter also studies a closely-related, but approximate, algorithm
whereby the estimators $\hat{\postDens}^M(\Par)$ and $\hat{\postDens}^M(\propPar)$ are both sampled independently
each iteration, rather than recycling the value of $\hat{\postDens}^M(\Par)$ from the previous iteration. This approach, 
typically known as \textit{Monte Carlo within Metropolis (MCwM)}, does not directly target $\postApproxNormMarg$
but is often more efficient \citep{noisyMCMC,stabilityNoisyMH,noisyMCSurvey}.
The limiting distribution of MCwM (if it exists) depends on the value of $M$. As $M \to \infty$, the limiting distribution 
will tend towards $\postApproxNormMarg$. \citet{FerEmulation} utilize a MCwM scheme with $M = 1$, adopting an
approximate computation viewpoint rather than explicitly trying to target $\postApproxNormMarg$.
\citet{BurknerSurrogate} propose an alternative approximation defined by sampling 
$\targetTraj^{(m)} \overset{\mathrm{iid}}{\sim} \emDist$ and then targeting the fixed density approximation
\begin{align}
\hat{\postDens}^M(\Par \given \{\targetTraj^{(m)}\}_{m=1}^{M}) \Def 
\frac{1}{M} \sum_{m=1}^{M} \postDens(\Par; \targetTraj^{(m)}).
\end{align}
In contract to the pseudo-marginal and MCwM algorithms, the Monte Carlo samples $\{\targetTraj^{(m)}\}_{m=1}^{M}$
are fixed across all iterations, akin to a sample average approximation from the optimization literature
\citep{SAA}. In this method, the target distribution also depends on $M$, and approaches $\postApproxNormMarg$
as $M \to \infty$. The benefit of this approach is that it defines a deterministic density approximation which can be input
into standard software (e.g., Stan). The downside is that it requires sampling trajectories $\targetTraj^{(m)}$, which are 
then evaluated at many different values of $\Par$ throughout the MCMC run. This is not a problem when the randomness
in $\funcEm$ derives from a finite-dimensional random vector. However, it presents challenges for infinite-dimensional models
such as GPs. In principle, such an approach could be implemented by repeatedly conditioning the GPs at new evaluation points,
thus iteratively building the GP trajectories ``just-in-time''. However, such an approach is fraught with numerical instability issues,
which has been well-documented in the Bayesian optimization literature \citep{pathwiseConditioning}. 
The pseudo-marginal and MCwM algorithms only require the ability to sample from $\emDist(\cdot \given \Par)$, 
rather than $\emDist$.

\subsection{Expected Posterior}
We next consider existing work on sampling methods for the EP approximation. In general, the
EP distribution is more challenging and computationally expensive to approximate than the EL. It also presents
additional challenges for GP emulators. Recall the target distribution in this case is 
$\postApproxEP(\Par) = \E_{\Ndesign}\left[\postEm(\Par)/\normCstEm \right]$.

\subsubsection{Direct Sampling and Extensions}
In principle, the EP distribution can be directly sampled via \Cref{alg:ep}.
One iteration of this algorithm is completed by first sampling 
a surrogate trajectory $\targetTraj \sim \emDist$, and then 
drawing samples 
$\Par^{(\sampleIndex, 1)}, \dots, \Par^{(\sampleIndex, M)} \overset{iid}{\sim} \postDensNorm(\cdot; \targetTraj^{(\sampleIndex)})$,
where $\postDensNorm(\cdot; \targetTraj^{(\sampleIndex)})$ is the induced trajectory of $\postNormEm$.
If $M = 1$, then the resulting samples of $\Par$ are independent; otherwise, they are dependent.
Both of these sampling steps may be infeasible in practice. In almost all cases, direct sampling from 
$\postDensNorm(\cdot; \targetTraj^{(\sampleIndex)})$ will not be possible. The samples $\Par^{(\sampleIndex, m)}$ are thus
typically generated via MCMC. This approach, sometimes called \textit{Metropolis within Monte Carlo (MwMC)} 
(not to be confused with MCwM), is considered in \citep{garegnani2021NoisyMCMC,BurknerSurrogate}.
If the distribution of $\postEm$ is complex, then a large value of $M$ may be required, implying 
$M$ runs of an MCMC algorithm. While this may not be problematic in modern parallel computing 
environments, it may still be desirable to consider adjustments to lower the required number of runs. 
Such considerations 
have been explored in the multiple imputation literature; e.g., assuming the EP is approximately Gaussian [\todo: cite].
\citep{BurknerTwoStep} present an alternative approach relying on Pareto-smoothed importance sampling. 
 
While these methods focus primarily on reducing the cost of the MCMC sampling step, they implicitly assume
that sampling from $\emDist$ is straightforward. This is not the case for GP surrogates, which 
has resulted in a lack of EP-based inference in the GP emulation literature \citep{VehtariParallelGP,StuartTeck2}. 
To our knowledge, the only exception is \citet{trainDynamics}.
Their method consists of approximating GP 
trajectories by sampling the surrogate only at a finite grid of points, and then approximating 
the trajectory as the GP mean, conditional on the sampled values at these points. This approach represents
a middle ground between discretizing the GP and applying just-in-time sampling. The authors present a method
for selecting the grid points, though this is likely to present challenges in higher dimensions. 

\subsubsection{Approximate MCMC Methods}
Instead of running $M$ MCMC algorithms, it is natural to wonder if a single MCMC scheme can be designed
to (approximately) target $\postApproxEP$. This question has not been investigated in the surrogate 
literature, but has been addressed in the case where $\postApproxEP$ corresponds to the ``cut posterior''
of a modular Bayesian model. \citet{PlummerCut} describes the implementation of such an algorithm 
in the WinBUGS software [\todo: cite], referred to as the \textit{naive cut algorithm}.
A single step of the algorithm proceeds as follows:
\begin{enumerate}
\item Apply a $\emDist$-invariant Markov update $\targetTraj \to \targetTraj^\prime$.
\item Apply a $\postDens(\cdot; \targetTraj^\prime)$-invariant Markov update $\Par \to \Par^\prime$.
\end{enumerate}
\citet{PlummerCut} shows that this algorithm does not admit $\postApproxEP$ as an invariant
distribution, and moreover that the implicit target distribution depends on the particular Markov kernels
chosen to perform the updates. Their analysis reveals two natural modifications geared towards 
improving the approximation. 
First, many $\Par$-updates can be performed for each fixed $\targetTraj^\prime$, with the final update
set to $\Par^\prime$. However, such an approach is effectively an implementation of the MwMC scheme
described above, which is what we sought to avoid in considering this algorithm. As an alternative,
\citet{PlummerCut} offers a middle ground solution that involves performing a smaller number of 
$\Par$ updates using a sequence of tempered distributions.
Second, one could seek to slow down the mixing rate of the $\targetTraj$ chain. 
If the updates  $\targetTraj \to \targetTraj^\prime$ are constrained to be small, then 
$\postDens(\cdot; \targetTraj^\prime)$ should be relatively stable. Consequently,
the $\Par$ samples might be expected to converge to a reasonable approximation of $\postApproxEP$.
Note that in the surrogate modeling setting, it is often the case that $\emDist$ can be sampled directly,
rather than requiring an update that merely leaves $\emDist$ invariant. However, using this natural 
update $\targetTraj^\prime \sim \emDist$ would be counterproductive with respect to the goal
of approximating EP in light of the effect on the mixing speed of $\targetTraj$.

\citet{SAACut} take a different approach to improve upon the naive cut algorithm, rooted in the use
of stochastic approximation Monte Carlo to approximate the intractable normalizing function 
$p(\obs \given \targetTraj^\prime)$. Their algorithm is more amenable to theoretical analysis, but 
typically requires more user tuning. [\todo: need to better understand their algorithm and verify this claim]

\subsection{Approximate MCMC Methods}
In the preceding sections, we introduced the MCwM and naive cut algorithms. These MCMC schemes
seek to target the EL and EP approximations, respectively, but in general are both approximate.
In this section, we provide connections between a family of different MCMC approximations and propose
a new approximate algorithm with intuitive appeal. While these algorithms can produce quite reasonable
empirical results, we also urge caution in their application, as seemingly arbitrary modifications to the 
algorithms result in varied levels of uncertainty propagation. We provide more insight into these mechanisms
in the particular setting of a GP log-density emulator.

\subsubsection{Expected Acceptance Probability}
We consider connections between MCwM and the naive cut algorithm, and then suggest 
a modification. To start, recall that the MCwM is a standard Metropolis-Hastings scheme, but
at each iteration the exact density evaluations $\postDens(\Par)$, $\postDens(\propPar)$
are replaced with $\hat{\postDens}^M(\Par)$ and $\hat{\postDens}^M(\propPar)$, respectively.
However, by neglecting the covariance between $\funcEm(\Par)$ and $\funcEm(\propPar)$ this 
approach typically overestimates the uncertainty in the Metropolis-Hastings acceptance probability.
We could thus consider sampling these values jointly, yielding the algorithm:
\begin{enumerate}
\item Sample $\targetTraj^\prime \sim \emDist$.
\item Apply a $\postDens(\cdot; \targetTraj^\prime)$-invariant Markov update $\Par \to \Par^\prime$.
\end{enumerate}
Note that this is identical to the naive cut algorithm in the special case that we can sample directly from 
$\emDist$. It also presents no computational issues for infinite-dimensional surrogates, since we only 
require the value of $\targetTraj^\prime$ at a finite set of points. In particular, if the second step is implemented
with a Metropolis-Hastings kernel, then the first step simplifies to 
$[\targetTraj^\prime(\Par), \targetTraj^\prime(\propPar)] \sim \emDist(\cdot, \cdot \given \Par, \propPar)$, since the values
of $\targetTraj^\prime$ are only required at the current and proposed states $(\Par, \propPar)$.

The MCwM algorithm uses the estimator $\hat{\postDens}^M(\Par)$, which averages the likelihood over $M$
samples from $\funcEm$. It is natural to wonder if averaging could also be utilized in the joint version of the 
algorithm. It seems reasonable to consider averaging over the acceptance probability
\begin{equation}
\alpha^M(\Par, \propPar) \Def \frac{1}{M} \sum_{m=1}^{M} \alpha(\Par, \propPar \given \targetEm^{(m)}),
\end{equation}
where $\targetEm^{(m)} \overset{iid}{\sim} \emDist$. We refer to this as the expected acceptance probability (E-Acc) method.
Unlike MCwM, the implicit target distribution for E-Acc does not depend on the value of $M$. However, also unlike MCwM, the 
target now depends on the proposal kernel. This is a particular case of the naive cut algorithm target distribution
depending on the Markov kernel used for the $\Par$-update. To provide intuition, we return to the GP log-density emulator example.
 
\subsubsection{Example: Gaussian Log-Density emulator}
We return to the example from \Cref{sec:ldens-Gaussian}. We consider Metropolis-Hastings 
implementations of various algorithms described above, illustrating how they incorporate noise
into the MH acceptance probability in various ways.  
Let 
\begin{align}
&\accProbMH^{\mathrm{mean}}(\Par, \propPar) \Def
\min\{1, r^{\mathrm{mean}}(\Par, \propPar)\},
&&r^{\mathrm{mean}}(\Par, \propPar) \Def 
\frac{\postDens(\propPar; \gpMean) \propDens(\propPar, \Par)}{\postDens(\Par; \gpMean) \propDens(\Par, \propPar)}
\label{eq:acc-prob-mean}
\end{align}
denote the \textit{plug-in mean} MH acceptance probability, which plugs in $\gpMean$ as an approximation 
to $\target$ without consideration of emulator uncertainty. In the setting $\postEm(\Par) = \Exp{\targetEm(\Par)}$
with $\targetEm \sim \GP(\gpMean, \gpKer)$, the Metropolis-Hastings ratio in \Cref{eq:acc-prob-mean} reduces to
\begin{equation}
r^{\mathrm{mean}}(\Par, \propPar) \Def 
\frac{\propDens(\propPar, \Par)}{\propDens(\Par, \propPar)}
\Exp{\gpMean(\propPar) - \gpMean(\Par)}. 
\end{equation}
The EL, MCwM, and E-Acc approximations introduce emulator uncertainty as
\begin{align}
r^{\mathrm{EL}}(\Par, \propPar) &\Def 
r^{\mathrm{mean}}(\Par, \propPar) \Exp{\frac{1}{2}\left(\gpKer(\propPar) - \gpKer(\Par) \right)} \\
r^{\mathrm{MCwM}}(\Par, \propPar) &\Def 
r^{\mathrm{mean}}(\Par, \propPar) \Exp{\frac{1}{2}\left[\gpKer(\propPar) + \gpKer(\Par) \right]} \\
r^{\textrm{E-acc}}(\Par, \propPar) &\Def 
r^{\mathrm{mean}}(\Par, \propPar) \Exp{\frac{1}{2}\left[\gpKer(\propPar) + \gpKer(\Par) \right] - \gpKer(\Par, \propPar)} 
\end{align}

\textbf{NOTE:} The latter two are technically not correct; the acc prob used by these algorithms
is of the form $\E[\alpha(1, r_N)]$, which reduces to a weighted average between $1$ and 
$\E[r_N]$. I report only the $\E[r_N]$ part for now.

The EL approximation scales $r^{\mathrm{mean}}(\Par, \propPar)$ with the exponential of the difference
$\gpKer(\propPar) - \gpKer(\Par)$, inflating the acceptance probability when emulator uncertainty is 
larger at the proposed point. By contrast, MCwM scales $r^{\mathrm{mean}}(\Par, \propPar)$ by 
the average of $\gpKer(\propPar)$ and $\gpKer(\Par)$; the GP uncertainty is included in a 
symmetric manner, not favoring transitions in one direction or the other. This is also the case 
with $r^{\textrm{E-acc}}(\Par, \propPar)$, but the E-acc inflation factor scales with the 
variance of $\funcEm(\propPar) - \funcEm(\Par)$, rather than the average of their variances.
If $\propPar$ and $\Par$ are close, then there may be little uncertainty in the difference
$\funcEm(\propPar) - \funcEm(\Par)$ even if the marginal variances are large. This example 
makes clear the dependence on the proposal kernel in the E-acc algorithm. If we choose a 
very small proposal, then there will be little uncertainty in the $\funcEm(\propPar) - \funcEm(\Par)$,
and thus the algorithm will target a distribution close to the plug-in mean approximation.
A proposal that favors far away points will imply $\gpKer(\Par, \propPar) \approx 0$, in which 
case E-acc will look like MCwM. Varying the width of the proposal will result in different 
degrees to which the covariance term $\gpKer(\Par, \propPar)$ exerts influence on 
the acceptance probability.

\subsection{An MCMC Approximation for the Expected Posterior} \label{sec:mcmc-ep}
In this section we propose a new Metropolis-Hastings algorithm that approximately targets 
the EP distribution, and is applicable in the GP setting. We start by noting that $\postApproxEP$
is a conditional posterior of the extended model 
\begin{align}
p(\Par, \obs, \targetEm) &\Def
\postDensNorm(\Par; \targetEm) \emDist(\targetEm) p(\obs) \\
&= \priorDens(\Par) \lik(\targetEm(\Par); \obs) \emDist(\targetEm) \frac{p(\obs)}{\normCst(\targetEm)},
\label{eq:EP-prob-model}
\end{align}
where $\normCst(\targetEm)$ is the normalizing constant for $\postDensNorm(\cdot; \targetEm)$.
The quantity $\emDist(\targetEm)$ is purely formal at the moment, since $\normCst$ does not 
have a Lebesgue density when $\targetEm$ is a GP.  Consider a Metropolis-within-Gibbs algorithm 
targeting the joint posterior over $(\Par, \targetEm)$, which updates the marginal conditionals  
\begin{align}
p(\Par \given \targetEm, \obs) \propto \priorDens(\Par) \lik(\targetEm(\Par); \obs) \label{eq:EP-conditionals} \\
p(\targetEm \given \Par, \obs) \propto \lik(\targetEm(\Par); \obs) \emDist(\targetEm) \frac{p(\obs)}{\normCst(\targetEm)}
\end{align}
in turn. The $\Par$ update can be accomplished with a standard Metropolis-Hastings step. We focus on the
$\targetEm$ update.

Suppose the current state of the chain is $(\Par, \targetTraj)$ and consider a proposal
$\tilde{\targetTraj} \sim q_{\target}(\targetTraj, \cdot)$. The MH acceptance ratio then takes the form
\begin{align}
\frac{\lik(\tilde{\targetTraj}(\Par); \obs)}{\lik(\targetTraj(\Par); \obs)} \frac{\emDist(\tilde{\targetTraj})}{\emDist(\targetTraj)} 
\frac{q_{\target}(\tilde{\targetTraj}, \targetTraj)}{q_{\target}(\targetTraj, \tilde{\targetTraj})} \frac{\normCst(\targetTraj)}{\normCst(\tilde{\targetTraj})}.
\end{align}
There are two issues to deal with here. The first is the inability to compute $\emDist(\targetTraj)$ in general, 
unless we replace $\targetTraj$ with a finite-dimensional discretization.
To address this, we can choose $q_\target$ to be $\emDist$-invariant so that the ratio simplifies to
\begin{align}
\frac{\lik(\tilde{\targetTraj}(\Par); \obs)}{\lik(\targetTraj(\Par); \obs)} 
\frac{\normCst(\targetTraj)}{\normCst(\tilde{\targetTraj})}.
\label{eq:MH-ratio-prior-invariant}
\end{align}
The simplest option is to set $q_\target(\targetTraj, \cdot) \Def \emDist(\cdot)$, which yields an MH independence
sampler update. If the emulator $\targetEm$ depends on a transformation of a latent Gaussian, then 
specialized alternatives such as the preconditioned Crank-Nicolson proposal, or an elliptical slice
sampling update, may be considered. We emphasize that the simplification in 
\Cref{eq:MH-ratio-prior-invariant} means that this ratio only depends on the emulator trajectories at the current 
value of $\Par$. Thus, there is no requirement to sample full trajectories; see \Cref{alg:mwg-ep}.

\begin{algorithm}
    \caption{Metropolis-within-Gibbs Approximation to $\postApproxEP$}
    \label{alg:mwg-ep}
    \begin{algorithmic}[1] 
    \State \textbf{Input:} Current state $(\Par, \targetTraj_\Par)$
    \State \textbf{Output:} Updated state $(\Par^\prime, \targetTraj^\prime_{\Par^\prime})$
     \State Propose $\tilde{\Par} \sim q(\Par, \cdot)$
     \State Propose $(\tilde{\targetTraj}_\Par, \tilde{\targetTraj}_{\propPar}) \sim \emDist(\cdot, \cdot \given \Par, \propPar)$ \Comment{Begin $\targetTraj$ update}
     \State $\alpha_{\targetTraj} \gets \min\left\{1, \lik(\tilde{\targetTraj}_\Par; \obs) / \lik(\targetTraj_\Par; \obs) \right\}$
      	\If{$\mathrm{Unif}(0,1) < \alpha_{\targetTraj}$}
                \State $(\targetTraj^\prime_\Par, \targetTraj^\prime_{\propPar}) \gets (\tilde{\targetTraj}_\Par, \tilde{\targetTraj}_{\propPar})$ 
            \Else
                \State $\targetTraj^\prime_{\Par} \gets \targetTraj_{\Par}$
                \State $\targetTraj^\prime_{\propPar} \gets \emDist(\cdot \given \targetEm(\Par) = \targetTraj_\Par)$
            \EndIf
      \State $\alpha_{\Par} \gets \min\left\{1, \priorDens(\propPar )\lik(\targetTraj^\prime_{\propPar}; \obs) / \priorDens(\Par) \lik(\targetTraj^\prime_{\Par}; \obs) \right\}$ \Comment{Begin $\Par$ update}
             \If{$\mathrm{Unif}(0,1) < \alpha_{\Par}$}
                \State $\Par^\prime \gets \propPar$
                \State $\targetTraj^\prime_{\Par^\prime} \gets \targetTraj^\prime_{\propPar}$ 
            \Else
            	\State $\Par^\prime \gets \Par$
		\State $\targetTraj^\prime_{\Par^\prime} \gets \targetTraj^\prime_{\Par}$
            \EndIf
    \end{algorithmic}
\end{algorithm}

The second issue is the dependence on the intractable normalizing constants 
$\normCst(\targetTraj)$ and $\normCst(\tilde{\targetTraj})$. We can consider applying techniques
from the doubly intractable MCMC literature, but these algorithms (e.g., the exchange algorithm)
typically require the ability to directly sample $\postDensNorm(\Par; \targetEm)$.
Alternatively, if the proposed value $\tilde{\targetTraj}$ is not too far from $\targetTraj$ then it may be 
reasonable to invoke the approximation $\normCst(\targetTraj) / \normCst(\tilde{\targetTraj}) \approx 1$.
\Cref{alg:mwg-ep} uses this approximation, along with the independence MH proposal.
 
\section{Alternative Viewpoints}
I've been playing around with some alternative viewpoints recently, so am adding them here for now.

\subsection{Nested Monte Carlo}
Consider approximating the normalizing constant $\normCst(\targetTraj)$ with an importance sampling
estimator of the form
\begin{equation}
\normCst^K(\targetTraj) \Def \sum_{k=1}^{K} w(\Par_k; \targetTraj) \postDens(\Par_k; \targetTraj), 
\qquad \Par_k \sim g
\end{equation}
for some proposal distribution $g$. We can then consider the 
(biased) nested Monte Carlo approximation 
\begin{align}
\hat{\pi}^{K,M}(\Par) \Def \E\left[\frac{\postDens(\Par; \targetTraj)}{\normCst(\targetTraj)} \right]
&\approx \E\left[\frac{\postDens(\Par; \targetTraj)}{\normCst^K(\targetTraj)} \right] \\
&\approx \frac{1}{M} \sum_{m=1}^{M} \frac{\postDens(\Par; \targetTraj_m)}{\normCst^K(\targetTraj_m)} \\
&= \frac{1}{M} \sum_{m=1}^{M}  
\left[\sum_{k=1}^{K} \frac{w(\Par_k; \targetTraj_m) \postDens(\Par_k; \targetTraj_m)}{\postDens(\Par; \targetTraj_m)}\right]^{-1}.
\end{align}

Suppose we ignored the bias and tried to target $\hat{\pi}(\Par) \Def \E[\hat{\pi}^{K,M}(\Par)]$. If the current state of the Markov chain is $\Par$
and the proposal is $\propPar$, then the random acceptance ratio takes the form $\hat{\pi}^{K,M}(\propPar)/\hat{\pi}^{K,M}(\Par)$. 
For example, with $M = K = 1$,
\begin{align}
&\frac{\hat{\pi}^{1,1}(\propPar)}{\hat{\pi}^{1,1}(\Par)}
= \frac{\postDens(\propPar; \targetTraj)}{\postDens(\Par; \targetTraj)} \cdot
\frac{w(\Par_1; \targetTraj)\postDens(\Par_1; \targetTraj)}{w(\Par_1; \targetTraj)\postDens(\Par_1; \targetTraj)}, &&\targetTraj \sim \emDist,
\end{align}
where the second term cancels. The MH algorithm that replaces the acceptance ratio $\hat{\pi}(\propPar)/\hat{\pi}(\Par)$ with the 
above unbiased estimate (freshly sampled each iteration) thus reduces to the E-Acc algorithm. Since we know the target distribution 
of E-acc depends on the proposal kernel, then it is clear that replacing the acceptance ratio with an unbiased estimate in this context does
not yield a valid algorithm targeting $\hat{\pi}$. Alternatively, we could consider a pseudo-marginal algorithm targeting $\hat{\pi}$ (still 
considering $M = K = 1$). In this case, the ratio would look like
\begin{align}
&\frac{\postDens(\propPar; \targetTraj^\prime)}{\postDens(\Par; \targetTraj)} \cdot
\frac{w(\Par_1; \targetTraj)\postDens(\Par_1; \targetTraj)}{w(\Par_1^\prime; \targetTraj^\prime)\postDens(\Par_1^\prime; \targetTraj^\prime)}, 
&&\targetTraj \sim \emDist, \ \Par_1 \sim g,
\end{align}
where $(\Par_1, \targetTraj)$ are recycled from the previous iteration. In this case, the second term does not cancel. Without the second
term, this is the pseudo-marginal algorithm targeting the EL approximation. This alternative pseudo-marginal algorithm incorporates 
information about the trajectory $\targetTraj$ at points other than the current and proposed states $(\Par, \propPar)$.
Compared to EL, acceptance is more probable if $\postDens(\Par_1; \targetTraj) > \postDens(\Par_1; \targetTraj^\prime)$.

\subsection{Some Similar Ideas}
We can arrive at some similar algorithms via a slightly different route. 
As discussed above, the EL distribution can be derived from the EP via the approximation
\begin{equation}
\E\left[\frac{\postDens(\Par; \targetTraj)}{\normCst(\targetTraj)} \right] \approx \frac{\E[\postDens(\Par; \targetTraj)]}{\E[\normCst(\targetTraj)]},
\end{equation}
which ignores the dependence between $\postDens(\Par; \targetTraj)$ and $\normCst(\targetTraj)$. We could consider 
the alternative approximation 
\begin{equation}
\E\left[\frac{\postDens(\Par; \targetTraj)}{\normCst(\targetTraj)} \right] 
= \E\left[\frac{1}{\normCst(\targetTraj)/\postDens(\Par; \targetTraj)}\right]
\approx \frac{1}{\E[\normCst(\targetTraj)/\postDens(\Par; \targetTraj)]}}
= \left(\int \E\left[\frac{\postDens(\Par^\prime; \targetTraj)}{\postDens(\Par; \targetTraj)} \right] d\Par^\prime \right)^{-1}
\label{eq-EP-approx}
\end{equation}
which still considers this dependence. Nested Monte Carlo estimators of the final expression will then 
yield similar expressions as above. 

\subsection{Avoiding Approximation of Normalizing Function}
Let's consider a pseudo-marginal scheme targeting 
\begin{equation}
\postApproxEP(\Par) = \E[\postDens(\Par; \targetEm)/\normCst(\targetEm)],
\end{equation}
ignoring the fact that we cannot compute $\normCst(\targetEm)$ for now. An unbiased estimate
is
\begin{align}
&\hat{\postDens}(\Par) \Def \frac{1}{M} \sum_{m=1}^{M} \frac{\postDens(\Par; \targetTraj_m)}{\normCst(\targetTraj_m)},
&&\targetTraj_m \overset{iid}{\sim} \emDist.
\end{align}
We consider a pseudo-marginal scheme targeting $\postApproxEP$. Let $\Par$ be the current state with associated
density estimate $\hat{\postDens}$, and $\propPar$ the proposed state with density estimate
$\hat{\postDens}^\prime$. The acceptance ratio (ignoring the proposal) is 
\begin{align}
\frac{\hat{\postDens}^\prime(\propPar)}{\hat{\postDens}(\Par)}
&= \sum_{m=1}^{M} \frac{\postDens(\propPar; \targetTraj_m^\prime)}{\sum_{l=1}^{L} \postDens(\Par; \targetTraj_l) 
\frac{\normCst(\targetTraj_m^\prime)}{\normCst(\targetTraj_l)}},
\end{align}
which is intractable only due to $\normCst(\targetTraj_m^\prime)/\normCst(\targetTraj_l)$. If we invoke the approximation
$\normCst(\targetTraj_m^\prime)/\normCst(\targetTraj_l) \approx 1$ for all $m,l$ (or that over and under-estimates even 
themselves out across the terms) then the acceptance ratio simplifies to
\begin{equation}
\frac{M^{-1} \sum_{m=1}^{M} \postDens(\propPar; \targetTraj_m^\prime)}{M^{-1} \sum_{l=1}^{M} \postDens(\Par; \targetTraj_l)},
\end{equation}
which recovers the pseudo-marginal algorithm targeting EL. However, the approximation
$\normCst(\targetTraj_m^\prime)/\normCst(\targetTraj_l) \approx 1$ may be quite bad given that the $\targetTraj^\prime_m$
are independently sampled at the new iteration. Supposing $\targetEm \sim \GP(\gpMean, \gpKer)$, we could consider applying the 
correlated pseudo-marginal algorithm, which draws samples via
\begin{align}
\targetTraj^\prime_m \Def \gpMean + \rho (\targetTraj_m  - \gpMean) + \sqrt{1 - \rho^2} \psi, \qquad \psi \sim \GP(0, \gpKer).
\end{align}
This slows down the mixing speed of $\targetTraj$, which will help make the approximation
$\normCst(\targetTraj_m^\prime)/\normCst(\targetTraj_l) \approx 1$ more reasonable. Note that the above proposal 
is precisely the preconditioned Crank-Nicholson proposal, and that this algorithm bears a very close resemblance to
\Cref{alg:mwg-ep}. The motivation for the correlated pseudo-marginal algorithm is typically to reduce the variance
of a noisy likelihood estimator, but in our case the motivation is achieve a better approximation of the 
normalizing function ratio. I need to work through the details of this more to better understand how this relates to the previous
algorithm; currently, they are different in that this algorithm does not include an accept-reject for $\targetEm$ (all samples
are accepted).
 
\subsection{Log-Density Emulators}
We now return to the special case of the GP log-density emulator 
\begin{align}
&\postEm(\Par) = \Exp{\targetEm(\Par)},
&&\targetEm \sim \GP(\gpMean, \gpKer),
\end{align}
which implies 
\begin{align}
\E\left[\frac{\postDens(\Par^\prime; \targetEm)}{\postDens(\Par; \targetEm)} \right]
&= \Exp{\gpMean(\Par^\prime) - \gpMean(\Par)} 
\Exp{\frac{1}{2}[\gpKer(\Par^\prime) + \gpKer(\Par)]} \Exp{-\gpKer(\Par, \Par^\prime)}.
\end{align}
Plugging this into \Cref{eq-EP-approx} we obtain
\begin{align}
&\left(\int \E\left[\frac{\postDens(\Par^\prime; \targetTraj)}{\postDens(\Par; \targetTraj)} \right] d\Par^\prime \right)^{-1} \nonumber \\
&= \Exp{\gpMean(\Par) - \frac{1}{2}\gpKer(\Par)} 
\left(\int \Exp{\gpMean(\Par^\prime) + \frac{1}{2}\gpKer(\Par^\prime)} \Exp{-\gpKer(\Par, \Par^\prime)} d\Par^\prime \right)^{-1} \nonumber \\
&\propto \Exp{\gpMean(\Par) - \frac{1}{2}\gpKer(\Par)}
\left(\int \Exp{-\gpKer(\Par, \Par^\prime)} \postApproxNormMarg(d\Par^\prime) \right)^{-1} \label{eq-EP-approx-ldens}
\end{align}

\paragraph{Naive approximation 1.}
The expression in \Cref{eq-EP-approx-ldens} reduces to $\postApproxMarg(\Par)$ by invoking the approximation
\begin{align}
\int \Exp{-\gpKer(\Par, \Par^\prime)} \postApproxNormMarg(d\Par^\prime)
&\approx \Exp{-\gpKer(\Par)}.
\end{align}
Intuitively, this seems like a poor approximation in general; e.g., if there are many 
values far away from from $\Par$ that are given significant weight under $\postApproxNormMarg$
then these values will tug the integral towards one.

\paragraph{Naive approximation 2.}
If $\postApproxNormMarg(d\Par^\prime) \approx \delta_{\Par_\star}(d\Par^\prime)$ then the approximation
\begin{align}
\int \Exp{-\gpKer(\Par, \Par^\prime)} \postApproxNormMarg(d\Par^\prime)
&\approx \Exp{-\gpKer(\Par, \Par_\star)}
\end{align}
may be reasonable. In this case, we obtain the (unnormalized) density approximation
\begin{align}
\hat{\pi}(\Par)
&= \Exp{\gpMean(\Par) - \frac{1}{2}\gpKer(\Par) + \gpKer(\Par, \Par_\star)} \\
&= \postApproxMarg(\Par) \Exp{\gpKer(\Par, \Par_\star) - \gpKer(\Par)} \\ 
&\propto \Exp{\gpMean(\Par) - \frac{1}{2} \Var[\funcEm(\Par) - \funcEm(\Par_\star)]} \\
&= \Exp{\gpMean(\Par) - \gamma_N(\Par, \Par_\star)},
\end{align}
where $\gamma_N}$ is the variogram of the random function $\funcEm$, a commonly-used
measure of dependence in geostatistics. 
Interestingly, this yields a computable unnormalized density approximation (like EL) which 
now depends on the GP covariance as well. Based on the final expression above, 
$\hat{\pi}(\Par)$ favors points where the predictive mean is large, and the dependence 
between $\funcEm(\Par)$ and $\funcEm(\Par_\star)$ is large (i.e., the variogram is small).  
To better compare to EL, consider
\begin{align}
\frac{\hat{\pi}(\propPar) / \hat{\pi}(\Par)}{\postApproxMarg(\propPar) / \postApproxMarg(\Par)}
&= \frac{\Exp{\gpKer(\Par) - \gpKer(\Par, \Par_\star)}}{\Exp{\gpKer(\propPar) - \gpKer(\propPar, \Par_\star)}}.
\end{align} 
We conclude that $\hat{\pi}$ inflates the point $\propPar$, relative to $\hat{\pi}$, when 
$\gpKer(\Par) - \gpKer(\Par, \Par_\star) > \gpKer(\propPar) - \gpKer(\propPar, \Par_\star)$.
Since this still seems to favor points close to $\Par_\star$, it is unclear how much this can 
help offset the undesirable behavior of EL in certain settings. 

\paragraph{Naive approximation 3.}
Next, suppose that $\postApproxNormMarg$ has multiple concentrated nodes so that 
$\postApproxNormMarg(d\Par^\prime) \approx \sum_{m=1}^{M} w_m \delta_{\Par_m}(d\Par^\prime)$,
in which case
\begin{align}
\int \Exp{-\gpKer(\Par, \Par^\prime)} \postApproxNormMarg(d\Par^\prime)
&\approx \sum_{m=1}^{M} w_m \Exp{-\gpKer(\Par, \Par_m)}.
\end{align}
In this case, we obtain the (unnormalized) density approximation
\begin{align}
\hat{\pi}(\Par)
&= \Exp{\gpMean(\Par) - \frac{1}{2}\gpKer(\Par)} \cdot \left[\sum_{m=1}^{M} w_m \Exp{-\gpKer(\Par, \Par_m)}\right]^{-1} \\
&= \left[\sum_{m=1}^{M} \frac{w_m}{\Exp{\gpMean(\Par) - \gamma_N(\Par, \Par_m)}}  \right]^{-1},
\end{align}
another harmonic mean type estimator.

% Appendix
\section{Appendix}

For the below proofs we use the following measure-theoretic setup. For brevity, we drop the 
$\Ndesign$ subscripts on quantities other than $\postNormEm$.
Let $\refMeas$ be a reference measure (e.g., Lebesgue) on $(\parSpace, \BorelSig)$,
and $\emDist[]$ a probability measure on $(\emSpace, \emSig)$. Assume that 
the map $(\Par, \targetTraj) \mapsto \postDens(\Par; \targetTraj)$ is measurable
and $\normCst(\targetTraj) \Def \int \postDens(\Par; \targetTraj) \emDist[](d\targetTraj) \in (0,\infty)$
$\emDist[]$-almost surely. Let $\postDensNorm(\Par; \targetTraj) \Def \postDens(\Par; \targetTraj) / \normCst(\targetTraj)$
and define the joint distribution 
$\emJoint(d\Par, d\targetTraj) \Def \postDensNorm(\Par; \targetTraj) \refMeas(d\Par)\emDist[](d\targetTraj)$.
Define $\postApproxEP[]$ to be the density corresponding to the $\Par$-marginal of $\emJoint$; that is,
$\postApproxEP[](\Par) \Def \int \postDensNorm(\Par; \targetTraj) \emDist[](d\targetTraj)$. 

\subsection{Proof of \Cref{prop:EP-variational}}
\paragraph{KL Divergence.} We start by proving the KL divergence result.
Let $\qMeas$ be a probability measure on $\parSpace$ with 
$\refMeas$-density $\qDens$. We restrict to measures 
$\postApproxEP[] \ll \qMeas$, as the KL divergence is infinite otherwise. 
Note that 
$\frac{d\emJoint}{d(\emDist[] \otimes \qMeas)}(\Par, \targetTraj) = \frac{\postDensNorm(\Par; \targetTraj)}{\qDens(\Par)}$.
Using Tonelli's theorem, we have 
\begin{align*}
\E_{\emDist[]}\left[\KL{\postNormEm}{\qMeas} \right]
&= \int_{\emSpace} \int_{\parSpace} 
\postDensNorm(\Par; \targetTraj) \log \frac{\postDensNorm(\Par; \targetTraj)}{\qDens(\Par)}
\refMeas(d\Par) \emDist[](d\targetTraj) \\
&= \int_{\parSpace \times \emSpace}  \log \frac{\postDensNorm(\Par; \targetTraj)}{\qDens(\Par)}
\emJoint(d\Par, d\targetTraj) \\
&= \int_{\parSpace \times \emSpace}  \log \frac{d\emJoint}{d(\emDist[] \otimes \qMeas)}(\Par, \targetTraj)
\ \emJoint(d\Par, d\targetTraj) \\
&= \KL{\emJoint}{\emDist[] \otimes \qMeas}.
\end{align*}
Finally, 
\begin{align*}
\KL{\emJoint}{\emDist[] \otimes \qMeas}
&= \int \log \frac{d\emJoint}{d(\emDist[] \otimes \qMeas)} d\emJoint \\
&= \int \log \left[\frac{d\emJoint}{d(\emDist[] \otimes \postApproxEP[])} \frac{d(\emDist[] \otimes \postApproxEP[])}{d(\emDist[] \otimes \qMeas)}\right] d\emJoint \\
&\propto \int \log \frac{d(\emDist[] \otimes \postApproxEP[])}{d(\emDist[] \otimes \qMeas)} d\emJoint \\
&= \int_{\emSpace \times \parSpace} 
\log \frac{\postApproxEP[](\Par)}{\qDens(\Par)} \postDensNorm(\Par; \targetTraj) \emDist[](d\targetTraj) \refMeas(d\Par) \\
&= \int_{\parSpace} \log \frac{\postApproxEP[](\Par)}{\qDens(\Par)} 
 \postApproxEP[](\Par) \refMeas(d\Par) \\
&= \KL{\postApproxEP[]}{\qMeas}.
\end{align*} 
where we have used ``$\propto$'' to absorb additive constants with respect to $\qMeas$. The result follows 
from the fact that $\KL{\postApproxEP[]}{\qMeas}$ is uniquely minimized at $\qMeas = \postApproxEP[]$. $\qquad \blacksquare$

\paragraph{Squared $L_2$ loss.} 
For the expected squared error objective, apply Tonneli's theorem 
\begin{align}
\E_{\emDist[]}\left[\norm{\postNormEm - \qMeas}^2_{L_2(\parSpace)} \right]
&= \int \E_{\emDist[]} \left[\postDensNorm(\Par; \targetTraj) - \qDens(\Par) \right]^2 \refMeas(d\Par) \nonumber
\end{align}
and notice that the integrand is minimized pointwise by
$\qDens(\Par) = \E_{\emDist[]}[\postDensNorm(\Par; \targetTraj)] = \postApproxEP[](\Par)$. $\qquad \blacksquare$
 
\bibliography{prob_surrogates_bayes} 
% \bibliographystyle{ieeetr}

\end{document}


