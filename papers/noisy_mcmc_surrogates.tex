\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode} % Note that this also loads algorithmicx
\usepackage{cleveref}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{caption, subcaption} % Captions and sub-figures. 
\usepackage{fancyvrb} % For writing using verbatim font inline. 
% \usepackage[demo]{graphicx}

% For tables
\usepackage{array}
\usepackage{booktabs}

% Bibliography
\bibliographystyle{plainnat}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Tables. 
\usepackage{multirow}

% Local custom commands. 
\include{latex_macros_general}
\include{latex_macros_gp_inv_prob}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{{./figures/}}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Uncertainty Propagation and Active Learning for Surrogate-Based Bayesian Inference}
\author{Andrew Roberts}

\begin{document}

\maketitle

Computer models have emerged as a key tool for studying complex systems 
within the physical, biological, and engineering sciences. Such models often have uncertain 
parameters that must be estimated from data. Bayesian methods are commonly employed
to quantify parameter uncertainty in these calibration tasks.
However, standard Bayesian inference algorithms such as Markov chain Monte Carlo (MCMC) 
are often hindered by the computational cost of the simulation model.
A popular approach to deal with this issue is to train a surrogate model 
(i.e., emulator) as a computationally thrifty approximation of the expensive computer code. 

While the use of surrogates offers the potential for significant computational savings,
emulator error induces biases in posterior inference. In order to avoid miscalibrated
posterior uncertainty estimates, it is crucial to acknowledge the surrogate uncertainty within
the posterior approximation. Probabilistic surrogates such as Gaussian processes \citep{gpML,gramacy2020surrogates} 
and probabilistic neural networks \citep{deepEnsembles,BayesOptNN} provide a notion of predictive uncertainty 
that can be propagated in downstream inference
 \citep{reviewPaper,BilionisBayesSurrogates,BurknerSurrogate,CES,FerEmulation}. 
 However, no single method has emerged as the objectively correct approach to propagate 
 surrogate uncertainty within a posterior approximation. 
 Various approximations and inference algorithms have been proposed, each of which propagate
 surrogate uncertainty in distinct ways 
 \citep{reviewPaper,BilionisBayesSurrogates,StuartTeck1,VehtariParallelGP,BurknerSurrogate,
 BurknerTwoStep,FerEmulation,garegnani2021NoisyMCMC}.

In the ideal setting, information from additional runs of the computer model can be leveraged to iteratively
refine the surrogate in order to eliminate the posterior bias. In practice, limited computational 
budgets render this infeasible, but targeted simulator runs can be used to reduce the error as much
as possible. Since the cost of running the computer model is the primary computational bottleneck,
the new parameter values at which to run the model should be carefully selected.
The surrogate predictive uncertainty can be used to guide this decision, targeting parameters with the potential 
for maximal improvement in the posterior approximation. 
Various active learning (i.e., sequential design) algorithms have been proposed for this 
purpose \citep{SinsbeckNowak,StuartTeck2,VehtariParallelGP,
Surer2023sequential, gp_surrogates_random_exploration,weightedIVAR}.

Many of the proposed uncertainty propagation and active learning algorithms can be categorized based on the
target quantity that is emulated. Two broad strategies have emerged in surrogate-based Bayesian inference:
(i.) forward model emulation, in which an emulator approximates the underlying computer model, and 
(ii) log-density emulation, in which an emulator directly approximates the log-likelihood or unnormalized
log-posterior density. These strategies are compared in \citep{StuartTeck1,GP_PDE_priors,reviewPaper}.
In either case, the underlying probabilistic emulator induces a random approximation of the posterior density.
Owing to the difficulty of working with the normalizing constant of this random posterior, the majority of previous 
work derives posterior approximations and active learning criteria based on the induced approximation to the
\textit{unnormalized} posterior density \citep{SinsbeckNowak,VehtariParallelGP,StuartTeck1}. 
This choice is primarily motivated by computational convenience, and has been shown to facilitate closed-form
computations in special cases involving Gaussian processes and Gaussian likelihoods 
\citep{reviewPaper,StuartTeck1,StuartTeck2,GP_PDE_priors,VehtariParallelGP,Surer2023sequential}.

\paragraph{Contributions.}
As summarized above, a wide variety of methods have been proposed for posterior approximation
and sequential design in surrogate-based Bayesian inference. 
In this paper, we provide new insights into when and why the performance of different methods can
deviate. In particular, we investigate the implications of basing algorithms on the unnormalized 
posterior density approximation induced by an underlying probabilistic emulator. We show that 
the effect of ignoring the normalizing constant depends on the particular emulator target, and is 
typically more significant in the log-density emulation setting.

Based on these insights, we propose posterior approximation and active learning algorithms 
that take into account the ratio between random density values at different parameter values.
Rather than deriving an estimate of the unnormalized posterior density, our posterior approximation
method centers on the idea of approximating the acceptance probability of a Metropolis-Hastings
MCMC scheme. This perspective leads to a class of noisy MCMC algorithms \citep{noisyMCMC}
that are empirically shown to be more robust than alternatives based on unnormalized density
estimates. These algorithms require only
slight adjustments to standard MCMC implementations, and are generally applicable to any 
surrogate with a predictive distribution from which samples can be drawn.

We then propose several new active learning criteria that sequentially select new emulator training
points by targeting regions in parameter space in which the surrogate-approximated Metropolis-Hastings
acceptance probability exhibits high uncertainty. 
\todo: finish this

% Surrogates for Bayesian Inverse Problems
\section{Surrogates for Bayesian Inverse Problems}

\subsection{Bayesian Inference Setting}
Our general goal is to estimate parameters $\Par \in \parSpace \subseteq \R^{\dimPar}$ given 
observations $\obs \in \obsSpace \subseteq \R^{\dimObs}$ within a Bayesian framework.
Assuming a prior density $\priorDens(\Par)$ and likelihood $p(\obs \given \Par)$, 
we seek to characterize the posterior distribution 
\begin{align}
&\postDensNorm(\Par) \Def p(\Par \given \obs) = \frac{1}{\normCst} \priorDens(\Par) p(\obs \given \Par), 
&&\normCst = \int_{\parSpace} \priorDens(\Par) p(\obs \given \Par) d\Par, \label{post_dens_generic}
\end{align}
where $\normCst$ is an (intractable) normalizing constant. 
Let $\postDens(\Par) \Def \priorDens(\Par) p(\obs \given \Par)$ denote the unnormalized posterior 
density, which plays a central role in our development. 
We assume that pointwise evaluations of  $\postDens(\Par)$ can be computed, but at significant 
computational expense. This renders standard iterative optimization and sampling algorithms 
(e.g., MCMC) infeasible, as each iteration requires a new density evaluation $\postDens(\Par)$.
To address this computational bottleneck, typical surrogate workflows ultimately induce 
a (random) approximation to $\postDens(\Par)$.
In the following, we motivate this setup by introducing common surrogate modeling strategies used 
in solving Bayesian inverse problems, and describe how each approach results in a 
stochastic approximation to $\postDens(\Par)$.

\subsection{Bayesian Inverse Problems}
The challenge posed by computationally expensive density evaluations $\postDens(\Par)$ commonly 
arises in the Bayesian approach to inverse problems \citep{Stuart_BIP}. In this setting, 
the likelihood often takes the form $\obs = \fwd(\Par) + \noise$ for some forward model
$\fwd: \parSpace \to \obsSpace$. For a concrete example, we consider the problem of estimating the 
parameters in a system of ordinary differential equations (ODEs)
\begin{align}
\frac{d}{d\Time} \state(\Time, \Par) &= \odeRHS(\state(\Time, \Par), \Par), &&x(\timeStart) = \stateIC, \label{ode_ivp}
\end{align}
where the dynamics depend on parameters $\Par$. Each value for $\Par$ implies a different state trajectory
$[\state(\Time, \Par)]_{\timeStart \leq \Time \leq \timeEnd}$, which we encode by the
map $\solutionOp: \Par \mapsto [\state(\Time, \Par)]_{\timeStart \leq \Time \leq \timeEnd.}$. The goal is then 
to identify the parameters that yield state trajectories that are in agreement with observed data 
$\obs$, which is assumed to be some noise-corrupted function $\obsOp$ of the true trajectory. Thus, the 
likelihood is of the form 
\begin{align}
&\obs = \fwd(\Par) + \noise, &&\fwd \Def \obsOp \circ \solutionOp.
\end{align}
In practice, the ODE is solved numerically so $\solutionOp$ represents the map induced by a numerical 
solver. Therefore, in this setting the computational cost of computing $\postDens(\Par)$ stems from the 
dependence of the likelihood on $\fwd(\Par)$, and in particular on the solver $\solutionOp(\Par)$.

\subsection{Common Surrogate Strategies} \label{sec:surrogate-examples}
Given the cost of computing $\postDens(\Par)$, we seek to approximate the posterior
using minimal queries to the posterior density. In addition, in many practical
settings it is necessary that these density evaluations be computed in parallel. 
A popular approach satisfying these criteria involves generating an 
\textit{initial design} $\{(\Par_{\idxDesign}, \postDens(\Par_{\idxDesign}))\}_{\idxDesign=1}^{\Ndesign}$ 
(in parallel), fitting a regression model $\postEm[\Ndesign](\Par)$ to approximate 
the map $\Par \mapsto \postDens(\Par)$, and then using the approximation $\postEm[\Ndesign](\Par)$
in place of the exact density. We refer to $\postEm[\Ndesign](\Par)$ as the 
\textit{surrogate density}, with the subscript indicating the number of design points
used for training. Typically, the 


A common solution to this computational bottleneck is to learn a statistical approximation of the map 
$\Par \mapsto \postDens(\Par)$, which we refer to synonymously as a \textit{surrogate} or \textit{emulator}
model. Learning the surrogate typically consists of an initial offline step where training data 
is generated by running the expensive simulator at a set of input parameter values. A regression model 
is then fit to this training set to learn an approximation of the input-output relationship 
$\Par \mapsto \postDens(\Par)$. Our interest is in \textit{probabilistic} surrogate models, which output
a prediction for $\postDens(\Par)$ in the form of a probability distribution, as opposed to just a point estimate.
We let $\postEm[\Ndesign](\Par)$ denote the random variable representing the surrogate prediction 
of $\postDens(\Par)$, with the subscript indicating the number of exact simulation runs that were used 
to train the surrogate. We refer to the distribution of $\postEm[\Ndesign](\Par)$ as the surrogate 
\textit{predictive distribution} at input $\Par$; we allow this distribution to be arbitrary, and potentially 
only accessible through samples. Note that the map $\Par \mapsto \postDens(\Par)$ is typically not 
approximated directly; rather, an emulator is fit to a quantity on which $\postDens(\Par)$ depends 
(e.g., the log-likelihood or an underlying mechanistic model), which then induces a random approximation 
of $\postDens(\Par)$. Our framework thus seeks to be agnostic to the particular quantity that is 
being emulated. Concrete examples of typical surrogate modeling workflows are given in \cref{sec:surrogate-examples}.

Given $\postEm[\Ndesign]$, a fixed random approximation of $\postDens$, a natural question is how to 
utilize this surrogate in approximating the posterior distribution in \cref{post_dens_generic}. \Cref{sec:post-approx}
is dedicated to this question, in which we summarize previous approaches and present our proposed method.




We focus on the Bayesian inference setting in which pointwise evaluations of the unnormalized posterior
density 
\begin{equation}
\postDens(\Par) \Def \priorDens(\Par) p(\obs \given \Par) \label{post_dens_generic}
\end{equation}
are available, but expensive owing to the cost of computing the likelihood $p(\obs \given \Par)$. This setting 
commonly arises in the Bayesian approach to inverse problems \citep{Stuart_BIP}. 
Consider a \textit{forward model} $\fwd: \parSpace \subseteq \R^{\dimPar} \to \R^{\dimObs}$ describing 
some system of interest, parameterized by input parameters $\Par \in \parSpace$. In addition, suppose that 
we have noisy observations $\obs \in \obsSpace \subseteq \R^{\dimObs}$ of the output signal that $\fwd(\Par)$ 
seeks to approximate. The \textit{inverse problem} concerns learning the parameter values $\Par$ such
 that $\fwd(\Par) \approx \obs$; i.e., \textit{calibrating} the model so that it agrees with the observations. 
 The statistical approach to this problem assumes that the link between model outputs and 
observations is governed by a probability distribution on $\obs \given \Par$. We assume that this distribution 
admits a density with corresponding log-likelihood 
\begin{align}
\llik: \parSpace \to \R, \label{log_likelihood}
\end{align}
such that $p(\obs \given \Par) = \Exp{\llik(\Par)}$. The notation $\llik(\Par)$ suppresses the dependence 
on $\obs$, as the observed data will be fixed throughout. We start by focusing inference only on the 
calibration parameter $\Par$, assuming that other likelihood parameters (e.g., noise covariance) 
are fixed. We discuss inference for such nuisance parameters in \Cref{section_lik_par}.

The Bayesian approach completes this specification with a prior distribution 
on $\Par$. Letting $\priorDens(\Par)$ denote the density of this distribution, the Bayesian solution of the inverse problem is given 
by the posterior distribution
\begin{align}
\postDensNorm(\Par) \Def p(\Par \given \obs) = \frac{1}{\normCst}\postDens(\Par) = \frac{1}{\normCst} \priorDens(\Par) \Exp{\llik(\Par)}, \label{post_dens}
\end{align}
with normalizing constant given by 
\begin{align}
\normCst &= \int \priorDens(\Par) \Exp{\llik(\Par)} d\Par. \label{norm_cst}
\end{align}
We emphasize that throughout this paper $\postDens(\Par)$ denotes the \textit{unnormalized} 
posterior density. We will also find it useful to introduce the notation
\begin{equation}
\lpost(\Par) \Def \log \postDens(\Par) = \log \priorDens(\Par) + \llik(\Par) \label{eq:lpost}
\end{equation}
for the logarithm of the unnormalized posterior density.
When relevant, we will make explicit the dependence on the forward 
model by writing $\llik(\Par; \fwd)$, $\lpost(\Par; \fwd)$, and $\postDens(\Par; \fwd)$. 

In addition to considering 
generic likelihoods, we will give special attention to the additive Gaussian noise model
\begin{align}
\obs &= \fwd(\Par) + \noise \label{inv_prob_Gaussian} \\
\noise &\sim \Gaussian(0, \likPar) \nonumber 
\end{align}
with corresponding log-likelihood 
\begin{align}
\llik(\Par; \fwd) &= \log\Gaussian(\obs| \fwd(\Par), \likPar) 
= -\frac{1}{2} \log\det (2\pi \likPar) - \frac{1}{2} (\obs - \fwd(\Par))^\top \likPar^{-1} (y - \fwd(\Par)), \label{llik_Gaussian}
\end{align}
as this model features prominently in the inverse problems literature.
In general, the nonlinearity of $\fwd$ precludes a closed-form characterization of the posterior. In this case, the 
standard approach is to instead draw samples from the distribution using a Markov chain Monte Carlo (MCMC) 
algorithm. Such algorithms are serial in nature, often requiring $\sim 10^5 - 10^7$ iterations, with each 
iteration involving an evaluation of the unnormalized posterior density $\postDens(\Par; \fwd)$. 
In the inverse problem context, this computational requirement is often prohibitive when the forward model 
evaluations $\fwd(\Par)$ incur significant computational cost, as each density evaluation requires the 
expensive computation $\fwd(\Par)$. Motivated by this problem, a large body of work has focused on deriving 
cheap approximations to either $\fwd(\Par)$ or $\lpost(\Par)$ (note that approximating the former induces 
an approximation of the latter). We focus on surrogate models that take the form of statistical 
regression models approximating the maps $\Par \mapsto \fwd(\Par)$ or $\Par \mapsto \lpost(\Par)$.
\footnote{This is in contrast to other surrogate modeling strategies (e.g., reduced-order modeling)
that exploit the specific structure of the forward model; see, e.g., \todo.}
We refer to methods that explicitly model the former map as \textit{forward model emulation}, and those that 
model the latter as \textit{log-density emulation}. We also include methods that emulate the log-likelihood
map $\Par \mapsto \llik(\Par)$ in this latter category (see \Cref{sec:llik_vs_lpost}).
Throughout this review, we will typically view the functions
$\fwd(\Par)$, $\llik(\Par)$, and $\lpost(\Par)$ as computationally expensive black-boxes. We will occasionally refer to 
such maps as \textit{simulators}, owing to the fact that in typical applications evaluating these maps requires
running an expensive computer code. The following section provides a concrete example of such a simulation
model stemming from the numerical solution of differential equations.


\bibliography{prob_surrogates_bayes} 
% \bibliographystyle{ieeetr}

\end{document}







