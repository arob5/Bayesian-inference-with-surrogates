{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3958738f-c7f6-4401-a12d-b90b554ffa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1105cdd30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40eb57f-7873-4090-8278-05d6d3c727f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "train_y = torch.sin(train_x * (2 * torch.pi)) + 0.2 * torch.randn(train_x.size())\n",
    "\n",
    "# Add batch dimension\n",
    "train_x = train_x.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4131ccd3-f692-45e1-b71f-bcdb07e1169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFFGPModel(\n",
      "  (likelihood): GaussianLikelihood(\n",
      "    (noise_covar): HomoskedasticNoise(\n",
      "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
      "    )\n",
      "  )\n",
      "  (mean_module): ConstantMean()\n",
      "  (covar_module): ScaleKernel(\n",
      "    (base_kernel): RFFKernel(\n",
      "      (raw_lengthscale_constraint): Positive()\n",
      "    )\n",
      "    (raw_outputscale_constraint): Positive()\n",
      "  )\n",
      ")\n",
      "likelihood.noise_covar.raw_noise tensor([0.])\n",
      "mean_module.raw_constant tensor(0.)\n",
      "covar_module.raw_outputscale tensor(0.)\n",
      "covar_module.base_kernel.raw_lengthscale tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "# ---- Define the GP Model ----\n",
    "class RFFGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_rff=100):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # Mean\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        # Base kernel\n",
    "        base_kernel = gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[-1])\n",
    "\n",
    "        # RFF approximation wrapped in ScaleKernel\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RFFKernel(num_rff, base_kernel=base_kernel)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Define likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = RFFGPModel(train_x, train_y, likelihood, num_rff=256)\n",
    "\n",
    "# Print model information\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbf5b851-b927-4f1f-a7a0-c6396da0fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('likelihood.noise_covar.raw_noise', tensor([0.])),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n",
       "              tensor(1.0000e-04)),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('mean_module.raw_constant', tensor(0.)),\n",
       "             ('covar_module.raw_outputscale', tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale', tensor([[0.]])),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('covar_module.raw_outputscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.raw_outputscale_constraint.upper_bound',\n",
       "              tensor(inf))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()\n",
    "# model.covar_module.base_kernel.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d7a580f-3b2c-4b3c-a8fb-875d98050140",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, train_y)\n\u001b[0;32m---> 13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py311/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py311/lib/python3.11/site-packages/torch/autograd/__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    252\u001b[0m     (inputs,)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py311/lib/python3.11/site-packages/torch/autograd/__init__.py:133\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 200\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Iter {i:3d}/{training_iter} - Loss: {loss.item():.3f}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f7561-a813-497a-9a11-bb13015e4a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Optimize hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# 5. Switch to eval mode and make predictions\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "test_x = torch.linspace(0, 1, 200).unsqueeze(-1)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    pred = likelihood(model(test_x))\n",
    "\n",
    "# Extract mean and confidence interval\n",
    "mean = pred.mean\n",
    "lower, upper = pred.confidence_region()\n",
    "\n",
    "# 6. Plot results\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_x.numpy(), train_y.numpy(), \"k*\", label=\"Training data\")\n",
    "plt.plot(test_x.numpy(), mean.numpy(), \"b\", label=\"Predictive mean\")\n",
    "plt.fill_between(\n",
    "    test_x.squeeze().numpy(),\n",
    "    lower.numpy(),\n",
    "    upper.numpy(),\n",
    "    alpha=0.3,\n",
    "    label=\"Confidence\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
